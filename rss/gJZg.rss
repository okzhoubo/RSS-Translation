<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-08-17T11:08:57.900-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="Education"></category><category term="conferences"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="MOOC"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="accessibility"></category><category term="optimization"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Android"></category><category term="Awards"></category><category term="ICML"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Maps"></category><category term="Google Translate"></category><category term="Responsible AI"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Interspeech"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="RAI-HCT Highlights"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://ai.googleblog.com/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1265&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-4779594044050650231&lt;/id>;&lt;发布>;2023-08-17T11:08:00.000-07:00&lt;/发布>;&lt;更新>;2023-08- 17T11:08:24.346-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;组合优化的神经网络修剪&lt;/stitle>;&lt;content type=&quot;html&quot;>; &lt;span class=&quot;byline-author&quot;>;发布者：Athena 团队研究科学家 Hussein Hazimeh 和麻省理工学院研究生 Riade Benbaki&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcN WNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s320/CHITA%20hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 现代神经网络在各种应用中都取得了令人印象深刻的性能，例如 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model /&quot;>;语言、数学推理&lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;愿景&lt;/a>; 。然而，这些网络通常使用需要大量计算资源的大型架构。这使得向用户提供此类模型变得不切实际，特别是在可穿戴设备和智能手机等资源有限的环境中。降低预训练网络的推理成本的&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;广泛使用的方法&lt;/a>;是通过删除一些网络来修剪它们它们的权重，不会显着影响效用。在标准神经网络中，每个权重定义两个神经元之间的连接。因此，在权重被修剪后，输入将通过较小的连接集传播，因此需要较少的计算资源。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto” ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw -bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBRYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEg D-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s1600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width= “1600”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FonAvjG2Hw J4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;原始网络与修剪后的网络。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;剪枝方法可以应用于网络训练过程的不同阶段：训练后、训练期间或训练之前（即权重初始化后立即）。在这篇文章中，我们重点关注训练后设置：给定一个预先训练的网络，我们如何确定应该修剪哪些权重？一种流行的方法是&lt;a href=&quot;https://jmlr.org/papers/volume22/21-0366/21-0366.pdf&quot;>;幅度剪枝&lt;/a>;，它删除幅度最小的权重。虽然有效，但该方法没有直接考虑删除权重对网络性能的影响。另一种流行的范例是&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;基于优化的剪枝&lt;/a>;，它根据权重的删除对损失函数的影响程度来删除权重。尽管在概念上很有吸引力，但大多数现有的基于优化的方法似乎都面临着性能和计算要求之间的严重权衡。进行粗略近似的方法（例如，假设对角线&lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;>;Hessian_matrix&quot;>;Hessian 矩阵&lt;/a>;）可以很好地扩展，但性能相对较低。另一方面，虽然进行较少近似的方法往往表现更好，但它们的可扩展性似乎要差得多。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2302.14623&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>;”中，介绍于 &lt;a href=&quot; https://icml.cc/Conferences/2023/&quot;>;ICML 2023&lt;/a>;，我们描述了如何开发一种基于优化的方法来大规模修剪预训练的神经网络。 CHITA（代表“组合 Hessian 迭代阈值算法”）在可扩展性和性能权衡方面优于现有的剪枝方法，并且它通过利用多个领域的进步来做到这一点，包括 &lt;a href=&quot;https://en. wikipedia.org/wiki/High-Dimensional_statistics&quot;>;高维统计&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;组合优化&lt;/a>;和神经网络修剪。例如，CHITA 的速度比最先进的修剪方法快 20 倍到 1000 倍 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>;，并将准确性提高 10 以上% 在许多设置中。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;贡献概述&lt;/h2>; &lt;p>; CHITA 相对于流行方法有两项显着的技术改进：&lt; /p>; &lt;ul>; &lt;li>;&lt;strong>;二阶信息的有效使用&lt;/strong>;：使用二阶信息的修剪方法（即，与&lt;a href=&quot;https://en.wikipedia. org/wiki/Second_derivative&quot;>;二阶导数&lt;/a>;）在许多设置中实现了最先进的技术。在文献中，此信息通常通过计算 Hessian 矩阵或其逆矩阵来使用，这种操作很难扩展，因为 Hessian 大小与权重数量成二次方。通过仔细的重新表述，CHITA 使用二阶信息，而无需显式计算或存储 Hessian 矩阵，从而实现更高的可扩展性。 &lt;/li>;&lt;li>;&lt;strong>;组合优化&lt;/strong>;：流行的基于优化的方法使用一种简单的优化技术，单独修剪权重，即，在决定修剪某个权重时，它们不考虑是否其他权重已被修剪。这可能会导致重要权重的修剪，因为当其他权重被修剪时，孤立地被认为不重要的权重可能会变得重要。 CHITA 通过使用更先进的组合优化算法来避免这个问题，该算法考虑了修剪一个权重如何影响其他权重。 &lt;/li>; &lt;/ul>; &lt;p>; 在下面的部分中，我们讨论 CHITA 的剪枝公式和算法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;计算友好的剪枝公式&lt;/h2>; &lt;p>; 有许多可能的剪枝候选者，其中通过仅保留原始网络权重的子集来获得。令&lt;em>;k&lt;/em>;为用户指定的参数，表示要保留的权重数量。剪枝可以自然地表述为一个&lt;a href=&quot;https://arxiv.org/abs/1803.01454&quot;>;最佳子集选择&lt;/a>;（BSS）问题：在所有可能的剪枝候选者（即权重子集）中仅保留 &lt;em>;k&lt;/em>; 个权重，选择损失最小的候选者。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithv XGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s1600/image5.gif&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY 1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;作为 BSS 问题的剪枝：在具有相同权重总数的所有可能的剪枝候选者中，最佳候选者被定义为损失最小的候选者。该图显示了四个候选者，但这个数字通常要大得多。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;解决原始损失函数上的剪枝BSS问题通常是通过计算来解决的棘手的。因此，与之前的工作类似，例如 &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf&quot;>;OBD&lt;/a>; 和 &lt;a href=&quot; https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&quot;>;OBS&lt;/a>;，我们用 &lt;a href=&quot; 来近似损失https://en.wikipedia.org/wiki/Quadratic_form&quot;>;二次函数&lt;/a>;，使用二阶&lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor_series&quot;>;泰勒级数&lt; /a>;，其中 Hessian 矩阵是根据经验 &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;>;Fisher 信息矩阵&lt;/a>; 进行估计的。虽然梯度通常可以有效地计算，但由于其庞大的规模，计算和存储 Hessian 矩阵的成本过高。在文献中，通常通过对 Hessian 矩阵（例如，对角矩阵）和算法（例如，单独剪枝权重）做出限制性假设来应对这一挑战。 &lt;/p>; &lt;p>; CHITA 使用修剪问题的有效重新表述（使用二次损失的 BSS），避免显式计算 Hessian 矩阵，同时仍然使用该矩阵中的所有信息。这是通过利用经验费希尔信息矩阵的低&lt;a href=&quot;https://en.wikipedia.org/wiki/Rank_(linear_algebra)&quot;>;秩&lt;/a>;结构来实现的。这种重新表述可以被视为稀疏&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;>;线性回归&lt;/a>;问题，其中每个回归系数对应于神经网络中的特定权重。获得此回归问题的解决方案后，设置为零的系数将对应于应修剪的权重。我们的回归数据矩阵是 (&lt;em>;n&lt;/em>; x &lt;em>;p&lt;/em>;)，其中 &lt;em>;n&lt;/em>; 是批次（子样本）大小，&lt;em>;p&lt;/em>; em>; 是原始网络中的权重数量。通常&lt;em>;n&lt;/em>; &lt;&lt; &lt;em>;p&lt;/em>;，因此使用此数据矩阵进行存储和操作比使用 (&lt;em>;p&lt;/em>; x &lt;em>;p&lt;/em>;) Hessian 操作的常见修剪方法更具可扩展性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdK vBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s1601/image3.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUf ysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;CHITA 将二次损失近似重新表述为线性回归 (LR) 问题，这需要昂贵的 Hessian 矩阵。 LR 的数据矩阵在 &lt;em>;p&lt;/em>; 中呈线性，这使得重构比原始二次近似更具可扩展性。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;可扩展的优化算法&lt;/h2>; &lt;p>; CHITA 在以下稀疏性约束下将剪枝减少为线性回归问题：最多 &lt; em>;k&lt;/em>; 个回归系数可以不为零。为了获得此问题的解决方案，我们考虑对著名的&lt;a href=&quot;https://arxiv.org/pdf/0805.0510.pdf&quot;>;迭代硬阈值&lt;/a>; (IHT) 算法进行修改。 IHT 执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;>;梯度下降&lt;/a>;，每次更新后执行以下后处理步骤：Top-&lt;em >;k&lt;/em>;（即，具有最大幅度的&lt;em>;k&lt;/em>;系数）设置为零。 IHT 通常会为问题提供良好的解决方案，并且它会迭代探索不同的剪枝候选对象并联合优化权重。 &lt;/p>; &lt;p>; 由于问题的规模，具有恒定&lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;>;学习率&lt;/a>;的标准 IHT 可能会非常慢收敛。为了更快地收敛，我们开发了一种新的&lt;a href=&quot;https://en.wikipedia.org/wiki/Line_search&quot;>;线搜索&lt;/a>;方法，该方法利用问题结构来找到合适的学习率，即导致损失足够大的减少。我们还采用了多种计算方案来提高 CHITA 的效率和二阶近似的质量，从而产生了一个改进的版本，我们称之为 CHITA++。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>; 我们将 CHITA 的运行时间和准确性与几种状态进行比较使用不同架构的最先进的修剪方法，包括 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/1704.04861 &quot;>;移动网络&lt;/a>;。 &lt;/p>; &lt;p>; &lt;strong>;运行时间&lt;/strong>;：CHITA 比执行联合优化的同类方法更具可扩展性（而不是单独修剪权重）。例如，CHITA 在剪枝 ResNet 时的加速比可以达到 1000 倍以上。 &lt;/p>; &lt;p>; &lt;strong>;后剪枝精度&lt;/strong>;：&lt;strong>; &lt;/strong>;下面，我们比较了 CHITA 和 CHITA++ 与幅度剪枝（MP）的性能，&lt;a href=&quot;https: //arxiv.org/abs/2004.14340&quot;>;Woodfisher&lt;/a>; (WF) 和&lt;a href=&quot;https://arxiv.org/abs/2203.04466&quot;>;组合脑外科医生&lt;/a>; (CBS)，用于修剪 70% 的模型权重。总体而言，我们看到 CHITA 和 CHITA++ 取得了良好的改进。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1 aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz- KwvpQVIJKDEnkg_/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlk Fi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a >;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ResNet20 上各种方法的后剪枝精度。报告修剪 70% 模型权重的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-标题容器&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_ AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img边框=“0”数据原始高度=“742”数据原始宽度=“1200”高度=“396”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA -5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-thHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-CP-w2OUUDsPUNMBhGCTkSBFjV6ODFY60 K-VCFnGENDN4LtfA/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MobileNet 上各种方法的后剪枝精度。报告修剪 70% 模型权重的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 接下来，我们报告修剪更大网络的结果：ResNet50（在此网络上，一些ResNet20 图中列出的方法无法扩展）。这里我们与幅度剪枝和&lt;a href=&quot;https://arxiv.org/abs/2107.03356&quot;>;M-FAC&lt;/a>;进行比较。下图显示，CHITA 在各种稀疏度水平下实现了更好的测试精度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU 5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/s1050/image6.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;784&quot; data-original-width=&quot;1050&quot; height=&quot;478&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6J MfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/w640-h478/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;测试使用不同方法获得的修剪网络的准确性。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论、局限性和未来工作&lt;/h2>; &lt;p>; 我们提出了 CHITA，一种基于优化的方法用于修剪预先训练的神经网络。 CHITA 通过有效地使用二阶信息并借鉴组合优化和高维统计的思想，提供可扩展性和有竞争力的性能。 &lt;/p>; &lt;p>; CHITA 专为&lt;em>;非结构化修剪&lt;/em>;而设计，可以去除任何重量。理论上，非结构化剪枝可以显着降低计算要求。然而，在实践中实现这些减少需要支持稀疏计算的特殊软件（可能还有硬件）。相比之下，结构化修剪会删除神经元等整个结构，可能会提供在通用软件和硬件上更容易实现的改进。将 CHITA 扩展到结构化修剪将会很有趣。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是 Google 之间研究合作的一部分和麻省理工学院。感谢 Rahul Mazumder、Natalia Ponomareva、Wenyu Chen、Xiang Men、Zhe Zhu 和 Sergei Vassivitskii 在准备这篇文章和论文时提供的帮助。同时感谢 John Guilyard 在本文中创建了图形。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4779594044050650231/comments/default&quot; rel= &quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/neural-network-pruning-with.html#comment -form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel= “编辑”类型=“application/atom+xml”/>;&lt;link href=“http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231”rel=“self”type=“application/atom” +xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/neural-network-pruning-with.html&quot; rel=&quot;alternate&quot; title=&quot;组合优化神经网络剪枝&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS 2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K /s72-c/CHITA%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr ：总计>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8689679451447865270&lt;/id>;&lt;发布>;2023-08-15T12:59:00.001-07:00&lt;/已发布>;&lt;更新>;2023-08-15T13:00:54.887-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt; /类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“推荐系统”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom” /ns#&quot; term=&quot;社交网络&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;研究：社会意识暂时因果解码器推荐系统&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;发布者：Google 研究工程师 Eltayeb Ahmed 和高级研究科学家 Subhrajit Roy&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt -PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx 8AO1_/s837/study-hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 阅读对于年轻学生有很多好处，例如&lt;a href=&quot;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/284286/reading_for_pleasure。 pdf&quot;>;更好的语言和生活技能&lt;/a>;，并且快乐阅读已被证明与&lt;a href=&quot;https://files.eric.ed.gov/fulltext/ED541404.pdf&quot;>;学业成功相关&lt; /a>;.此外，学生们还表示，阅读&lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/1361454042000312284&quot;>;情绪健康得到改善&lt;/a>;，并且&lt;a href=&quot;https:// eric.ed.gov/?id=ED496343&quot;>;更好的常识和对其他文化的更好的理解&lt;/a>;。由于线上和线下有大量的阅读材料，找到适合年龄的、相关的和引人入胜的内容可能是一项具有挑战性的任务，但帮助学生做到这一点是让他们参与阅读的必要步骤。向学生提供相关阅读材料的有效推荐有助于学生持续阅读，而这正是机器学习 (ML) 可以提供帮助的地方。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 机器学习已广泛应用于构建&lt;a href=&quot;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592 -5&quot;>;推荐系统&lt;/a>;，适用于各种类型的数字内容，从视频到书籍再到电子商务项目。推荐系统在一系列数字平台上使用，以帮助向用户展示相关且有吸引力的内容。在这些系统中，机器学习模型经过训练，可以根据用户偏好、用户参与度和推荐项目单独向每个用户推荐项目。这些数据为模型提供了强大的学习信号，使其能够推荐可能感兴趣的项目，从而改善用户体验。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2306.07946&quot;>;研究：社交意识暂时因果解码器推荐系统&lt;/a>;”中，我们提出了一个有声读物的内容推荐系统在教育环境中考虑阅读的社会性质。我们与 &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 合作开发了 STUDY 算法，这是一个教育非营利组织，旨在促进阅读障碍学生的阅读，通过学校向学生提供有声读物广泛的订阅计划。利用 Learning Ally 图书馆中的各种有声读物，我们的目标是帮助学生找到合适的内容，以帮助提高他们的阅读体验和参与度。由于一个人的同龄人当前正在阅读的内容会对他们感兴趣的阅读内容产生重大影响，因此我们共同处理同一教室中学生的阅读参与历史。这使得我们的模型能够从有关学生本地社交群体（在本例中为他们的教室）当前趋势的实时信息中受益。 &lt;/p>; &lt;br />; &lt;h2>;数据&lt;/h2>; &lt;p>; &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 拥有一个大型数字图书馆，其中包含针对以下人群的精选有声读物：学生，使其非常适合构建社会推荐模型，以帮助提高学生的学习成果。我们收到了两年的匿名有声读物消费数据。数据中的所有学生、学校和分组都是匿名的，仅通过随机生成的 ID 进行识别，无法通过 Google 追溯到真实实体。此外，所有潜在可识别元数据仅以聚合形式共享，以保护学生和机构不被重新识别。这些数据包括学生与有声读物互动的带时间戳的记录。对于每次交互，我们都有一个匿名的学生 ID（包括学生的年级和匿名的学校 ID）、有声读物标识符和日期。虽然许多学校将单一年级的学生分布在多个教室中，但我们利用此元数据做出简化的假设，即同一所学校和同一年级的所有学生都在同一教室。虽然这为建立更好的社交推荐模型提供了基础，但值得注意的是，这并不能让我们重新识别个人、班级群体或学校。 &lt;/p>; &lt;br />; &lt;h2>;STUDY 算法&lt;/h2>; &lt;p>; 我们将推荐问题描述为&lt;a href=&quot;https://arxiv.org/abs/2104.10584&quot;>;点击率&lt;/a>; 预测问题，我们对用户与每个特定项目交互的条件概率进行建模，条件是 1）用户和项目特征以及 2）当前用户的项目交互历史序列。 &lt;a href=&quot;https://arxiv.org/abs/1905.06874&quot;>;之前的工作&lt;/a>;建议&lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer&lt;基于 /a>; 的模型是 Google Research 开发的一种广泛使用的模型类，非常适合对这个问题进行建模。当单独处理每个用户时，这将成为一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;自回归序列建模问题&lt;/a>;。我们使用这个概念框架来建模我们的数据，然后扩展这个框架来创建研究方法。 &lt;/p>; &lt;p>; 虽然这种点击率预测方法可以对单个用户过去和未来的项目偏好之间的依赖关系进行建模，并且可以在训练时学习用户之间的相似性模式，但它无法在推理时对不同用户之间的依赖关系进行建模时间。为了认识阅读的社会性并弥补这一缺陷，我们开发了 STUDY 模型，该模型将每个学生阅读的多个书籍序列连接成一个序列，该序列收集单个教室中多个学生的数据。 &lt;/p>; &lt;p>; 然而，如果要通过 Transformer 建模，这种数据表示需要仔细研究。在 Transformer 中，注意力掩码是控制哪些输入可用于通知哪些输出的预测的矩阵。使用序列中的所有先验标记来通知输出预测的模式导致传统上在因果解码器中发现的上三角注意矩阵。然而，由于输入 STUDY 模型的序列不是按时间排序的，即使它的每个组成子序列都是标准的&lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;>;因果解码器&lt;/a>;不再适合这个序列。当尝试预测每个标记时，模型不允许关注序列中位于其之前的每个标记；其中一些令牌可能具有较晚的时间戳，并且包含在部署时不可用的信息。 &lt;br />; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tq Rd-ZWDXOCRpHAy0FZE9sV0xB8reyJ-- Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/s1787/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1651&quot; data-original-width = =第1787章pHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/w400-h370/image1.png&quot;宽度=&quot; 400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在此图中，我们显示了通常用于因果解码器。每列代表一个输出，每列代表一个输出。特定位置处的矩阵条目的值为 1（显示为蓝色）表示模型在预测相应列的输出时可以观察到该行的输入，而值为 0（显示为白色）表示相反的情况.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; STUDY 模型建立在因果变换器的基础上，通过将三角矩阵注意掩码替换为灵活的注意掩码，其值基于时间戳，以允许关注不同的子序列。与常规 Transformer 相比，常规 Transformer 不允许跨不同子序列进行关注，并且在序列内具有三角矩阵掩码，而 STUDY 在序列内维护因果三角关注矩阵，并且跨序列具有灵活的值，其值取决于时间戳。因此，序列中任何输出点的预测都是由过去相对于当前时间点发生的所有输入点通知的，无论它们是出现在序列中当前输入之前还是之后。这种因果约束很重要，因为如果不在训练时强制执行，模型可能会学习使用未来的信息进行预测，而这对于现实世界的部署来说是不可用的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6Zm UAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s1104/image3.jpg&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFHANO2eJdeFMIWt7K 58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;在（a）中，我们展示了一个具有因果注意力的顺序自回归变压器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQf vFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s1035/Study.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1035&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB 8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s16000/Study.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;在 (a) 中，我们展示了一个具有因果注意力的顺序自回归转换器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; !--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt; tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2 _wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/s1104/image3.png&quot;样式= “左边距：自动；右边距：自动；”&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;548&quot; height=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9Fd caJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/w318-h640/image3.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>; &lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在 (a) 中，我们展示了一个具有因果注意力的顺序自回归转换器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>; 我们使用 Learning Ally 数据集来训练 STUDY 模型以及多个基线以进行比较。我们实现了一个自回归点击率转换器解码器，我们将其称为“个体”，一个&lt;em>;k&lt;/em>;最近邻基线（KNN），以及一个可比较的社交基线，社交注意力记忆网络（SAMN） 。我们使用第一学年的数据进行训练，并使用第二学年的数据进行验证和测试。 &lt;/p>; &lt;p>; 我们通过测量用户实际交互的下一个项目在模型的前 &lt;em>;n&lt;/em>; 个推荐中的时间百分比来评估这些模型，即，hits@&lt;em>;n， &lt;/em>;对于&lt;em>;n&lt;/em>;的不同值。除了在整个测试集上评估模型之外，我们还报告了模型在测试集的两个子集上的得分，这两个子集比整个数据集更具挑战性。我们观察到，学生通常会在多个会话中与有声读物进行交互，因此简单地推荐用户最近读过的一本书将是一个强有力的简单推荐。因此，第一个测试子集（我们称之为“非延续”）是指当学生与与之前交互不同的书籍进行交互时，我们仅查看每个模型在推荐上的表现。我们还观察到学生会重温他们过去读过的书籍，因此通过将针对每个学生的推荐仅限于他们过去读过的书籍，可以在测试集上取得出色的表现。尽管向学生推荐旧的收藏夹可能很有价值，但推荐系统的大部分价值来自于向用户展示新的和未知的内容。为了衡量这一点，我们在学生第一次与标题交互的测试集子集上评估模型。我们将这个评估子集命名为“小说”。 &lt;/p>; &lt;p>; 我们发现，在我们评估的几乎每个切片中，STUDY 都优于所有其他测试模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr 7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/s1526/Study-img2.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1486&quot; data-original-width=&quot;1526&quot; height=&quot;390&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx 0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/w400-h390/Study-img2.png&quot;宽度=“400”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在此图中，我们比较了 Study、Individual、KNN 和 SAMN 四种模型的性能。我们用 hist@5 来衡量性能，即模型在模型的前 5 个推荐中建议用户阅读的下一个标题的可能性有多大。我们在整个测试集（全部）以及新颖的和非连续的分割上评估模型。我们发现 STUDY 在所有分组中始终优于其他三个模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;适当分组的重要性&lt;/h2>; &lt;p>; STUDY 算法的核心是将用户组织成组，并在模型的单次前向传递中对同一组中的多个用户进行联合推理。我们进行了一项消融研究，研究了实际分组对模型性能的重要性。在我们提出的模型中，我们将同一年级和学校的所有学生分组在一起。然后，我们对由同一年级和学区的所有学生定义的组进行实验，并将所有学生放入一个组中，并在每次前向传递中使用随机子集。我们还将这些模型与个体模型进行比较以供参考。 &lt;/p>; &lt;p>; 我们发现使用更加本地化的分组更为有效，学校和年级分组的效果优于地区和年级分组。这支持了这样的假设：学习模型之所以成功，是因为阅读等活动的社会性质——人们的阅读选择可能与周围人的阅读选择相关。这两种模型都优于其他两种模型（单组和个人），其中不使用年级水平对学生进行分组。这表明来自具有相似阅读水平和兴趣的用户的数据有利于性能。 &lt;/p>; &lt;br />; &lt;h2>;未来的工作&lt;/h2>; &lt;p>; 这项工作仅限于为假设社交关系同质的用户群进行推荐建模。将来，对关系不均匀的用户群体进行建模将是有益的，即，存在明显不同类型的关系或已知不同关系的相对强度或影响。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作涉及由研究人员、软件工程师和教育主题专家组成的多学科团队的协作努力。我们感谢我们的合著者：来自 Google 的 Diana Mincu、Lauren Harrell 和 Katherine Heller。我们还要感谢 Learning Ally 的同事 Jeff Ho、Akshat Shah、Erin Walker 和 Tyler Bastian，以及 Google 的合作者 Marc Repnyek、Aki Estrella、Fernando Diaz、Scott Sanner、Emily Salkey 和 Lev Proleev。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8689679451447865270/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; 类型=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;链接 href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai .googleblog.com/2023/08/study-socially-aware-temporally-causal.html&quot; rel=&quot;alternate&quot; title=&quot;STUDY：社交意识时间因果解码器推荐系统&quot; type=&quot;text/html&quot;/>;&lt;author >;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16 “rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>;&lt;/gd :image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqa JM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s72-c/study-hero.png “ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-3240052415427459327&lt;/id>;&lt;发布>;2023-08-09T11:32:00.001-07:00&lt;/发布>;&lt;更新>;2023-08-09T12 :26:53.831-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言理解&quot;>;&lt;/category>;&lt;title type=&quot;text &quot;>;文档理解方面的进展&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Athena 团队 Google 研究部软件工程师 Sandeep Tata&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85z UbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s320/Document%20understanding%20hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 过去几年，能够自动处理复杂业务文档并将其转换为结构化对象的系统取得了快速进展。一个可以从收据、保险报价等文档中&lt;a href=&quot;https://ai.googleblog.com/2020/06/extracting-structured-data-from.html&quot;>;自动提取数据&lt;/a>;的系统和财务报表，通过避免容易出错的手动工作，有可能显着提高业务工作流程的效率。基于 &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; 架构的最新模型已经展示了&lt;a href=&quot; https://ai.googleblog.com/2022/04/formnet-beyond-sequential-modeling-for.html&quot;>;准确度显着提升&lt;/a>;。更大的模型，例如 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>;，也被用来进一步简化这些业务工作流程。然而，学术文献中使用的数据集未能捕捉到现实世界用例中遇到的挑战。因此，学术基准报告了很强的模型准确性，但这些相同的模型在用于复杂的现实应用程序时表现不佳。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;VRDU：视觉丰富文档理解的基准&lt; /a>;”，在 &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>; 上发布，我们宣布发布新的&lt;a href=&quot;https://research.google /resources/datasets/visually-rich-document-understanding/&quot;>;视觉丰富文档理解&lt;/a>; (VRDU) 数据集旨在弥补这一差距，帮助研究人员更好地跟踪文档理解任务的进展。根据经常使用文档理解模型的现实文档类型，我们列出了良好的文档理解基准的五个要求。然后，我们描述了研究界当前使用的大多数数据集为何无法满足其中一项或多项要求，而 VRDU 却满足了所有这些要求。我们很高兴地宣布公开发布 VRDU &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;数据集&lt;/a>;和&lt;a href=&quot;https ://github.com/google-research-datasets/vrdu&quot;>;评估代码&lt;/a>;，采用&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;知识共享许可&lt;/a>;一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;基准要求&lt;/h2>; &lt;p>; 首先，我们比较了最先进的模型准确性（例如，使用 &lt;a href=&quot;https://arxiv.org/abs/2203.08411&quot;>;FormNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2012.14740&quot;>;LayoutLMv2&lt;/ a>;）关于现实世界用例与学术基准（例如，&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;、&lt;a href=&quot;https://openreview.a>;） net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;）。我们观察到，最先进的模型与学术基准结果不符，并且在现实世界中的准确性要低得多。接下来，我们将经常使用文档理解模型的典型数据集与学术基准进行比较，并确定了五个数据集要求，使数据集能够更好地捕获现实应用程序的复杂性：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;丰富模式：&lt;/strong>;在实践中，我们看到了各种用于结构化提取的丰富模式。实体具有不同的数据类型（数字、字符串、日期等），这些数据类型可能是必需的、可选的或在单个文档中重复，甚至可能是嵌套的。像（标题、问题、答案）这样的简单平面模式的提取任务并不能反映实践中遇到的典型问题。 &lt;/li>;&lt;li>;&lt;strong>;布局丰富的文档：&lt;/strong>;文档应该具有复杂的布局元素。实际设置中的挑战来自以下事实：文档可能包含表格、键值对、在单列和双列布局之间切换、不同部分具有不同的字体大小、包括带有标题甚至脚注的图片。将此与大多数文档以句子、段落和带有节标题的章节组织的数据集进行对比 - 这些文档通常是 &lt;a href=&quot;https://arxiv.org/abs 上经典自然语言处理文献的焦点/2007.14062&quot;>;长&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2004.08483&quot;>;输入&lt;/a>;。 &lt;/li>;&lt;li>;&lt;strong>;多样化模板：&lt;/strong>;基准测试应包括不同的结构布局或模板。对于大容量模型来说，通过记忆结构从特定模板中提取数据是微不足道的。然而，在实践中，人们需要能够推广到新的模板/布局，这是基准测试中的训练-测试分割应该衡量的一种能力。 &lt;/li>;&lt;li>;&lt;strong>;高质量 OCR&lt;/strong>;：文档应具有高质量&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;光学字符识别&lt;/a>; (OCR) 结果。我们此基准测试的目标是专注于 VRDU 任务本身，并排除 OCR 引擎选择带来的可变性。 &lt;/li>;&lt;li>;&lt;strong>;令牌级注释&lt;/strong>;：文档应包含可以映射回相应输入文本的真实注释，以便每个令牌都可以注释为相应实体的一部分。这与简单地提供要为实体提取的值的文本形成对比。这是生成干净的训练数据的关键，我们不必担心与给定值的偶然匹配。例如，在某些收据中，如果税额为零，“税前总计”字段可能具有与“总计”字段相同的值。具有令牌级别注释会阻止我们生成训练数据，其中匹配值的两个实例都被标记为“total”字段的真实值，从而产生嘈杂的示例。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn 5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s1600 /Benchmark%20GIF.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6Ai WX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s16000/Benchmark%20GIF.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;VRDU 数据集和任务&lt;/h2>; &lt;p>; VRDU 数据集是两个公开可用数据集的组合，&lt;a href=&quot;https://efile.40%;&quot;>; fara.gov/ords/fara/f?p=1235:10&quot;>;注册表&lt;/a>;和&lt;a href=&quot;https://publicfiles.fcc.gov/&quot;>;广告购买表单&lt;/a>;。这些数据集提供了代表现实世界用例的示例，并满足上述五个基准要求。 &lt;/p>; &lt;p>; Ad-buy Forms 数据集包含 641 个包含政治广告详细信息的文档。每份文件都是由电视台和竞选团队签署的发票或收据。文档使用表格、多列和键值对来记录广告信息，例如产品名称、播放日期、总价、发布日期和时间。 &lt;/p>; &lt;p>; 登记表数据集包含 1,915 个文档，其中包含有关外国代理人在美国政府登记的信息。每份文件都记录了涉及需要公开披露的活动的外国代理人的基本信息。 Contents include the name of the registrant, the address of related bureaus, the purpose of activities, and other details. &lt;/p>; &lt;p>; We gathered a random sample of documents from the public &lt;a href=&quot;https://www.fcc.gov/&quot;>;Federal Communications Commission&lt;/a>; (FCC) and &lt;a href=&quot;https://www.justice.gov/nsd-fara&quot;>;Foreign Agents Registration Act&lt;/a>; (FARA) sites, and converted the images to text using &lt;a href=&quot;https://cloud.google.com/&quot;>;Google Cloud&#39;s&lt;/a>; &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;OCR&lt;/a>;. We discarded a small number of documents that were several pages long and the processing did not complete in under two minutes. This also allowed us to avoid sending very long documents for manual annotation — a task that can take over an hour for a single document. Then, we defined the schema and corresponding labeling instructions for a team of annotators experienced with document-labeling tasks. &lt;/p>; &lt;p>; The annotators were also provided with a few sample labeled documents that we labeled ourselves. The task required annotators to examine each document, draw a bounding box around every occurrence of an entity from the schema for each document, and associate that bounding box with the target entity. After the first round of labeling, a pool of experts were assigned to review the results. The corrected results are included in the published VRDU dataset. Please see the &lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;paper&lt;/a>; for more details on the labeling protocol and the schema for each dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s1600/Chart.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s16000/Chart.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Existing academic benchmarks (&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;, &lt;a href=&quot;https://openreview.net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;, &lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-NDA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-Charity&lt;/a>;, &lt;a href=&quot;https://wandb.ai/stacey/deepform_v1/reports/DeepForm-Understand-Structured-Documents-at-Scale--VmlldzoyODQ3Njg&quot;>;DeepForm&lt;/a>;) fall-short on one or more of the five requirements we identified for a good document understanding benchmark. VRDU satisfies all of them. See our &lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;paper&lt;/a>; for background on each of these datasets and a discussion on how they fail to meet one or more of the requirements.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We built four different model training sets with 10, 50, 100, and 200 samples respectively. Then, we evaluated the VRDU datasets using three tasks (described below): (1) Single Template Learning, (2) Mixed Template Learning, and (3) Unseen Template Learning. For each of these tasks, we included 300 documents in the testing set. We evaluate models using the &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1 score&lt;/a>; on the testing set. &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Single Template Learning&lt;/em>; (STL): This is the simplest scenario where the training, testing, and validation sets only contain a single template. This simple task is designed to evaluate a model&#39;s ability to deal with a fixed template. Naturally, we expect very high F1 scores (0.90+) for this task. &lt;/li>;&lt;li>;&lt;em>;Mixed Template Learning&lt;/em>; (MTL): This task is similar to the task that most related papers use: the training, testing, and validation sets all contain documents belonging to the same set of templates. We randomly sample documents from the datasets and construct the splits to make sure the distribution of each template is not changed during sampling. &lt;/li>;&lt;li>;&lt;em>;Unseen Template Learning&lt;/em>; (UTL): This is the most challenging setting, where we evaluate if the model can generalize to unseen templates. For example, in the Registration Forms dataset, we train the model with two of the three templates and test the model with the remaining one. The documents in the training, testing, and validation sets are drawn from disjoint sets of templates. To our knowledge, previous benchmarks and datasets do not explicitly provide such a task designed to evaluate the model&#39;s ability to generalize to templates not seen during training. &lt;/li>; &lt;/ul>; &lt;p>; The objective is to be able to evaluate models on their data efficiency. In our &lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;paper&lt;/a>;, we compared two recent models using the STL, MTL, and UTL tasks and made three observations. First, unlike with other benchmarks, VRDU is challenging and shows that models have plenty of room for improvements. Second, we show that few-shot performance for even state-of-the-art models is surprisingly low with even the best models resulting in less than an F1 score of 0.60. Third, we show that models struggle to deal with structured repeated fields and perform particularly poorly on them. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We release the new &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;Visually Rich Document Understanding&lt;/a>; (VRDU) dataset that helps researchers better track progress on document understanding tasks. We describe why VRDU better reflects practical challenges in this domain. We also present experiments showing that VRDU tasks are challenging, and recent models have substantial headroom for improvements compared to the datasets typically used in the literature with F1 scores of 0.90+ being typical. We hope the release of the VRDU dataset and evaluation code helps research teams advance the state of the art in document understanding. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Many thanks to Zilong Wang, Yichao Zhou, Wei Wei, and Chen-Yu Lee, who co-authored the paper along with Sandeep Tata. Thanks to Marc Najork, Riham Mansour and numerous partners across Google Research and the Cloud AI team for providing valuable insights. Thanks to John Guilyard for creating the animations in this post. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3240052415427459327/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/advances-in-document-understanding.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/advances-in-document-understanding.html&quot; rel=&quot;alternate&quot; title=&quot;Advances in document understanding&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s72-c/Document%20understanding%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-595171581401765238&lt;/id>;&lt;published>;2023-08-08T14:02:00.000-07:00&lt;/published>;&lt;updated>;2023-08-08T14:02:34.659-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AdaTape: Foundation model with adaptive computation and dynamic read-and-write&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Fuzhao Xue, Research Intern, and Mostafa Dehghani, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIqJhdQrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudIEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s2000/adatape.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;>;Adaptive computation&lt;/a>; refers to the ability of a machine learning system to adjust its behavior in response to changes in the environment. While conventional neural networks have a fixed function and computation capacity, ie, they spend the same number of FLOPs for processing different inputs, a model with adaptive and dynamic computation modulates the computational budget it dedicates to processing each input, depending on the complexity of the input. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Adaptive computation in neural networks is appealing for two key reasons. First, the mechanism that introduces adaptivity provides an &lt;a href=&quot;https://en.wikipedia.org/wiki/Inductive_bias&quot;>;inductive bias&lt;/a>; that can play a key role in solving some challenging tasks. For instance, enabling different numbers of computational steps for different inputs can be crucial in solving arithmetic problems that require modeling hierarchies of different depths. Second, it gives practitioners the ability to tune the cost of inference through greater flexibility offered by dynamic computation, as these models can be adjusted to spend more FLOPs processing a new input. &lt;/p>; &lt;p>; Neural networks can be made adaptive by using different functions or computation budgets for various inputs. A deep neural network can be thought of as a function that outputs a result based on both the input and its parameters. To implement adaptive function types, a subset of parameters are selectively activated based on the input, a process referred to as conditional computation.在 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;mixture-of-experts&lt;/a>; 的研究中探索了基于函数类型的自适应性，其中每个输入的稀疏激活参数样本是通过路由确定的。 &lt;/p>; &lt;p>; Another area of research in adaptive computation involves dynamic computation budgets. Unlike in standard neural networks, such as &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, and &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;ViT&lt;/a>;, whose computation budget is fixed for different samples, &lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;recent research&lt;/a>; has demonstrated that adaptive computation budgets can improve performance on tasks where transformers fall short. Many of these works achieve adaptivity by using dynamic depth to allocate the computation budget. For example, the &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;>;Adaptive Computation Time&lt;/a>; (ACT) algorithm was proposed to provide an adaptive computational budget for recurrent neural networks. The &lt;a href=&quot;https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html&quot;>;Universal Transformer&lt;/a>; extends the ACT algorithm to transformers by making the computation budget dependent on the number of transformer layers used for each input example or token. Recent studies, like &lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;PonderNet&lt;/a>;, follow a similar approach while improving the dynamic halting mechanisms. &lt;/p>; &lt;p>; In the paper “&lt;a href=&quot;https://arxiv.org/abs/2301.13195&quot;>;Adaptive Computation with Elastic Input Sequence&lt;/a>;”, we introduce a new model that utilizes adaptive computation, called &lt;em>;AdaTape&lt;/em>;. This model is a Transformer-based architecture that uses a dynamic set of tokens to create elastic input sequences, providing a unique perspective on adaptivity in comparison to previous works. AdaTape uses an adaptive tape reading mechanism to determine a varying number of tape tokens that are added to each input based on input&#39;s complexity. AdaTape is very simple to implement, provides an effective knob to increase the accuracy when needed, but is also much more efficient compared to &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;other adaptive baselines&lt;/a>; because it directly injects adaptivity into the input sequence instead of the model depth. Finally, Adatape offers better performance on standard tasks, like image classification, as well as algorithmic tasks, while maintaining a favorable quality and cost tradeoff. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Adaptive computation transformer with elastic input sequence&lt;/h2>; &lt;p>; AdaTape uses both the adaptive function types and a dynamic computation budget. Specifically, for a batch of input sequences after tokenization (eg, a linear projection of non-overlapping patches from an image in the vision transformer), AdaTape uses a vector representing each input to dynamically select a variable-sized sequence of tape tokens. &lt;/p>; &lt;p>; AdaTape uses a bank of tokens, called a “tape bank”, to store all the candidate tape tokens that interact with the model through the adaptive tape reading mechanism. We explore two different methods for creating the tape bank: an input-driven bank and a learnable bank. &lt;/p>; &lt;p>; 输入驱动库的总体思想是从输入中提取一组令牌，同时采用与原始模型令牌生成器不同的方法将原始输入映射到输入标记的序列。 This enables dynamic, on-demand access to information from the input that is obtained using a different point of view, eg, a different image resolution or a different level of abstraction. &lt;/p>; &lt;p>; In some cases, tokenization in a different level of abstraction is not possible, thus an input-driven tape bank is not feasible, such as when it&#39;s difficult to further split each node in a &lt;a href=&quot;https://arxiv.org/abs/2012.09699&quot;>;graph transformer&lt;/a>;. To address this issue, AdaTape offers a more general approach for generating the tape bank by using a set of trainable vectors as tape tokens. This approach is referred to as the &lt;em>;learnable bank&lt;/em>; and can be viewed as an embedding layer where the model can dynamically retrieve tokens based on the complexity of the input example. The learnable bank enables AdaTape to generate a more flexible tape bank, providing it with the ability to dynamically adjust its computation budget based on the complexity of each input example, eg, more complex examples retrieve more tokens from the bank, which let the model not only use the knowledge stored in the bank, but also spend more FLOPs processing it, since the input is now larger. &lt;/p>; &lt;p>; Finally, the selected tape tokens are appended to the original input and fed to the following transformer layers. For each transformer layer, the same multi-head attention is used across all input and tape tokens. However, two different feed-forward networks (FFN) are used: one for all tokens from the original input and the other for all tape tokens. We observed slightly better quality by using separate feed-forward networks for input and tape tokens. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfdaEU1-w8kv7USzuc-VBwGumvHxfccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s1786/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;871&quot; data-original-width=&quot;1786&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfdaEU1-w8kv7USzuc-VBwGumvHxfccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of AdaTape. For different samples, we pick a variable number of different tokens from the tape bank. The tape bank can be driven from input, eg, by extracting some extra fine-grained information or it can be a set of trainable vectors. Adaptive tape reading is used to recursively select different sequences of tape tokens, with variable lengths, for different inputs. These tokens are then simply appended to inputs and fed to the transformer encoder.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;AdaTape provides helpful inductive bias&lt;/h2>; &lt;p>; We evaluate AdaTape on parity, a very challenging task for the standard Transformer, to study the effect of inductive biases in AdaTape. With the parity task, given a sequence 1s, 0s, and -1s, the model has to predict the evenness or oddness of the number of 1s in the sequence. Parity is the simplest non-counter-free or &lt;a href=&quot;https://arxiv.org/abs/2009.11264&quot;>;periodic regular language&lt;/a>;, but perhaps surprisingly, the task is unsolvable by the standard Transformer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3EoYw4UhzcKK0fGIscOJSv36zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/s866/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;432&quot; data-original-width=&quot;866&quot; height=&quot;319&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3EoYw4UhzcKK0fGIscOJSv36zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/w640-h319/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Evaluation on the parity task. The standard Transformer and Universal Transformer were unable to perform this task, both showing performance at the level of a random guessing baseline.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Despite being evaluated on short, simple sequences, both the standard Transformer and Universal Transformers were unable to perform the parity task as they are unable to maintain a counter within the model. However, AdaTape outperforms all baselines, as it incorporates a lightweight recurrence within its input selection mechanism, providing an inductive bias that enables the implicit maintenance of a counter, which is not possible in standard Transformers. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation on image classification&lt;/h2>; &lt;p>; We also evaluate AdaTape on the image classification task. To do so, we trained AdaTape on &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet-1K&lt;/a>; from scratch. The figure below shows the accuracy of AdaTape and the baseline methods, including &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>;, and the Universal Transformer ViT (UViT and U2T) versus their speed (measured as number of images, processed by each code, per second). In terms of quality and cost tradeoff, AdaTape performs much better than the alternative adaptive transformer baselines. In terms of efficiency, larger AdaTape models (in terms of parameter count) are faster than smaller baselines. Such results are consistent with the finding from &lt;a href=&quot;https://arxiv.org/abs/2110.12894&quot;>;previous work that&lt;/a>; shows that the adaptive model depth architectures are not well suited for many accelerators, like the TPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7ZIh6Rujxnd-H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H-Hxzek9ziXZsw5adSEMZU/s1473/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;657&quot; data-original-width=&quot;1473&quot; height=&quot;285&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7ZIh6Rujxnd-H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H-Hxzek9ziXZsw5adSEMZU/w640-h285/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We evaluate AdaTape by training on ImageNet from scratch. For &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>;, we not only report their results from the paper but also re-implement A-ViT by training from scratch, ie, A-ViT(Ours).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A study of AdaTape&#39;s behavior&lt;/h2>; &lt;p>; In addition to its performance on the parity task and ImageNet-1K, we also evaluated the token selection behavior of AdaTape with an input-driven bank on the &lt;a href=&quot;https://arxiv.org/abs/1707.02968&quot;>;JFT-300M&lt;/a>; validation set. To better understand the model&#39;s behavior, we visualized the token selection results on the &lt;em>;input-driven bank&lt;/em>; as heatmaps, where lighter colors mean that position is more frequently selected.热图显示 AdaTape 更频繁地选择中心补丁。这与我们的先验知识相一致，因为中心补丁通常包含更多信息，尤其是在具有自然图像的数据集的背景下，其中主要对象位于图像的中间。这一结果凸显了 AdaTape 的智能性，因为它可以有效地识别信息更丰富的补丁并确定优先级，以提高其性能。 &lt;/p>; &lt;p>;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgkR2VcDoKTicWQ_hufXwrlAcPMoeYqcrCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/s839/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;839&quot; height=&quot;270&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgkR2VcDoKTicWQ_hufXwrlAcPMoeYqcrCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/w640-h270/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We visualize the tape token selection heatmap of AdaTape-B/32 (left) and AdaTape-B/16 (right). The hotter / lighter color means the patch at this position is more frequently selected.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; AdaTape is characterized by elastic sequence lengths generated by the adaptive tape reading mechanism.这还引入了一种新的感应偏置，使 AdaTape 能够解决对标准变压器和现有自适应变压器都具有挑战性的任务。通过对图像识别基准进行全面的实验，我们证明了当计算保持不变时，AdaTape 的性能优于标准转换器和自适应架构转换器。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;One of the authors of this post, Mostafa Dehghani, is now at Google DeepMind. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/595171581401765238/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html&quot; rel=&quot;alternate&quot; title=&quot;AdaTape: Foundation model with adaptive computation and dynamic read-and-write&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIqJhdQrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudIEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s72-c/adatape.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4436946873570093049&lt;/id>;&lt;published>;2023-08-03T11:24:00.001-07:00&lt;/published>;&lt;updated>;2023-08-03T11:24:40.465-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Multimodal medical AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Greg Corrado, Head of Health AI, Google Research, and Yossi Matias, VP, Engineering and Research, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjmG9lDTBWdy5kH_bR3-tqN8KzdhgmrLL_N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3/s1600/medpalm.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Medicine is an inherently multimodal discipline. When providing care, clinicians routinely interpret data from a wide range of modalities including medical images, clinical notes, lab tests, electronic health records, genomics, and more. Over the last decade or so, AI systems have achieved expert-level performance on specific tasks &lt;em>;within specific&lt;/em>; modalities — some AI systems &lt;a href=&quot;https://www.nature.com/articles/s41591-019-0447-x&quot;>;processing CT scans&lt;/a>;, while others &lt;a href=&quot;https://jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;analyzing high magnification pathology slides&lt;/a>;, and still others &lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/NEJMc2112090&quot;>;hunting for rare genetic variations&lt;/a>;. The inputs to these systems tend to be complex data such as images, and they typically provide structured outputs, whether in the form of discrete grades or &lt;a href=&quot;https://www.nature.com/articles/s41591-018-0107-6&quot;>;dense image segmentation masks.&lt;/a>; In parallel, the capacities and capabilities of large language models (LLMs) have &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;become so advanced&lt;/a>; that they have demonstrated comprehension and expertise in medical knowledge by both interpreting and responding in plain language. But how do we bring these capabilities together to build medical AI systems that can leverage information from &lt;em>;all&lt;/em>; these sources? &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In today&#39;s blog post, we outline a spectrum of approaches to bringing multimodal capabilities to LLMs and share some exciting results on the tractability of building multimodal medical LLMs, as described in three recent research papers. The papers, in turn, outline how to introduce &lt;em>;de novo&lt;/em>; modalities to an LLM, how to graft a state-of-the-art medical imaging foundation model onto a conversational LLM, and first steps towards building a truly generalist multimodal medical AI system. If successfully matured, multimodal medical LLMs might serve as the basis of new assistive technologies spanning professional medicine, medical research, and consumer applications. As with our prior work, we emphasize the need for careful evaluation of these technologies in collaboration with the medical community and healthcare ecosystem. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;A spectrum of approaches&lt;/h2>; &lt;p>; Several methods for building multimodal LLMs have been proposed in recent months [&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;1&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;2&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;3&lt;/a>;], and no doubt new methods will continue to emerge for some time. For the purpose of understanding the opportunities to bring new modalities to medical AI systems, we&#39;ll consider three broadly defined approaches: tool use, model grafting, and generalist systems. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ZKRMY82JiKLD26G2skhiJEEh8Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s1600/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ZKRMY82JiKLD26G2skhiJEEh8Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The spectrum of approaches to building multimodal LLMs range from having the LLM use existing tools or models, to leveraging domain-specific components with an adapter, to joint modeling of a multimodal model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Tool use&lt;/h2>; &lt;p>; In the &lt;em>;tool use&lt;/em>; approach, one central medical LLM outsources analysis of data in various modalities to a set of software subsystems independently optimized for those tasks: the tools. The common mnemonic example of tool use is teaching an LLM to use a calculator rather than do arithmetic on its own. In the medical space, a medical LLM faced with a chest X-ray could forward that image to a radiology AI system and integrate that response. This could be accomplished via application programming interfaces (APIs) offered by subsystems, or more fancifully, two medical AI systems with different specializations engaging in a conversation. &lt;/p>; &lt;p>; This approach has some important benefits. It allows maximum flexibility and independence between subsystems, enabling health systems to mix and match products between tech providers based on validated performance characteristics of subsystems. Moreover, human-readable communication channels between subsystems maximize auditability and debuggability. That said, getting the communication right between independent subsystems can be tricky, narrowing the information transfer, or exposing a risk of miscommunication and information loss. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Model grafting&lt;/h2>; &lt;p>; A more integrated approach would be to take a neural network specialized for each relevant domain, and adapt it to plug directly into the LLM — &lt;em>;grafting&lt;/em>; the visual model onto the core reasoning agent. In contrast to tool use where the specific tool(s) used are determined by the LLM, in model grafting the researchers may choose to use, refine, or develop specific models during development. In two recent papers from Google Research, we show that this is in fact feasible. Neural LLMs typically process text by first mapping words into a &lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings&quot;>;vector embedding space&lt;/a>;. Both papers build on the idea of mapping data from a new modality into the input word embedding space already familiar to the LLM. The first paper, “&lt;a href=&quot;https://arxiv.org/abs/2307.09018&quot;>;Multimodal LLMs for health grounded in individual-specific data&lt;/a>;”, shows that asthma risk prediction in the &lt;a href=&quot;https://www.ukbiobank.ac.uk/&quot;>;UK Biobank&lt;/a>; can be improved if we first train a neural network classifier to interpret &lt;a href=&quot;https://www.mayoclinic.org/tests-procedures/spirometry/about/pac-20385201&quot;>;spirograms&lt;/a>; (a modality used to assess breathing ability) and then adapt the output of that network to serve as input into the LLM. &lt;/p>; &lt;p>; The second paper, “&lt;a href=&quot;https://arxiv.org/abs/2308.01317&quot;>;ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders&lt;/a>;”, takes this same tack, but applies it to full-scale image encoder models in radiology. Starting with a &lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;foundation model for understanding chest X-rays&lt;/a>;, already shown to be a good basis for building a variety of classifiers in this modality, this paper describes training a lightweight &lt;em>;medical information adapter&lt;/em>; that re-expresses the top layer output of the foundation model as a series of tokens in the LLM&#39;s input embeddings space. Despite fine-tuning neither the visual encoder nor the language model, the resulting system displays capabilities it wasn&#39;t trained for, including &lt;a href=&quot;https://arxiv.org/abs/2210.10163&quot;>;semantic search&lt;/a>; and &lt;a href=&quot;https://www.nature.com/articles/sdata2018251&quot;>;visual question answering&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFM aMMzTSUu4fif7t9SVsr9MduEcISlA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s1600/image3.gif&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFMaMMzTSUu4fif7t9SVsr9MduEc ISlA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;我们嫁接模型的方法是通过训练医疗信息适配器来将现有或改进的图像编码器的输出映射为法学硕士可理解的形式。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 模型嫁接有很多优点。 It uses relatively modest computational resources to train the adapter layers but allows the LLM to build on existing highly-optimized and validated models in each data domain. The modularization of the problem into encoder, adapter, and LLM components can also facilitate testing and debugging of individual software components when developing and deploying such a system. The corresponding disadvantages are that the communication between the specialist encoder and the LLM is no longer human readable (being a series of high dimensional vectors), and the grafting procedure requires building a new adapter for not just every domain-specific encoder, but also every &lt;em>;revision&lt;/em>; of each of those encoders. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Generalist systems&lt;/h2>; &lt;p>; The most radical approach to multimodal medical AI is to build one integrated, fully generalist system natively capable of absorbing information from all sources. In our third paper in this area, “&lt;a href=&quot;https://arxiv.org/abs/2307.14334&quot;>;Towards Generalist Biomedical AI&lt;/a>;”, rather than having separate encoders and adapters for each data modality, we build on &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;PaLM-E&lt;/a>;, a recently published multimodal model that is itself a combination of a single LLM (&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;) and a &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;single vision encoder (ViT)&lt;/a>;. In this set up, text and tabular data modalities are covered by the LLM text encoder, but now all other data are treated as an image and fed to the vision encoder. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42Hepv4DNsEWebRmtorN05xdpkJ2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s1600/image2.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42Hepv4DNsEWebRmtorN05xdpkJ2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same model weights.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We specialize PaLM-E to the medical domain by fine-tuning the complete set of model parameters on medical datasets described in the paper. The resulting generalist medical AI system is a multimodal version of &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM&lt;/a>; that we call Med-PaLM M. The flexible multimodal sequence-to-sequence architecture allows us to interleave various types of multimodal biomedical information in a single interaction. To the best of our knowledge, it is the first demonstration of a single unified model that can interpret multimodal biomedical data and handle a diverse range of tasks using the same set of model weights across all tasks (detailed evaluations in the paper). &lt;/p>; &lt;p>; This generalist-system approach to multimodality is both the most ambitious and simultaneously most elegant of the approaches we describe. In principle, this direct approach maximizes flexibility and information transfer between modalities. With no APIs to maintain compatibility across and no proliferation of adapter layers, the generalist approach has arguably the simplest design. But that same elegance is also the source of some of its disadvantages. Computational costs are often higher, and with a unitary vision encoder serving a wide range of modalities, domain specialization or system debuggability could suffer. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;The reality of multimodal medical AI&lt;/h2>; &lt;p>; To make the most of AI in medicine, we&#39;ll need to combine the strength of expert systems trained with predictive AI with the flexibility made possible through generative AI. Which approach (or combination of approaches) will be most useful in the field depends on a multitude of as-yet unassessed factors. Is the flexibility and simplicity of a generalist model more valuable than the modularity of model grafting or tool use? Which approach gives the highest quality results for a specific real-world use case? Is the preferred approach different for supporting medical research or medical education vs. augmenting medical practice? Answering these questions will require ongoing rigorous empirical research and continued direct collaboration with healthcare providers, medical institutions, government entities, and healthcare industry partners broadly. We look forward to finding the answers together. &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4436946873570093049/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/multimodal-medical-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/multimodal-medical-ai.html&quot; rel=&quot;alternate&quot; title=&quot;Multimodal medical AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjmG9lDTBWdy5kH_bR3-tqN8KzdhgmrLL_N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3/s72-c/medpalm.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5980448298988153366&lt;/id>;&lt;published>;2023-07-26T09:33:00.003-07:00&lt;/published>;&lt;updated>;2023-08-01T08:31:25.216-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;In search of a generalizable method for source-free domain adaptation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eleni Triantafillou, Research Scientist, and Malik Boudiaf, Student Researcher, Google &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4epC-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgyM4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/s1500/notela.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Deep learning has recently made tremendous progress in a wide range of problems and applications, but models often fail unpredictably when deployed in unseen domains or distributions. &lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;Source-free domain adaptation&lt;/a>; (SFDA) is an area of research that aims to design methods for adapting a pre-trained model (trained on a “source domain”) to a new “target domain”, using only unlabeled data from the latter. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Designing adaptation methods for deep models is an important area of research. While the increasing scale of models and training datasets has been a key ingredient to their success, a negative consequence of this trend is that training such models is increasingly computationally expensive, in some cases making large model training &lt;a href=&quot;https://spectrum.ieee.org/deep-learning-computational-cost&quot;>;less accessible&lt;/a>; and unnecessarily &lt;a href=&quot;https://penntoday.upenn.edu/news/hidden-costs-ai-impending-energy-and-resource-strain&quot;>;increasing the carbon footprint&lt;/a>;.&amp;nbsp;One avenue to mitigate this issue is through designing techniques that can leverage and reuse already trained models for tackling new tasks or generalizing to new domains. Indeed, adapting models to new tasks is widely studied under the umbrella of &lt;a href=&quot;https://arxiv.org/abs/1911.02685&quot;>;transfer learning&lt;/a>;. &lt;/p>; &lt;p>; SFDA is a particularly practical area of this research because several real-world applications where adaptation is desired suffer from the unavailability of labeled examples from the target domain. In fact, SFDA is enjoying increasing attention [&lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;2&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2110.04202&quot;>;3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;4&lt;/a>;]. However, albeit motivated by ambitious goals, most SFDA research is grounded in a very narrow framework, considering simple &lt;a href=&quot;https://arxiv.org/abs/2108.13624&quot;>;distribution shifts&lt;/a>; in image classification tasks. &lt;/p>; &lt;p>; In a significant departure from that trend, we turn our attention to the field of bioacoustics, where naturally-occurring distribution shifts are ubiquitous, often characterized by insufficient target labeled data, and represent an obstacle for practitioners. Studying SFDA in this application can, therefore, not only inform the academic community about the generalizability of existing methods and identify open research directions, but can also directly benefit practitioners in the field and aid in addressing one of the biggest challenges of our century: biodiversity preservation. &lt;/p>; &lt;p>; In this post, we announce “&lt;a href=&quot;https://arxiv.org/abs/2302.06658&quot;>;In Search for a Generalizable Method for Source-Free Domain Adaptation&lt;/a>;”, appearing at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023. &lt;/a>;We show that state-of-the-art SFDA methods can underperform or even collapse when confronted with realistic distribution shifts in bioacoustics. Furthermore, existing methods perform differently relative to each other than observed in vision benchmarks, and surprisingly, sometimes perform worse than no adaptation at all. We also propose NOTELA, a new simple method that outperforms existing methods on these shifts while exhibiting strong performance on a range of vision datasets. Overall, we conclude that evaluating SFDA methods (only) on the commonly-used datasets and distribution shifts leaves us with a myopic view of their relative performance and generalizability. To live up to their promise, SFDA methods need to be tested on a wider range of distribution shifts, and we advocate for considering naturally-occurring ones that can benefit high-impact applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Distribution shifts in bioacoustics&lt;/h2>; &lt;p>; Naturally-occurring distribution shifts are ubiquitous in bioacoustics. The largest labeled dataset for bird songs is &lt;a href=&quot;https://www.researchgate.net/publication/280978332_The_Xeno-canto_collection_and_its_relation_to_sound_recognition_and_classification&quot;>;Xeno-Canto&lt;/a>; (XC), a collection of user-contributed recordings of wild birds from across the world. Recordings in XC are “focal”: they target an individual captured in natural conditions, where the song of the identified bird is at the foreground. For continuous monitoring and tracking purposes, though, practitioners are often more interested in identifying birds in &lt;em>;passive recordings &lt;/em>;(“soundscapes”), obtained through omnidirectional microphones. This is a well-documented problem that &lt;a href=&quot;https://www.researchgate.net/publication/344893963_Overview_of_BirdCLEF_2020_Bird_Sound_Recognition_in_Complex_Acoustic_Environments&quot;>;recent&lt;/a>; &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1574954121000273&quot;>;work&lt;/a>; shows is very challenging. Inspired by this realistic application, we study SFDA in bioacoustics using a bird species classifier that was pre-trained on XC as the source model, and several “soundscapes” coming from different geographical locations — &lt;a href=&quot;https://doi.org/10.5281/zenodo.7050013&quot;>;Sierra Nevada&lt;/a>; (S. Nevada); &lt;a href=&quot;https://doi.org/10.1002/ecy.3329&quot;>;Powdermill&lt;/a>; Nature Reserve, Pennsylvania, USA; &lt;a href=&quot;https://doi.org/10.5281/zenodo.7078498&quot;>;Hawai&#39;i&lt;/a>;; Caples Watershed, California, USA; &lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;Sapsucker Woods&lt;/a>;, New York, USA (SSW); and &lt;a href=&quot;https://www.google.com/url?q=https://zenodo.org/record/7525349%23.ZB8z_-xudhE&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1688498539746392&amp;amp;usg=AOvVaw07CsIKIE-dcNyMKFT-n_JT&quot;>;Colombia&lt;/a>; —&amp;nbsp;as our target domains. &lt;/p>; &lt;p>; This shift from the focalized to the passive domain is substantial: the recordings in the latter often feature much lower signal-to-noise ratio, several birds vocalizing at once, and significant distractors and environmental noise, like rain or wind. In addition, different soundscapes originate from different geographical locations, inducing extreme label shifts since a very small portion of the species in XC will appear in a given location. Moreover, as is common in real-world data, both the source and target domains are significantly class imbalanced, because some species are significantly more common than others. In addition, we consider a &lt;em>;multi-label &lt;/em>;classification problem since there may be several birds identified within each recording, a significant departure from the standard single-label image classification scenario where SFDA is typically studied. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeGafX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQOzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s1378/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;1378&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeGafX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQOzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the &quot;focal → soundscapes&quot; shift. In the focalized domain, recordings are typically composed of a single bird vocalization in the foreground, captured with high signal-to-noise ratio (SNR), though there may be other birds vocalizing in the background.&lt;strong>; &lt;/strong>;On the other hand, soundscapes contain recordings from omnidirectional microphones and can be composed of multiple birds vocalizing simultaneously, as well as environmental noises from insects, rain, cars, planes, etc.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Audio files&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;Focal domain&lt;em>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audio controls=&quot;controls&quot; src=&quot;https://storage.googleapis.com/chirp-public-bucket/notela-blog-post/XC417991%20-%20Yellow-throated%20Vireo%20-%20Vireo%20flavifrons.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;Soundscape domain&lt;em>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audio controls=&quot;controls&quot; src=&quot;https://storage.googleapis.com/chirp-public-bucket/notela-blog-post/yetvir-soundscape.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/em>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Spectogram images&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR4VQQSIFLm6u1t0NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/s936/left.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR4VQQSIFLm6u1t0NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/w640-h546/left.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iyjBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTCmlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/s936/right.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iyjBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTCmlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/w640-h546/right.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the distribution shift from the focal domain (&lt;b>;left&lt;/b>;) to the soundscape domain (&lt;b>;right&lt;/b>;), in terms of the audio files (&lt;b>;top&lt;/b>;) and spectrogram images (&lt;b>;bottom&lt;/b>;) of a representative recording from each dataset. Note that in the second audio clip, the bird song is very faint; a common property in soundscape recordings where bird calls aren&#39;t at the “foreground”. Credits: &lt;b>;Left:&lt;/b>; XC &lt;a href=&quot;https://xeno-canto.org/417991&quot;>;recording&lt;/a>; by Sue Riffe (&lt;a href=&quot;https://creativecommons.org/licenses/by-nc-sa/4.0/&quot;>;CC-BY-NC license&lt;/a>;). &lt;b>;Right:&lt;/b>; Excerpt from a recording made available by Kahl, Charif, &amp;amp; Klinck. (2022) &quot;A collection of fully-annotated soundscape recordings from the Northeastern United States&quot; [&lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;link&lt;/a>;] from the SSW soundscape dataset (&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;CC-BY license&lt;/a>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;State-of-the-art SFDA models perform poorly on bioacoustics shifts&lt;/h2>; &lt;p>; As a starting point, we benchmark six state-of-the-art SFDA methods on our bioacoustics benchmark, and compare them to the &lt;em>;non-adapted&lt;/em>; baseline (the source model). Our findings are surprising: without exception, existing methods are unable to consistently outperform the source model on all target domains. In fact, they often underperform it significantly. &lt;/p>; &lt;p>; As an example, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>;, a recent method, aims to make models produce confident predictions for each example by reducing the uncertainty of the model&#39;s output probabilities. While Tent performs well in various tasks, it doesn&#39;t work effectively for our bioacoustics task. In the single-label scenario, minimizing entropy forces the model to choose a single class for each example confidently. However, in our multi-label scenario, there&#39;s no such constraint that any class should be selected as being present. Combined with significant distribution shifts, this can cause the model to collapse, leading to zero probabilities for all classes. Other benchmarked methods like &lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;, &lt;a href=&quot;https://openreview.net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;DUST&lt;/a>; and &lt;a href=&quot;https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;Pseudo-Labelling&lt;/a>;, which are strong baselines for standard SFDA benchmarks, also struggle with this bioacoustics task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj38druynuW9zh26no15rOJCj1aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s1434/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;730&quot; data-original-width=&quot;1434&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj38druynuW9zh26no15rOJCj1aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Evolution of the test &lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Mean_average_precision&quot;>;mean average precision&lt;/a>; (mAP), a standard metric for multilabel classification, throughout the adaptation procedure on the six soundscape datasets. We benchmark our proposed NOTELA and Dropout Student (see below), as well as &lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;, &lt;a href=&quot;https://openreview.net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;DUST&lt;/a>; and &lt;a href=&quot;https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;Pseudo-Labelling&lt;/a>;. Aside from NOTELA, all other methods fail to consistently improve the source model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Introducing NOisy student TEacher with Laplacian Adjustment (NOTELA)&lt;/h2>; &lt;p>; Nonetheless, a surprisingly positive result stands out: the less celebrated &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;>;Noisy Student&lt;/a>; principle appears promising. This unsupervised approach encourages the model to reconstruct its own predictions on some target dataset, but under the application of random noise. While noise may be introduced through various channels, we strive for simplicity and use &lt;a href=&quot;https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9&quot;>;model dropout&lt;/a>; as the only noise source: we therefore refer to this approach as&lt;em>; Dropout Student (DS)&lt;/em>;. In a nutshell, it encourages the model to limit the influence of individual neurons (or filters) when making predictions on a specific target dataset. &lt;/p>; &lt;p>; DS, while effective, faces a model collapse issue on various target domains. We hypothesize this happens because the source model initially lacks confidence in those target domains. We propose improving DS stability by using the feature space directly as an auxiliary source of truth. NOTELA does this by encouraging similar pseudo-labels for nearby points in the feature space, inspired by &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&#39;s method&lt;/a>; and Laplacian &lt;a href=&quot;https://www.researchgate.net/publication/220319905_Manifold_Regularization_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples&quot;>;regularization&lt;/a>;. This simple approach is visualized below, and consistently and significantly outperforms the source model in both audio and visual tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4VZgRaN-w1jovOwJ4gEXoupksTEpvb5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s1870/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;124&quot; data-original-width=&quot;1870&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4VZgRaN-w1jovOwJ4gEXoupksTEpvb5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9QnJb5JxwLFwbmJJN_3jmlGB9-CTug92969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s1080/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9QnJb5JxwLFwbmJJN_3jmlGB9-CTug92969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;NOTELA in action. The audio recordings are forwarded through the full model to obtain a first set of predictions, which are then refined through Laplacian regularization, a form of post-processing based on clustering nearby points. Finally, the refined predictions are used as targets for the &lt;em>;noisy model &lt;/em>;to reconstruct.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The standard artificial image classification benchmarks have inadvertently limited our understanding of the true generalizability and robustness of SFDA methods. We advocate for broadening the scope and adopt a new assessment framework that incorporates naturally-occurring distribution shifts from bioacoustics. We also hope that NOTELA serves as a robust baseline to facilitate research in that direction. NOTELA&#39;s strong performance perhaps points to two factors that can lead to developing more generalizable models: first, developing methods with an eye towards harder problems and second, favoring simple modeling principles. However, there is still future work to be done to pinpoint and comprehend existing methods&#39; failure modes on harder problems. We believe that our research represents a significant step in this direction, serving as a foundation for designing SFDA methods with greater generalizability. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;One of the authors of this post, Eleni Triantafillou, is now at Google DeepMind. We are posting this blog post on behalf of the authors of the NOTELA paper: Malik Boudiaf, Tom Denton, Bart van Merriënboer, Vincent Dumoulin*, Eleni Triantafillou* (where * denotes equal contribution). We thank our co-authors for the hard work on this paper and the rest of the Perch team for their support and feedback.&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;Note that in this audio clip, the bird song is very faint; a common property in soundscape recordings where bird calls aren&#39;t at the “foreground”.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5980448298988153366/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/in-search-of-generalizable-method-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/in-search-of-generalizable-method-for.html&quot; rel=&quot;alternate&quot; title=&quot;In search of a generalizable method for source-free domain adaptation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4epC-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgyM4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/s72-c/notela.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2519516457542613363&lt;/id>;&lt;published>;2023-07-23T14:13:00.002-07:00&lt;/published>;&lt;updated>;2023-08-07T10:08:27.713-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ICML 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Cat Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4nvXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s320/Google-ICML-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Google 各个团队积极开展机器学习 (ML) 领域的理论和应用研究。我们构建机器学习系统来解决语言、音乐、视觉处理、算法开发等领域的深层科学和工程挑战。我们的目标是通过开源工具和数据集、发布我们的工作并积极参加会议，与更广泛的机器学习研究社区建立一个更具协作性的生态系统。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Google is proud to be a &lt;a href=&quot;https://icml.cc/virtual/2023/sponsor_list&quot;>;Diamond Sponsor&lt;/a>; of the 40th &lt;a href=&quot;https://icml.cc/virtual/2023/index.html&quot;>;International Conference on Machine Learning&lt;/a>; (ICML 2023), a premier annual conference, which is being held this week in Honolulu, Hawaii. As a leader in ML research, Google has a strong presence at this year&#39;s conference with over 120 accepted papers and active involvement in a number of workshops and tutorials. Google is also proud to be a Platinum Sponsor for both the &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; and &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;Women in Machine Learning&lt;/a>; workshops. We look forward to sharing some of our extensive ML research and expanding our partnership with the broader ML research community. &lt;/p>; &lt;p>; Registered for ICML 2023? We hope you&#39;ll visit the Google booth to learn more about the exciting work, creativity, and fun that goes into solving a portion of the field&#39;s most interesting challenges. Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions). See &lt;a href=&quot;http://www.deepmind.com/blog/google-deepmind-research-at-icml-2023&quot;>;Google DeepMind&#39;s blog&lt;/a>; to learn about their technical participation at ICML 2023. &lt;/p>; &lt;p>; Take a look below to learn more about the Google research being presented at ICML 2023 (Google affiliations in &lt;strong>;bold&lt;/strong>;).&lt;/p>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Board Members include: &lt;strong>;&lt;em>;Corinna Cortes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>; &lt;br>; Tutorial Chairs include: &lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Lhyy8H75KA&quot;>;Scaling Vision Transformers to 22 Billion Parameters&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Josip Djolonga&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Basil Mustafa&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Piotr Padlewski&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Heek&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Justin Gilmer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andreas Steiner&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Robert Geirhos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ibrahim Alabdulmohsin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lucas Beyer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Tschannen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiao Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Carlos Riquelme&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthias Minderer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joan Puigcerver,&lt;/em>; &lt;em>;Utku Evci&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Manoj Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gamaleldin F. Elsayed&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fisher Yu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Avital Oliver&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fantine Huot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mark Patrick Collier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alexey Gritsenko&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vighnesh Birodkar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Cristina Vasconcelos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Mensink&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alexander Kolesnikov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Filip Pavetić&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dustin Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mario Lučić&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiaohua Zhai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel Keysers&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremiah Harmsen&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Neil Houlsby&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=C9NEblP8vS&quot;>;Fast Inference from Transformers via Speculative Decoding&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yaniv Leviathan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matan Kalman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yossi Matias&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=bUFUaawOTk&quot;>;Best of Both Worlds Policy Optimization&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;, &lt;em>;Chen-Yu Wei&lt;/em>;, &lt;strong>;&lt;em>;Julian Zimmert&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=9PJ2V6qvQL&quot;>;Inflow, Outflow, and Reciprocity in Machine Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Walid Krichene&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;Transformers Learn In-Context by Gradient Descent&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Eyvind Niklasson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ettore Randazzo&lt;/em>;&lt;/strong>;, &lt;em>;João Sacramento&lt;/em>;,&lt;strong>; &lt;em>;Alexander Mordvintsev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andrey Zhmoginov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Max Vladymyrov&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=EfhmBBrXY2&quot;>;Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Luke Vilnis&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;, &lt;em>;Patrick Murray*&lt;/em>;, &lt;em>;Alexandre Passos*&lt;/em>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ayBKRjGDEI&quot;>;Differentially Private Hierarchical Clustering with Provable Approximation Guarantees&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/differentially-private-clustering-for.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Jacob Imola&lt;/em>;*,&lt;strong>; &lt;em>;Alessandro Epasto&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mohammad Mahdian&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vincent Cohen-Addad&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ZVxT2ToHR5&quot;>;Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;H. Brendan McMahan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Keith Rush&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=1UaGAhLAsL&quot;>;Random Classification Noise Does Not Defeat All Convex Potential Boosters Irrespective of Model Choice&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Richard Nock&lt;/em>;&lt;/strong>;, &lt;em>;Robert Williamson&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=qw8zAw6mzJ&quot;>;Simplex Random Features&lt;/a>; &lt;br>; &lt;em>;Isaac Reid&lt;/em>;,&lt;strong>; &lt;em>;Krzysztof Choromanski&lt;/em>;&lt;/strong>;, &lt;em>;Valerii Likhosherstov&lt;/em>;, &lt;em>;Adrian Weller&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=bF1LVbP493&quot;>;Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mandar Joshi&lt;/em>;&lt;/strong>;, &lt;em>;Iulia Turc&lt;/em>;,&lt;strong>; &lt;em>;Hexiang Hu&lt;/em>;&lt;/strong>;, &lt;em>;Fangyu Liu&lt;/em>;,&lt;strong>; &lt;em>;Julian Eisenschlos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Shaw&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Wei Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kristina Toutanova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=eIQIcUKs0T&quot;>;Mu&lt;sup>;2&lt;/sup>;SLAM: Multitask, Multilingual Speech and Language Models&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yong Cheng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yu Zhang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Melvin Johnson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wolfgang Macherey&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ankur Bapna&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=5h42xM0pwn&quot;>;Robust Budget Pacing with a Single Sample&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Santiago Balseiro&lt;/em>;&lt;/strong>;, &lt;em>;Rachitesh Kumar&lt;/em>;*,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Balasubramanian Sivan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Di Wang&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=0bR5JuxaoN&amp;amp;name=pdf&quot;>;A Statistical Perspective on Retrieval-Based Models&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Soumya Basu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;, &lt;em>;Manzil Zaheer&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=XjTcC4EA4P&quot;>;Approximately Optimal Core Shapes for Tensor Decompositions&lt;/a>; &lt;br>; &lt;em>;Mehrdad Ghadiri&lt;/em>;,&lt;strong>; &lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Fu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=rWGp9FbS0Q&amp;amp;name=pdf&quot;>;Efficient List-Decodable Regression Using Batches&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Abhimanyu Das&lt;/em>;&lt;/strong>;, &lt;em>;Ayush Jain&lt;/em>;*,&lt;strong>; &lt;em>;Weihao Kong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rajat Sen&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=SpFIO5Mdso&amp;amp;name=pdf&quot;>;Efficient Training of Language Models Using Few-Shot Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Sashank J. Reddi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sobhan Miryoosefi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Stefani Karp&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shankar Krishnan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Seungyeon Kim&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sanjiv Kumar&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=Bj76bauv1Q&amp;amp;name=pdf&quot;>;Fully Dynamic Submodular Maximization Over Matroids&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;, &lt;em>;Federico Fusco&lt;/em>;, &lt;strong>;&lt;em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Morteza Zadimoghaddam&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=VlEAJkmlMs&amp;amp;name=pdf&quot;>;GFlowNet-EM for Learning Compositional Latent Variable Models&lt;/a>; &lt;br>; &lt;em>;Edward J Hu&lt;/em>;, &lt;em>;Nikolay Malkin&lt;/em>;, &lt;em>;Moksh Jain&lt;/em>;, &lt;strong>;&lt;em>;Katie Everett&lt;/em>;&lt;/strong>;, &lt;em>;Alexandros Graikos&lt;/em>;, &lt;em>;Yoshua Bengio&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rB0VaD44FZ&quot;>;Improved Online Learning Algorithms for CTR Prediction in Ad Auctions&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Zhe Feng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>;, &lt;em>;Zixin Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sfdKdeczaw&amp;amp;name=pdf&quot;>;Large Language Models Struggle to Learn Long-Tail Knowledge&lt;/a>; &lt;br>; &lt;em>;Nikhil Kandpal&lt;/em>;, &lt;em>;Haikang Deng&lt;/em>;,&lt;strong>; &lt;em>;Adam Roberts&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Eric Wallace&lt;/em>;&lt;/strong>;, &lt;em>;Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=UdiUd99I81&quot;>;Multi-channel Autobidding with Budget and ROI Constraints&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yuan Deng&lt;/em>;&lt;/strong>;, &lt;em>;Negin Golrezaei&lt;/em>;, &lt;em>;Patrick Jaillet&lt;/em>;, &lt;em>;Jason Cheuk Nam Liang&lt;/em>;, &lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZMvv6laV5b&amp;amp;name=pdf&quot;>;Multi-layer Neural Networks as Trainable Ladders of Hilbert Spaces&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Zhengdao Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfkSyUJyqg&quot;>;On User-Level Private Convex Optimization&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Badih Ghazi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pritish Kamath&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ravi Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghu Meka&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pasin Manurangsi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Chiyuan Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zAgouWgI7b&amp;amp;name=pdf&quot;>;PAC Generalization via Invariant Representations&lt;/a>; &lt;br>; &lt;em>;Advait U Parulekar&lt;/em>;,&lt;strong>; &lt;em>;Karthikeyan Shanmugam&lt;/em>;&lt;/strong>;, &lt;em>;Sanjay Shakkottai&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0rZvMIfECW&amp;amp;name=pdf&quot;>;Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice&lt;/a>; &lt;br>; &lt;em>;Toshinori Kitamura&lt;/em>;, &lt;em>;Tadashi Kozuno&lt;/em>;, &lt;em>;Yunhao Tang&lt;/em>;, &lt;strong>;&lt;em>;Nino Vieillard&lt;/em>;&lt;/strong>;, &lt;em>;Michal Valko&lt;/em>;, &lt;em>;Wenhao Yang&lt;/em>;,&lt;strong>; &lt;em>;Jincheng Mei&lt;/em>;&lt;/strong>;, &lt;em>;Pierre Menard&lt;/em>;, &lt;em>;Mohammad Gheshlaghi Azar&lt;/em>;, &lt;em>;Remi Munos&lt;/em>;,&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;,&lt;em>;Csaba Szepesvari&lt;/em>;, &lt;em>;Wataru Kumagai&lt;/em>;, &lt;em>;Yutaka Matsuo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mrykt39VUw&amp;amp;name=pdf&quot;>;Speeding Up Bellman Ford via Minimum Violation Permutations&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;, &lt;em>;Ola Svensson&lt;/em>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LxodbQa62n&amp;amp;name=pdf&quot;>;Statistical Indistinguishability of Learning Algorithms&lt;/a>; &lt;br>; &lt;em>;Alkis Kalavasis&lt;/em>;,&lt;strong>; &lt;em>;Amin Karbasi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shay Moran&lt;/em>;&lt;/strong>;, &lt;em>;Grigoris Velegkas&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=G5vKSJVhJL&quot;>;Test-Time Adaptation with Slot-Centric Models&lt;/a>; &lt;br>; &lt;em>;Mihir Prabhudesai&lt;/em>;, &lt;em>;Anirudh Goyal&lt;/em>;,&lt;strong>; &lt;em>;Sujoy Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Gaurav Aggarwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;, &lt;em>;Deepak Pathak&lt;/em>;, &lt;em>;Katerina Fragkiadaki>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=2WEMW6rGgG&quot;>;Algorithms for Bounding Contribution for Histogram Estimation Under User-Level Privacy&lt;/a>; &lt;br>; &lt;em>;Yuhan Liu&lt;/em>;*, &lt;strong>;&lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Wennan Zhu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Marco Gruteser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=SgeIqUvo4w&amp;amp;name=pdf&quot;>;Bandit Online Linear Optimization with Hints and Queries&lt;/a>; &lt;br>; &lt;em>;Aditya Bhaskara&lt;/em>;, &lt;em>;Ashok Cutkosky&lt;/em>;,&lt;strong>; &lt;em>;Ravi Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Manish Purohit&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=wagsJnR5GO&amp;amp;name=pdf&quot;>;CLUTR: Curriculum Learning via Unsupervised Task Representation Learning&lt;/a>; &lt;br>; &lt;em>;Abdus Salam Azad&lt;/em>;,&lt;strong>; &lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;, &lt;em>;Jasper Emhoff&lt;/em>;, &lt;em>;Nathaniel Alexis&lt;/em>;,&lt;strong>; &lt;em>;Aleksandra Faust&lt;/em>;&lt;/strong>;, &lt;em>;Pieter Abbeel&lt;/em>;, &lt;em>;Ion Stoica&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=R3WrLjtzG8&amp;amp;name=pdf&quot;>;CSP: Self-Supervised Contrastive Spatial Pre-training for Geospatial-Visual Representations&lt;/a>; &lt;br>; &lt;em>;Gengchen Mai&lt;/em>;, &lt;strong>;&lt;em>;Ni Lao&lt;/em>;&lt;/strong>;, &lt;em>;Yutong He&lt;/em>;, &lt;em>;Jiaming Song&lt;/em>;, &lt;em>;Stefano Ermon&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=vd5JYAml0A&amp;amp;name=pdf&quot;>;Ewald-Based Long-Range Message Passing for Molecular Graphs&lt;/a>; &lt;br>; &lt;em>;Arthur Kosmala&lt;/em>;,&lt;strong>; &lt;em>;Johannes Gasteiger&lt;/em>;&lt;/strong>;, &lt;em>;Nicholas Gao&lt;/em>;, &lt;em>;Stephan Günnemann&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Iey50XHA3g&amp;amp;name=pdf&quot;>;Fast (1+ε)-Approximation Algorithms for Binary Matrix Factorization&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;, &lt;em>;Maximilian Vötsch&lt;/em>;, &lt;strong>;&lt;em>;David Woodruff&lt;/em>;&lt;/strong>;, &lt;em>;Samson Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=b9opfVNw6O&amp;amp;name=pdf&quot;>;Federated Linear Contextual Bandits with User-Level Differential Privacy&lt;/a>; &lt;br>; &lt;em>;Ruiquan Huang&lt;/em>;, &lt;em>;Huanyu Zhang&lt;/em>;, &lt;em>;Luca Melis&lt;/em>;, &lt;em>;Milan Shen&lt;/em>;,&lt;strong>; &lt;em>;Meisam Hejazinia&lt;/em>;&lt;/strong>;, &lt;em>;Jing Yang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=m21SgZnBWZ&amp;amp;name=pdf&quot;>;Investigating the Role of Model-Based Learning in Exploration and Transfer&lt;/a>; &lt;br>; &lt;em>;Jacob C Walker&lt;/em>;, &lt;em>;Eszter Vértes&lt;/em>;, &lt;em>;Yazhe Li&lt;/em>;,&lt;strong>; &lt;em>;Gabriel Dulac-Arnold&lt;/em>;&lt;/strong>;, &lt;em>;Ankesh Anand&lt;/em>;, &lt;em>;Theophane Weber&lt;/em>;,&lt;em>; Jessica B Hamrick&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=K1sJiHvy02&quot;>;Label Differential Privacy and Private Training Data Release&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Robert Busa-Fekete&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andres Munoz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Umar Syed&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Q4QFG5Fe4O&amp;amp;name=pdf&quot;>;Lifelong Language Pretraining with Distribution-Specialized Experts&lt;/a>; &lt;br>; &lt;em>;Wuyang Chen&lt;/em>;*, &lt;strong>;&lt;em>;Yanqi Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Nan Du&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yanping Huang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;James Laudon&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Zhifeng Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Claire Cui&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=06djx2x2Rf&amp;amp;name=pdf&quot;>;Multi-User Reinforcement Learning with Low Rank Rewards&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Dheeraj Mysore Nagaraj&lt;/em>;&lt;/strong>;, &lt;em>;Suhas S Kowshik&lt;/em>;, &lt;strong>;&lt;em>;Naman Agarwal&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Praneeth Netrapalli&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Prateek Jain&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DwOUndjwiV&amp;amp;name=pdf&quot;>;Multi-View Masked World Models for Visual Robotic Manipulation&lt;/a>; &lt;br>; &lt;em>;Younggyo Seo&lt;/em>;, &lt;em>;Junsu Kim&lt;/em>;, &lt;em>;Stephen James&lt;/em>;,&lt;strong>; &lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;, &lt;em>;Jinwoo Shin&lt;/em>;, &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VTpHpqM3Cf&amp;amp;name=pdf&quot;>;PaLM-E: An Embodied Multimodal Language Model&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Danny Driess&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Fei Xia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Corey Lynch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Ichter&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;Ayzaan Wahid&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan Tompson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Quan Vuong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tianhe Yu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wenlong Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yevgen Chebotar&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pierre Sermanet&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Duckworth&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vincent Vanhoucke&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Karol Hausman&lt;/em>;&lt;/strong>;, &lt;em>;Marc Toussaint&lt;/em>;,&lt;strong>; &lt;em>;Klaus Greff&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Igor Mordatch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pete Florence&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=y8qAZhWbNs&quot;>;Private Federated Learning with Autotuned Compression&lt;/a>; &lt;br>; &lt;em>;Enayat Ullah&lt;/em>;*, &lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=7WdMBofQFx&amp;amp;name=pdf&quot;>;Refined Regret for Adversarial MDPs with Linear Function Approximation&lt;/a>; &lt;br>; &lt;em>;Yan Dai&lt;/em>;, &lt;em>;Haipeng Luo&lt;/em>;, &lt;em>;Chen-Yu Wei&lt;/em>;, &lt;strong>;&lt;em>;Julian Zimmert&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ccwSdYv1GI&amp;amp;name=pdf&quot;>;Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory&lt;/a>; &lt;br>; &lt;em>;Justin Cui&lt;/em>;,&lt;em>; Ruoche Wan&lt;/em>;, &lt;strong>;&lt;em>;Si Si&lt;/em>;&lt;/strong>;, &lt;em>;Cho-Jui Hsieh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=X7jMTrwuCz&amp;amp;name=pdf&quot;>;SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance&lt;/a>; &lt;br>; &lt;em>;Amit Attia&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6EVUnWGBMU&quot;>;The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation&lt;/a>; &lt;br>; &lt;em>;Mark Rowland&lt;/em>;, &lt;em>;Yunhao Tang&lt;/em>;, &lt;em>;Clare Lyle&lt;/em>;, &lt;em>;Rémi Munos&lt;/em>;, &lt;strong>;&lt;em>;Marc G. Bellemare&lt;/em>;&lt;/strong>;, &lt;em>;Will Dabney&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=iMHNLJRSVz&amp;amp;name=pdf&quot;>;Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features&lt;/a>; &lt;br>; &lt;em>;Chieh Hubert Lin&lt;/em>;, &lt;em>;Hung-Yu Tseng&lt;/em>;, &lt;em>;Hsin-Ying Lee&lt;/em>;, &lt;em>;Maneesh Kumar Singh&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4UStsbnfVT&quot;>;User-Level Private Stochastic Convex Optimization with Optimal Rates&lt;/a>; &lt;br>; &lt;em>;Raef Bassily&lt;/em>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6MU5xdrO7t&amp;amp;name=pdf&quot;>;A Simple Zero-Shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models&lt;/a>; &lt;br>; &lt;em>;James Urquhart Allingham&lt;/em>;*, &lt;strong>;&lt;em>;Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael W Dusenberry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiuye Gu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yin Cui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dustin Tran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremiah Zhe Liu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mXv2aVqUGG&amp;amp;name=pdf&quot;>;Can Large Language Models Reason About Program Invariants?&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kexin Pei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Bieber&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kensen Shi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Charles Sutton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pengcheng Yin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=pWeQdceMHL&amp;amp;name=pdf&quot;>;Concurrent Shuffle Differential Privacy Under Continual Observation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jay Tenenbaum&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Haim Kaplan&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Uri Stemmer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Xqedp0Iu1S&amp;amp;name=pdf&quot;>;Constant Matters: Fine-Grained Error Bound on Differentially Private Continual Observation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Hendrik Fichtenberger&lt;/em>;&lt;/strong>;, &lt;em>;Monika Henzinger&lt;/em>;, &lt;em>;Jalaj Upadhyay&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=NfCA622s8O&amp;amp;name=pdf&quot;>;Cross-Entropy Loss Functions: Theoretical Analysis and Applications&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;, &lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;, &lt;em>;Yutao Zhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=5UZYtGEPTt&amp;amp;name=pdf&quot;>;Efficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation&lt;/a>; &lt;br>; &lt;em>;Orin Levy&lt;/em>;, &lt;strong>;&lt;em>;Alon Cohen&lt;/em>;&lt;/strong>;,&lt;em>; Asaf Cassel&lt;/em>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KrsaROSs8b&amp;amp;name=pdf&quot;>;Fairness in Streaming Submodular Maximization Over a Matroid Constraint&lt;/a>; &lt;br>; &lt;em>;Marwa El Halabi&lt;/em>;, &lt;em>;Federico Fusco&lt;/em>;, &lt;strong>;&lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jakab Tardos&lt;/em>;&lt;/strong>;, &lt;em>;Jakub Tarnawski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZX4uS605XV&amp;amp;name=pdf&quot;>;The Flan Collection: Designing Data and Methods for Effective Instruction Tuning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Shayne Longpre&lt;/em>;, &lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tu Vu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Albert Webson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hyung Won Chung&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Quoc V Le&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Barret Zoph&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jason Wei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adam Roberts&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=rzN05i4GOE&amp;amp;name=pdf&quot;>;Graph Reinforcement Learning for Network Control via Bi-level Optimization&lt;/a>; &lt;br>; &lt;em>;Daniele Gammelli&lt;/em>;,&lt;strong>; &lt;em>;James Harrison&lt;/em>;&lt;/strong>;, &lt;em>;Kaidi Yang&lt;/em>;, &lt;em>;Marco Pavone&lt;/em>;, &lt;em>;Filipe Rodrigues&lt;/em>;, &lt;em>;Francisco C. Pereira&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Fgn23Fsmtv&amp;amp;name=pdf&quot;>;Learning-Augmented Private Algorithms for Multiple Quantile Release&lt;/a>; &lt;br>; &lt;em>;Mikhail Khodak&lt;/em>;*, &lt;strong>;&lt;em>;Kareem Amin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Travis Dick&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=uSHBQdWmuC&amp;amp;name=pdf&quot;>;LegendreTron: Uprising Proper Multiclass Loss Learning&lt;/a>; &lt;br>; &lt;em>;Kevin H Lam&lt;/em>;, &lt;strong>;&lt;em>;Christian Walder&lt;/em>;&lt;/strong>;, &lt;em>;Spiridon Penev&lt;/em>;,&lt;strong>; &lt;em>;Richard Nock&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VnGIZsmxDG&amp;amp;name=pdf&quot;>;Measuring the Impact of Programming Language Distribution&lt;/a>; &lt;br>; &lt;em>;Gabriel Orlanski&lt;/em>;*, &lt;strong>;&lt;em>;Kefan Xiao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeffrey Hui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Howland&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Malmaud&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jacob Austin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rishabh Singh&lt;/em>;&lt;/strong>;, &lt;em>;Michele Catasta&lt;/em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=f69OtekDi4&quot;>;Multi-task Differential Privacy Under Distribution Skew&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Walid Krichene&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shuang Song&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Li Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=hi9UssZdHR&quot;>;Muse: Text-to-Image Generation via Masked Generative Transformers&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Huiwen Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Han Zhang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jarred Barber&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;AJ Maschinot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;José Lezama&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lu Jiang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kevin Murphy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;William T. Freeman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Rubinstein&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuanzhen Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dilip Krishnan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;On the Convergence of Federated Averaging with Cyclic Client Participation&lt;/a>; &lt;br>; &lt;em>;Yae Jee Cho&lt;/em>;, &lt;em>;Pranay Sharma&lt;/em>;, &lt;em>;Gauri Joshi&lt;/em>;, &lt;strong>;&lt;em>;Zheng Xu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tong Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=GimajxXNc0&amp;amp;name=pdf&quot;>;Optimal Stochastic Non-smooth Non-convex Optimization Through Online-to-Non-convex Conversion&lt;/a>; &lt;br>; &lt;em>;Ashok Cutkosky&lt;/em>;, &lt;strong>;&lt;em>;Harsh Mehta&lt;/em>;&lt;/strong>;, &lt;em>;Francesco Orabona&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=4SHQv4cp3I&amp;amp;name=pdf&quot;>;Out-of-Domain Robustness via Targeted Augmentations&lt;/a>; &lt;br>; &lt;em>;Irena Gao&lt;/em>;, &lt;em>;Shiori Sagawa&lt;/em>;,&lt;strong>; &lt;em>;Pang Wei Koh&lt;/em>;&lt;/strong>;, &lt;em>;Tatsunori Hashimoto&lt;/em>;,&lt;em>; Percy Liang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=b6Hxt4Jw10&quot;>;Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models&lt;/a>; &lt;br>; &lt;em>;Jamil Arbas&lt;/em>;, &lt;em>;Hassan Ashtiani&lt;/em>;,&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=nlUAvrMbUZ&amp;amp;name=pdf&quot;>;Pre-computed Memory or On-the-Fly Encoding? A Hybrid Approach to Retrieval Augmentation Makes the Most of Your Compute&lt;/a>; &lt;br>; &lt;em>;Michiel de Jong&lt;/em>;, &lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nicholas FitzGerald&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fei Sha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;William W. Cohen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1FldU7JzGh&amp;amp;name=pdf&quot;>;Scalable Adaptive Computation for Iterative Generation&lt;/a>; &lt;br>; &lt;em>;Allan Jabri&lt;/em>;*, &lt;strong>;&lt;em>;David J. Fleet&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ting Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HiKPaeowPB&quot;>;Scaling Spherical CNNs&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Carlos Esteves&lt;/em>;&lt;/strong>;, &lt;em>;Jean-Jacques Slotine&lt;/em>;, &lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0O7b2Y198V&amp;amp;name=pdf&quot;>;STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition&lt;/a>; &lt;br>; &lt;em>;Yucheng Lu&lt;/em>;, &lt;strong>;&lt;em>;Shivani Agrawal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Suvinay Subramanian&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Oleg Rybakov&lt;/em>;&lt;/strong>;, &lt;em>;Christopher De Sa&lt;/em>;, &lt;em>;Amir Yazdanbakhsh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LZt1HIEoAf&amp;amp;name=pdf&quot;>;Stratified Adversarial Robustness with Rejection&lt;/a>; &lt;br>; &lt;em>;Jiefeng Chen&lt;/em>;, &lt;em>;Jayaram Raghuram&lt;/em>;, &lt;em>;Jihye Choi&lt;/em>;,&lt;strong>; &lt;em>;Xi Wu&lt;/em>;&lt;/strong>;, &lt;em>;Yingyu Liang&lt;/em>;, &lt;em>;Somesh Jha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=CnHxxjqkMi&amp;amp;name=pdf&quot;>;When Does Privileged information Explain Away Label Noise?&lt;/a>; &lt;br>; &lt;em>;Guillermo Ortiz-Jimenez&lt;/em>;*,&lt;strong>; &lt;em>;Mark Collier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anant Nawalgaria&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alexander D&#39;Amour&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jesse Berent&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Effrosyni Kokiopoulou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2bGTacOn8v&amp;amp;name=pdf&quot;>;Adaptive Computation with Elastic Input Sequence&lt;/a>; &lt;br>; &lt;em>;Fuzhao Xue&lt;/em>;*, &lt;strong>;&lt;em>;Valerii Likhosherstov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;, &lt;em>;Yang You&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Pbaiy3fRCt&amp;amp;name=pdf&quot;>;Can Neural Network Memorization Be Localized?&lt;/a>; &lt;br>; &lt;em>;Pratyush Maini&lt;/em>;,&lt;strong>; &lt;em>;Michael C. Mozer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>;, &lt;em>;Zachary C. Lipton&lt;/em>;, &lt;em>;J. Zico Kolter&lt;/em>;,&lt;strong>; &lt;em>;Chiyuan Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Ct2N6RWZpQ&amp;amp;name=pdf&quot;>;Controllability-Aware Unsupervised Skill Discovery&lt;/a>; &lt;br>; &lt;em>;Seohong Park&lt;/em>;,&lt;strong>; &lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;, &lt;em>;Youngwoon Lee&lt;/em>;, &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2Mbo7IEtZW&amp;amp;name=pdf&quot;>;Efficient Learning of Mesh-Based Physical Simulation with Bi-Stride Multi-Scale Graph Neural Network&lt;/a>; &lt;br>; &lt;em>;Yadi Cao&lt;/em>;,&lt;strong>; &lt;em>;Menglei Chai&lt;/em>;&lt;/strong>;, &lt;em>;Minchen Li&lt;/em>;, &lt;em>;Chenfanfu Jiang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zN4oRCrlnM&amp;amp;name=pdf&quot;>;Federated Heavy Hitter Recovery Under Linear Sketching&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Adria Gascon&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Peter Kairouz&lt;/strong>;&lt;/em>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;Graph Generative Model for Benchmarking Graph Neural Networks&lt;/a>; &lt;br>; &lt;em>;Minji Yoon&lt;/em>;, &lt;em>;Yue Wu&lt;/em>;, &lt;strong>;&lt;em>;John Palowitch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;em>;Russ Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=IFhGrPAn8f&amp;amp;name=pdf&quot;>;H-Consistency Bounds for Pairwise Misranking Loss Surrogates&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;, &lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;, &lt;em>;Yutao Zhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DF6ypWrepg&amp;amp;name=pdf&quot;>;Improved Regret for Efficient Online Reinforcement Learning with Linear Function Approximation&lt;/a>; &lt;br>; &lt;em>;Uri Sherman&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZXeTCRZJp9&amp;amp;name=pdf&quot;>;Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames&lt;/a>; &lt;br>; &lt;em>;Ondrej Biza&lt;/em>;*,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gamaleldin Fathy Elsayed&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=a35tteW8if&quot;>;Multi-task Off-Policy Learning from Bandit Feedback&lt;/a>; &lt;br>; &lt;em>;Joey Hong&lt;/em>;, &lt;em>;Branislav Kveton&lt;/em>;, &lt;em>;Manzil Zaheer&lt;/em>;, &lt;em>;Sumeet Katariya&lt;/em>;,&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=yv8GUQREda&amp;amp;name=pdf&quot;>;Optimal No-Regret Learning for One-Sided Lipschitz Functions&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Guru Guruganesh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jon Schneider&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Joshua Ruizhi Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=AwxfYvdPZV&amp;amp;name=pdf&quot;>;Policy Mirror Ascent for Efficient and Independent Learning in Mean Field Games&lt;/a>; &lt;br>; &lt;em>;Batuhan Yardim&lt;/em>;, &lt;em>;Semih Cayci&lt;/em>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;, &lt;em>;Niao He&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ILMHlUn4k6&amp;amp;name=pdf&quot;>;Regret Minimization and Convergence to Equilibria in General-Sum Markov Games&lt;/a>; &lt;br>; &lt;em>;Liad Erez&lt;/em>;, &lt;em>;Tal Lancewicki&lt;/em>;, &lt;em>;Uri Sherman&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=skDVsmXjPR&amp;amp;name=pdf&quot;>;Reinforcement Learning Can Be More Efficient with Multiple Rewards&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rdOuTlTUMX&quot;>;Reinforcement Learning with History-Dependent Dynamic Contexts&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Guy Tennenholtz&lt;/em>;&lt;/strong>;, &lt;em>;Nadav Merlis&lt;/em>;, &lt;strong>;&lt;em>;Lior Shani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Martin Mladenov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Craig Boutlier&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sdhcjMzhHN&amp;amp;name=pdf&quot;>;User-Defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems&lt;/a>; &lt;br>; &lt;em>;Marc Anton Finzi&lt;/em>;*,&lt;strong>; &lt;em>;Anudhyan Boral&lt;/em>;&lt;/strong>;, &lt;em>;Andrew Gordon Wilson&lt;/em>;,&lt;strong>; &lt;em>;Fei Sha&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Leonardo Zepeda-Nunez&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LDBIVZCnLl&amp;amp;name=pdf&quot;>;Discrete Key-Value Bottleneck&lt;/a>; &lt;br>; &lt;em>;Frederik Träuble&lt;/em>;, &lt;em>;Anirudh Goyal&lt;/em>;, &lt;em>;Nasim Rahaman&lt;/em>;,&lt;strong>; &lt;em>;Michael Curtis Mozer&lt;/em>;&lt;/strong>;, &lt;em>;Kenji Kawaguchi&lt;/em>;, &lt;em>;Yoshua Bengio&lt;/em>;, &lt;em>;Bernhard Schölkopf&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nVO6YTca8O&quot;>;DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm&lt;/a>; &lt;br>; &lt;em>;Lisang Ding&lt;/em>;, &lt;em>;Kexin Jin&lt;/em>;,&lt;strong>; &lt;em>;Bicheng Ying&lt;/em>;&lt;/strong>;, &lt;em>;Kun Yuan&lt;/em>;, &lt;em>;Wotao Yin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3Ge74dgjjU&amp;amp;name=pdf&quot;>;Exphormer: Sparse Transformers for Graphs&lt;/a>; &lt;br>; &lt;em>;Hamed Shirzad&lt;/em>;, &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Balaji Venkatachalam&lt;/em>;&lt;/strong>;, &lt;em>;Danica J. Sutherland&lt;/em>;,&lt;strong>; &lt;em>;Ali Kemal Sinop&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=dolp65Z6re&amp;amp;name=pdf&quot;>;Fast, Differentiable and Sparse Top-k: A Convex Analysis Perspective&lt;/a>; &lt;br>; &lt;em>;Michael Eli Sander&lt;/em>;*, &lt;strong>;&lt;em>;Joan Puigcerver&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Josip Djolonga&lt;/em>;&lt;/strong>;, &lt;em>;Gabriel Peyré&lt;/em>;,&lt;strong>; &lt;em>;Mathieu Blondel&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=priTMs7n6e&quot;>;Improved Policy Evaluation for Randomized Trials of Algorithmic Resource Allocation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Aditya Mate&lt;/em>;&lt;/strong>;, &lt;em>;Bryan Wilder&lt;/em>;,&lt;strong>; &lt;em>;Aparna Taneja&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Milind Tambe&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Yh9sFZQk7Y&amp;amp;name=pdf&quot;>;In Search for a Generalizable Method for Source Free Domain Adaptation&lt;/a>; &lt;br>; &lt;em>;Malik Boudiaf&lt;/em>;*, &lt;strong>;&lt;em>;Tom Denton&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bart van Merrienboer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vincent Dumoulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Eleni Triantafillou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mSofpvUxCL&amp;amp;name=pdf&quot;>;Learning Rate Schedules in the Presence of Distribution Shift&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adel Javanmard&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pratik Worah&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=slM2r4bRD1&amp;amp;name=pdf&quot;>;Not All Semantics Are Created Equal: Contrastive Self-Supervised Learning with Automatic Temperature Individualization&lt;/a>; &lt;br>; &lt;em>;Zi-Hao Qiu&lt;/em>;, &lt;em>;Quanqi Hu&lt;/em>;, &lt;em>;Zhuoning Yuan&lt;/em>;,&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;Lijun Zhang&lt;/em>;, &lt;em>;Tianbao Yang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=EUQsBO975P&amp;amp;name=pdf&quot;>;On the Relationship Between Explanation and Prediction: A Causal View&lt;/a>; &lt;br>; &lt;em>;Amir-Hossein Karimi&lt;/em>;*, &lt;em>;Krikamol Muandet&lt;/em>;,&lt;strong>; &lt;em>;Simon Kornblith&lt;/em>;&lt;/strong>;, &lt;em>;Bernhard Schölkopf&lt;/em>;,&lt;strong>; &lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=qorOnDor89&amp;amp;name=pdf&quot;>;On the Role of Attention in Prompt-Tuning&lt;/a>; &lt;br>; &lt;em>;Samet Oymak&lt;/em>;,&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;, &lt;em>;Mahdi Soltanolkotabi&lt;/em>;, &lt;em>;Christos Thrampoulidis&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2jvwyTm6Pk&amp;amp;name=pdf&quot;>;PLay: Parametrically Conditioned Layout Generation Using Latent Diffusion&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Chin-Yi Cheng&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yang Li&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=631FTQB0UB&amp;amp;name=pdf&quot;>;The Power of Learned Locally Linear Models for Nonlinear Policy Optimization&lt;/a>; &lt;br>; &lt;em>;Daniel Pfrommer&lt;/em>;, &lt;em>;Max Simchowitz&lt;/em>;, &lt;em>;Tyler Westenbroek&lt;/em>;, &lt;em>;Nikolai Matni&lt;/em>;,&lt;strong>; &lt;em>;Stephen Tu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=BDYIci7bVs&amp;amp;name=pdf&quot;>;Relevant Walk Search for Explaining Graph Neural Networks&lt;/a>; &lt;br>; &lt;em>;Ping Xiong&lt;/em>;, &lt;em>;Thomas Schnake&lt;/em>;, &lt;em>;Michael Gastegger&lt;/em>;, &lt;em>;Grégoire Montavon&lt;/em>;,&lt;strong>; &lt;em>;Klaus Robert Muller&lt;/em>;&lt;/strong>;,&lt;em>;Shinichi Nakajima&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RX70NHEPE0&amp;amp;name=pdf&quot;>;Repository-Level Prompt Generation for Large Language Models of Code&lt;/a>; &lt;br>; &lt;em>;Disha Shrivastava&lt;/em>;,&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel Tarlow&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=r3M5cBtpYq&amp;amp;name=pdf&quot;>;Robust and Private Stochastic Linear Bandits&lt;/a>; &lt;br>; &lt;em>;Vasileios Charisopoulos&lt;/em>;*, &lt;strong>;&lt;em>;Hossein Esfandiari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6l9YG3wHA9&amp;amp;name=pdf&quot;>;Simple Diffusion: End-to-End Diffusion for High Resolution Images&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Emiel Hoogeboom&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Heek&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tim Salimans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=cw6Zb0sEiT&amp;amp;name=pdf&quot;>;Tied-Augment: Controlling Representation Similarity Improves Data Augmentation&lt;/a>; &lt;br>; &lt;em>;Emirhan Kurtulus&lt;/em>;, &lt;strong>;&lt;em>;Zichao Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yann Dauphin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ekin D. Cubuk&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1d3O0b1rbL&amp;amp;name=pdf&quot;>;Why Is Public Pre-Training Necessary for Private Model Training?&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Arun Ganesh&lt;/em>;&lt;/strong>;, &lt;em>;Mahdi Haghifam&lt;/em>;*, &lt;strong>;&lt;em>;Milad Nasr&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Steinke&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Om Thakkar&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Abhradeep Guha Thakurta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lun Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XXC601YWgq&amp;amp;name=pdf&quot;>;A Connection Between One-Step RL and Critic Regularization in Reinforcement Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Benjamin Eysenbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;, &lt;em>;Ruslan Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;Beyond Uniform Lipschitz Condition in Differentially Private Optimization&lt;/a>; &lt;br>; &lt;em>;Rudrajit Das&lt;/em>;*, &lt;strong>;&lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tong Zhang&lt;/em>;&lt;/strong>;, &lt;em>;Sujay Sanghavi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Y5jGkbZ0W3&amp;amp;name=pdf&quot;>;Efficient Graph Field Integrators Meet Point Clouds&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Krzysztof Choromanski&lt;/em>;&lt;/strong>;, &lt;em>;Arijit Sehanobish&lt;/em>;, &lt;em>;Han Lin&lt;/em>;, &lt;em>;Yunfan Zhao&lt;/em>;, &lt;em>;Eli Berger,&lt;/em>; &lt;em>;Tetiana Parshakova&lt;/em>;, &lt;em>;Alvin Pan&lt;/em>;, &lt;em>;David Watkins&lt;/em>;, &lt;em>;Tianyi Zhang&lt;/em>;, &lt;em>;Valerii Likhosherstov&lt;/em>;, &lt;em>;Somnath Basu Roy Chowdhury&lt;/em>;,&lt;strong>; &lt;em>;Avinava Dubey&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Deepali Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tamas Sarlos&lt;/em>;&lt;/strong>;, &lt;em>;Snigdha Chaturvedi&lt;/em>;, &lt;em>;Adrian Weller&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RAeN6s9RZV&amp;amp;name=pdf&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>; &lt;br>; &lt;em>;Riade Benbaki&lt;/em>;, &lt;em>;Wenyu Chen&lt;/em>;, &lt;em>;Xiang Meng&lt;/em>;, &lt;strong>;&lt;em>;Hussein Hazimeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhe Zhao&lt;/em>;&lt;/strong>;, &lt;em>;Rahul Mazumder&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2M7lwN0DTp&amp;amp;name=pdf&quot;>;Jump-Start Reinforcement Learning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Ikechukwu Uchendu&lt;/em>;*, &lt;strong>;&lt;em>;Ted Xiao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yao Lu&lt;/em>;&lt;/strong>;, &lt;em>;Banghua Zhu&lt;/em>;, &lt;em>;Mengyuan Yan&lt;/em>;, &lt;em>;Joséphine Simon&lt;/em>;, &lt;em>;Matthew Bennice&lt;/em>;, &lt;em>;Chuyuan Fu&lt;/em>;, &lt;em>;Cong Ma&lt;/em>;, &lt;em>;Jiantao Jiao&lt;/em>;,&lt;strong>; &lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Karol Hausman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=WPjMrOi1KE&amp;amp;name=pdf&quot;>;Learning in POMDPs is Sample-Efficient with Hindsight Observability&lt;/a>; &lt;br>; &lt;em>;Jonathan Lee&lt;/em>;,&lt;strong>; &lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;, &lt;em>;Tong Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=K0InBsKODr&amp;amp;name=pdf&quot;>;Low-Variance Gradient Estimation in Unrolled Computation Graphs with ES-Single&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Vicol&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Qh0Gbq3lkh&amp;amp;name=pdf&quot;>;Masked Trajectory Models for Prediction, Representation, and Control&lt;/a>; &lt;br>; &lt;em>;Philipp Wu&lt;/em>;, &lt;em>;Arjun Majumdar&lt;/em>;, &lt;em>;Kevin Stone&lt;/em>;, &lt;em>;Yixin Lin&lt;/em>;,&lt;strong>; &lt;em>;Igor Mordatch&lt;/em>;&lt;/strong>;, &lt;em>;Pieter Abbeel&lt;/em>;, &lt;em>;Aravind Rajeswaran&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DnTVBs6zbz&amp;amp;name=pdf&quot;>;Overcoming Simplicity Bias in Deep Networks Using a Feature Sieve&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Rishabh Tiwari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pradeep Shenoy&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KKaTURYcKG&amp;amp;name=pdf&quot;>;Pairwise Ranking Losses of Click-Through Rates Prediction for Welfare Maximization in Ad Auctions&lt;/a>; &lt;br>; &lt;em>;Boxiang Lyu&lt;/em>;,&lt;strong>; &lt;em>;Zhe Feng&lt;/em>;&lt;/strong>;, &lt;em>;Zachary Robertson&lt;/em>;,&lt;strong>; &lt;em>;Sanmi Koyejo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=UTtYSDO1MK&amp;amp;name=pdf&quot;>;Predictive Flows for Faster Ford-Fulkerson&lt;/a>; &lt;br>; &lt;em>;Sami Davies&lt;/em>;, &lt;em>;Benjamin Moseley&lt;/em>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuyan Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=SVCYSBgFIr&amp;amp;name=pdf&quot;>;Scaling Laws for Multilingual Neural Machine Translation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Patrick Fernandes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Behrooz Ghorbani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=msZrQQAlBA&amp;amp;name=pdf&quot;>;Sequential Monte Carlo Learning for Time Series Structure Discovery&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Feras Saad&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Patton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthew Douglas Hoffman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rif A. Saurous&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vikash Mansinghka&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XqyXhjVRxR&amp;amp;name=pdf&quot;>;Stochastic Gradient Succeeds for Bandits&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jincheng Mei&lt;/em>;&lt;/strong>;, &lt;em>;Zixin Zhong&lt;/em>;,&lt;strong>; &lt;em>;Bo Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;, &lt;em>;Csaba Szepesvari&lt;/em>;,&lt;strong>; &lt;em>;Dale Schuurmans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nm4NwFfp7a&quot;>;Subset-Based Instance Optimality in Private Estimation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Travis Dick&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Kulesza&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zvCSNsoyKW&amp;amp;name=pdf&quot;>;The Unreasonable Effectiveness of Few-Shot Learning for Machine Translation&lt;/a>; &lt;br>; &lt;em>;Xavier Garcia&lt;/em>;, &lt;em>;Yamini Bansal&lt;/em>;,&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;George Foster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Maxim Krikun&lt;/em>;&lt;/strong>;, &lt;em>;Melvin Johnson&lt;/em>;, &lt;em>;Orhan Firat&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21552&quot;>;Self-Supervised Learning in Vision: from Research Advances to Best Practices&lt;/a>; &lt;br>; &lt;em>;Xinlei Chen&lt;/em>;, &lt;em>;Ishan Misra&lt;/em>;, &lt;em>;Randall Balestriero&lt;/em>;, &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;, &lt;em>;Christoph Feichtenhofer&lt;/em>;, &lt;em>;Mark Ibrahim&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21560&quot;>;How to DP-fy ML: A Practical Tutorial to Machine Learning with Differential Privacy&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21558&quot;>;Recent Advances in the Generalization Theory of Neural Networks&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Tengyu Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Damian&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EXPO Day workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;Graph Neural Networks in Tensorflow: A Practical Guide&lt;/a>; &lt;br>; Workshop Organizers include: &lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan Halcrow&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google sponsored affinity workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; (LAXAI) &lt;br>; Platinum Sponsor &lt;br>; Keynote Speaker:&lt;em>; &lt;strong>;Monica Ribero&lt;/strong>;&lt;/em>; &lt;br>; Panelist: &lt;strong>;&lt;em>;Yao Qin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;Women in Machine Learning&lt;/a>; (WiML) &lt;br>; Platinum Sponsor &lt;br>; Panelists:&lt;strong>;&lt;em>; Yao Qin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://fl-icml2023.github.io/&quot;>;Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/imlh2023/home?authuser=1&quot;>;Interpretable Machine Learning in Healthcare&lt;/a>; (IMLH) &lt;br>; Organizer: &lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://klr-icml2023.github.io/&quot;>;Knowledge and Logical Reasoning in the Era of Data-Driven Learning&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Beliz Günel&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mfpl-icml-2023&quot;>;The Many Facets of Preference-Based Learning&lt;/a>; (MFPL) &lt;br>; Organizer: &lt;strong>;&lt;em>;Robert Busa-Fekete&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://syns-ml.github.io/2023/&quot;>;The Synergy of Scientific and Machine Learning Modelling&lt;/a>; (SynS &amp;amp; ML) &lt;br>; Speaker: &lt;strong>;&lt;em>;Sercan Arik&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://tomworkshop.github.io/&quot;>;Theory of Mind in Communicating Agents&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Pei Zhou&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/aihci/home&quot;>;Artificial Intelligence &amp;amp; Human Computer Interaction&lt;/a>; &lt;br>; Organizer:&lt;em>; &lt;strong>;Yang Li&lt;/strong>;&lt;/em>;,&lt;strong>; &lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://dmlr.ai/&quot;>;Data-Centric Machine Learning Research&lt;/a>; (DMLR) &lt;br>; Organizer: &lt;strong>;&lt;em>;Alicia Parrish&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Peter Mattson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Isabelle Guyon&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://neuralcompression.github.io/workshop23&quot;>;Neural Compression: from Information Theory to Applications&lt;/a>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Johannes Ballé&lt;/em>;&lt;/strong>; &lt;br>; Panelist: &lt;strong>;&lt;em>;George Toderici&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/teach-icml-23/home&quot;>;Neural Conversational AI Workshop - What&#39;s Left to TEACH (Trustworthy, Enhanced, Adaptable, Capable and Human-centric) Chatbots?&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/scis-workshop-23&quot;>;Spurious Correlations, Invariance and Stability&lt;/a>; (SCIS) &lt;br>; Organizer: &lt;strong>;&lt;em>;Amir Feder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>; &lt;br>; Title: Unsupervised Graph Embedding @ Google (&lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;paper&lt;/a>;, &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;EXPO workshop&lt;/a>;) &lt;br>; Tuesday, July 25th at 10:30 AM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;br>; Title: Federated Learning of Gboard Language Models with Differential Privacy (&lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;paper 1&lt;/a>;, &lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;paper 2&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot;>;blog post&lt;/a>;) &lt;br>; Tuesday, July 25th at 3:30 PM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;br>; Title: Self-supervised scene understanding (&lt;a href=&quot;https://arxiv.org/abs/2302.04973&quot;>;paper 1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2203.11194&quot;>;paper 2&lt;/a>;) &lt;br>; Wednesday, July 26th at 10:30 AM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Max Vladymyrov&lt;/em>;&lt;/strong>; &lt;br>; Title: Transformers learn in-context by gradient descent (&lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;paper&lt;/a>;) &lt;br>; Wednesday, July 26th at 3:30 PM HST &lt;/p>; &lt;/div>; &lt;br>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2519516457542613363/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-icml-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-icml-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICML 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4nvXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s72-c/Google-ICML-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4421751509192561388&lt;/id>;&lt;published>;2023-07-20T09:22:00.002-07:00&lt;/published>;&lt;updated>;2023-07-20T15:31:26.737-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Using societal context knowledge to foster the responsible application of AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Donald Martin, Jr., Technical Program Manager, Head of Societal Context Understanding Tools and Solutions (SCOUTS), Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s1100/scouts.png&quot; style=&quot;display: none;&quot; />; &lt;p>; AI-related products and technologies are constructed and deployed in a &lt;em>;societal context&lt;/em>;: that is, a dynamic and complex collection of social, cultural, historical, political and economic circumstances. Because societal contexts by nature are dynamic, complex, non-linear, contested, subjective, and highly qualitative, they are challenging to translate into the quantitative representations, methods, and practices that dominate standard machine learning (ML) approaches and responsible AI product development practices. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The first phase of AI product development is &lt;em>;problem understanding&lt;/em>;, and this phase has tremendous influence over how problems (eg, increasing cancer screening availability and accuracy) are formulated for ML systems to solve as well many other downstream decisions, such as dataset and ML architecture choice. When the societal context in which a product will operate is not articulated well enough to result in robust problem understanding, the resulting ML solutions can be fragile and even propagate unfair biases. &lt;/p>; &lt;p>; When AI product developers lack access to the knowledge and tools necessary to effectively understand and consider societal context during development, they tend to abstract it away. This abstraction leaves them with a shallow, quantitative understanding of the problems they seek to solve, while product users and society stakeholders — who are proximate to these problems and embedded in related societal contexts — tend to have a deep qualitative understanding of those same problems. This qualitative–quantitative divergence in ways of understanding complex problems that separates product users and society from developers is what we call the &lt;em>;problem understanding chasm&lt;/em>;. &lt;/p>; &lt;p>; This chasm has repercussions in the real world: for example, it was the root cause of &lt;a href=&quot;https://www.science.org/doi/10.1126/science.aax2342&quot;>;racial bias discovered by a widely used healthcare algorithm&lt;/a>; intended to solve the problem of choosing patients with the most complex healthcare needs for special programs. Incomplete understanding of the societal context in which the algorithm would operate led system designers to form incorrect and oversimplified causal theories about what the key problem factors were. Critical socio-structural factors, including lack of access to healthcare, lack of trust in the health care system, and underdiagnosis due to human bias,&lt;em>; &lt;/em>;were left out while spending on healthcare was highlighted as a predictor of complex health need. &lt;/p>; &lt;p>; To bridge the problem understanding chasm responsibly, AI product developers need tools that put community-validated and structured knowledge of societal context about complex societal problems at their fingertips — starting with problem understanding, but also throughout the product development lifecycle. To that end, &lt;a href=&quot;https://sites.research.google/scouts/&quot;>;Societal Context Understanding Tools and Solutions&lt;/a>; (SCOUTS) — part of the &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; (RAI-HCT) team within Google Research — is a dedicated research team focused on the mission to “empower people with the scalable, trustworthy societal context knowledge required to realize responsible, robust AI and solve the world&#39;s most complex societal problems.” SCOUTS is motivated by the significant challenge of articulating societal context, and it conducts innovative foundational and applied research to produce structured societal context knowledge and to integrate it into all phases of the AI-related product development lifecycle. Last year we &lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;announced&lt;/a>; that &lt;a href=&quot;https://jigsaw.google.com/&quot;>;Jigsaw&lt;/a>;, Google&#39;s incubator for building technology that explores solutions to threats to open societies, leveraged our structured societal context knowledge approach during the data preparation and evaluation phases of model development to scale bias mitigation for their widely used &lt;a href=&quot;https://perspectiveapi.com/&quot;>;Perspective API&lt;/a>; toxicity classifier. Going forward SCOUTS&#39; research agenda focuses on the problem understanding phase of AI-related product development with the goal of bridging the problem understanding chasm. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Bridging the AI problem understanding chasm&lt;/h2>; &lt;p>; Bridging the AI problem understanding chasm requires two key ingredients: 1) a reference frame for organizing structured societal context knowledge and 2) participatory, non-extractive methods to elicit community expertise about complex problems and represent it as structured knowledge. SCOUTS has published innovative research in both areas.&lt;/p>; &lt;br>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 0%; margin-right: 0%;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://sites.research.google/scouts/videos/scouts_pull_intro.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;div style=&quot;line-height: 80%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the problem understanding chasm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;A societal context reference frame&lt;/h3>; &lt;p>; An essential ingredient for producing structured knowledge is a taxonomy for creating the structure to organize it. SCOUTS collaborated with other RAI-HCT teams (&lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;TasC&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/03/responsible-ai-at-google-research.html&quot;>;Impact Lab&lt;/a>;), &lt;a href=&quot;https://www.deepmind.com/&quot;>;Google DeepMind&lt;/a>;, and external system dynamics experts to &lt;a href=&quot;https://arxiv.org/abs/2006.09663&quot;>;develop a taxonomic reference frame&lt;/a>; for societal context.为了应对社会环境的复杂性、动态性和适应性，我们利用&lt;a href=&quot;https://en.wikipedia.org/wiki/Complex_adaptive_system&quot;>;复杂适应性系统&lt;/a>; (CAS) 理论提出用于组织社会背景知识的高级分类模型。 The model pinpoints three key elements of societal context and the dynamic feedback loops that bind them together&lt;strong>;: &lt;/strong>;agents, precepts, and artifacts. &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Agents&lt;/em>;: These can be individuals or institutions. &lt;/li>;&lt;li>;&lt;em>;Precepts&lt;/em>;: The preconceptions — including beliefs, values, stereotypes and biases — that constrain and drive the behavior of agents. An example of a basic precept is that “all basketball players are over 6 feet tall.” That limiting assumption can lead to failures in identifying basketball players of smaller stature. &lt;/li>;&lt;li>;&lt;em>;Artifacts&lt;/em>;: Agent behaviors produce many kinds of artifacts, including language, data, technologies, societal problems and products. &lt;/li>; &lt;/ul>; &lt;p>; The relationships between these entities are dynamic and complex. Our work hypothesizes that precepts are the most critical element of societal context and we highlight &lt;em>;the problems people perceive&lt;/em>; and &lt;em>;the causal theories they hold about why those problems exist&lt;/em>; as particularly influential precepts that are core to understanding societal context. For example, in the case of racial bias in a medical algorithm described earlier, the causal theory precept held by designers was that&lt;em>; complex health problems would cause healthcare expenditures to go up for all populations&lt;/em>;. That incorrect precept directly led to the choice of healthcare spending as the proxy variable for the model to predict complex healthcare need, which in turn led to the model being biased against Black patients who, due to societal factors such as lack of access to healthcare and underdiagnosis due to bias on average, do not always spend more on healthcare when they have complex healthcare needs. A key open question is how can we ethically and equitably elicit causal theories from the people and communities who are most proximate to problems of inequity and transform them into useful structured knowledge? &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s1600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrative version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s1600/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Taxonomic version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Working with communities to foster the responsible application of AI to healthcare&lt;/h3>; &lt;p>; Since its inception, SCOUTS has worked&lt;a href=&quot;https://accelerate.withgoogle.com/stories/exploring-systems-dynamics-inclusive-ml-and-societal-impact-meet-googlers-donald-martin-and-jamaal-sebastian-barnes&quot;>; to build capacity&lt;/a>; in historically marginalized communities to articulate the broader societal context of the complex problems that matter to them using a practice called community based system dynamics (CBSD). &lt;a href=&quot;https://en.wikipedia.org/wiki/System_dynamics&quot;>;System dynamics&lt;/a>; (SD) is a methodology for articulating causal theories about complex problems, both &lt;em>;qualitatively&lt;strong>; &lt;/strong>;&lt;/em>;as causal loop and &lt;a href=&quot;https://online.visual-paradigm.com/knowledge/business-design/what-is-stock-and-flow-diagram/&quot;>;stock and flow diagrams&lt;/a>; (CLDs and SFDs, respectively) and &lt;em>;quantitatively&lt;/em>; as simulation models. The inherent support of visual qualitative tools, quantitative methods, and collaborative model building makes it an ideal ingredient for bridging the problem understanding chasm. CBSD is a &lt;a href=&quot;https://medium.com/people-ai-research/qa-donald-martin-on-community-based-system-dynamics-and-machine-learning-3fa21e42c680&quot;>;community-based, participatory variant of SD&lt;/a>; specifically focused on building capacity within communities to collaboratively describe and model the problems they face as causal theories, directly without intermediaries. With CBSD we&#39;ve witnessed community groups learn the basics and begin drawing CLDs within 2 hours. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s1147/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;863&quot; data-original-width=&quot;1147&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community members learning system dynamics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There is a huge potential for AI to &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9955430/#:~:text=AI%20algorithms%20can%20analyze%20medical,diseases%20more%20accurately%20and%20quickly.&quot;>;improve medical diagnosis&lt;/a>;. But the safety, equity, and reliability of AI-related health diagnostic algorithms depends on diverse and balanced training datasets. An open challenge in the health diagnostic space is the dearth of training sample data from historically marginalized groups. SCOUTS collaborated with the &lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community and CBSD experts to produce &lt;a href=&quot;https://arxiv.org/abs/2305.13485&quot;>;qualitative and quantitative causal theories&lt;/a>; for the data gap problem. The theories include critical factors that make up the broader societal context surrounding health diagnostics, including cultural memory of death and trust in medical care. &lt;/p>; &lt;p>; The figure below depicts the causal theory generated during the collaboration described above as a CLD. It hypothesizes that trust in medical care influences all parts of this complex system and is the key lever for increasing screening, which in turn generates data to overcome the data diversity gap. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s1024/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;835&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s1072/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;561&quot; data-original-width=&quot;1072&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Causal loop diagram of the health diagnostics data gap&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; These community-sourced causal theories are a first step to bridge the problem understanding chasm with trustworthy societal context knowledge. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; As discussed in this blog, the problem understanding chasm is a critical open challenge in responsible AI. SCOUTS conducts exploratory and applied research in collaboration with other teams within Google Research, external community, and academic partners across multiple disciplines to make meaningful progress solving it. Going forward our work will focus on three key elements, guided by our &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;: &lt;/p>; &lt;ol>; &lt;li>;Increase awareness and understanding of the problem understanding chasm and its implications through talks, publications, and training. &lt;/li>;&lt;li>;Conduct foundational and applied research for representing and integrating societal context knowledge into AI product development tools and workflows, from conception to monitoring, evaluation and adaptation. &lt;/li>;&lt;li>;Apply community-based causal modeling methods to the AI health equity domain to realize impact and build society&#39;s and Google&#39;s capability to produce and leverage global-scale societal context knowledge to realize responsible AI. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s960/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SCOUTS flywheel for bridging the problem understanding chasm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;Thank you to John Guilyard for graphics development, everyone in SCOUTS, and all of our collaborators and sponsors.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4421751509192561388/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html&quot; rel=&quot;alternate&quot; title=&quot;Using societal context knowledge to foster the responsible application of AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s72-c/scouts.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8123471024413959829&lt;/id>;&lt;published>;2023-07-18T13:15:00.000-07:00&lt;/published>;&lt;updated>;2023-07-18T13:15:15.633-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Self-Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SimPer: Simple self-supervised learning of periodic targets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Daniel McDuff, Staff Research Scientist, and Yuzhe Yang, Student Researcher, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s320/SimPer%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; Learning from periodic data (signals that repeat, such as a heart beat or the daily temperature changes on Earth&#39;s surface) is crucial for many real-world applications, from &lt;a href=&quot;https://cloud.google.com/blog/topics/sustainability/weather-prediction-with-ai&quot;>;monitoring weather systems&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/take-pulse-health-and-wellness-your-phone/&quot;>;detecting vital signs&lt;/a>;. For example, in the environmental remote sensing domain, periodic learning is often needed to enable nowcasting of environmental changes, such as &lt;a href=&quot;https://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html&quot;>;precipitation patterns or land surface temperature&lt;/a>;. In the health domain, learning from video measurement has shown to extract (quasi-)periodic vital signs such as &lt;a href=&quot;https://www.ahajournals.org/doi/full/10.1161/JAHA.118.008585&quot;>;atrial fibrillation&lt;/a>; and &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9298820&quot;>;sleep apnea episodes&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Approaches like &lt;a href=&quot;https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html&quot;>;RepNet&lt;/a>; highlight the importance of these types of tasks, and present a solution that recognizes repetitive activities within a single video. However, these are supervised approaches that require a significant amount of data to capture repetitive activities, all labeled to indicate the number of times an action was repeated. Labeling such data is often challenging and resource-intensive, requiring researchers to manually capture gold-standard temporal measurements that are synchronized with the modality of interest (eg, video or satellite imagery). &lt;/p>; &lt;p>; 或者，自我监督学习（SSL）方法（例如，&lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html &quot;>;SimCLR&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;)，它们利用大量未标记的数据来学习捕获周期性或准周期的表示-周期性时间动态，在&lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j.html&quot;>;解决分类任务&lt;/a>;方面取得了成功。然而，他们忽视了数据中固有的周期性（即识别帧是否是周期性过程的一部分的能力），并且无法学习捕获周期性或频率属性的鲁棒表示。 This is because periodic learning exhibits characteristics that are distinct from prevailing learning tasks. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s1338/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1338&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Feature similarity is different in the context of periodic representations as compared to static features (eg, images). For example, videos that are offset by short time delays or are reversed should be similar to the original sample, whereas videos that have been upsampled or downsampled by a factor &lt;em>;x&lt;/em>; should be different from the original sample by a factor of &lt;em>;x&lt;/em>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To address these challenges, in “&lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;SimPer: Simple Self-Supervised Learning of Periodic Targets&lt;/a>;”, published at the eleventh &lt;a href=&quot;https://iclr.cc/&quot;>;International Conference on Learning Representations&lt;/a>; (ICLR 2023), we introduced a self-supervised contrastive framework for learning periodic information in data. Specifically, SimPer leverages the temporal properties of periodic targets using &lt;em>;temporal self-contrastive learning&lt;/em>;, where positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the &lt;em>;same&lt;/em>; input instance. We propose &lt;em>;periodic feature similarity&lt;/em>; that explicitly defines how to measure similarity in the context of periodic learning.此外，我们设计了一种广义对比损失&lt;em>;&lt;/em>;，将经典的&lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE损失&lt;/a>;扩展到软回归变体可以对连续标签（频率）进行对比。 Next, we demonstrate that SimPer effectively learns period feature representations compared to state-of-the-art SSL methods, highlighting its intriguing properties including better data efficiency, robustness to spurious correlations, and generalization to distribution shifts. Finally, we are excited to release the &lt;a href=&quot;https://github.com/YyzHarry/SimPer&quot;>;SimPer code repo&lt;/a>; with the research community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The SimPer framework&lt;/h2>; &lt;p>; SimPer introduces a temporal self-contrastive learning framework. Positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the same input instance. For temporal video examples, periodicity-invariant changes are cropping, rotation or flipping, whereas periodicity-variant changes involve increasing or decreasing the speed of a video. &lt;/p>; &lt;p>; To explicitly define how to measure similarity in the context of periodic learning, SimPer proposes periodic feature similarity. This construction allows us to formulate training as a contrastive learning task. A model can be trained with data without any labels and then fine-tuned if necessary to map the learned features to specific frequency values. &lt;/p>; &lt;p>; Given an input sequence &lt;em>;x&lt;/em>;, we know there&#39;s an underlying associated periodic signal. We then transform &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo- speed or frequency labels for the unlabeled input x. &lt;/p>; &lt;p>; Conventional &lt;a href=&quot;https://en.wikipedia.org/wiki/Similarity_measure&quot;>;similarity measures&lt;/a>; such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; emphasize strict proximity between two feature vectors, and are sensitive to index shifted features (which represent different time stamps), reversed features, and features with changed frequencies. In contrast, periodic feature similarity should be high for samples with small temporal shifts and or reversed indexes, while capturing a continuous similarity change when the feature frequency varies. This can be achieved via a similarity metric in the frequency domain, such as the distance between two &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;>;Fourier transforms&lt;/a>;. &lt;/p>; &lt;p>; To harness the intrinsic continuity of augmented samples in the frequency domain, SimPer designs a generalized contrastive loss that extends the classic &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE&lt;/a>; loss to a soft regression variant that enables contrasting over continuous labels (frequency). This makes it suitable for regression tasks, where the goal is to recover a continuous signal, such as a heart beat. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;361&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SimPer constructs negative views of data through transformations in the frequency domain. The input sequence &lt;em>;x&lt;/em>; has an underlying associated periodic signal. SimPer transforms &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo speed or frequency labels for unlabeled input &lt;em>;x&lt;/em>; (periodicity-variant augmentations &lt;em>;τ&lt;/em>;). SimPer takes transformations that do not change the identity of the input and defines these as periodicity-invariant augmentations &lt;em>;σ&lt;/em>;, thus creating different positive views of the sample. Then, it sends these augmented views to the encoder &lt;em>;f&lt;/em>;,&lt;em>; &lt;/em>;which extracts corresponding features. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate SimPer&#39;s performance, we benchmarked it against state-of-the-art SSL schemes (eg, &lt;a href=&quot;https://github.com/google-research/simclr&quot;>;SimCLR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf&quot;>;BYOL&lt;/a>;, &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Spatiotemporal_Contrastive_Video_Representation_Learning_CVPR_2021_paper.html&quot;>;CVRL&lt;/a>;) on a set of six diverse periodic learning datasets for common real-world tasks in human behavior analysis, environmental remote sensing, and healthcare. Specifically, below we present results on heart rate measurement and exercise repetition counting from video. The results show that SimPer outperforms the state-of-the-art SSL schemes across all six datasets, highlighting its superior performance in terms of data efficiency, robustness to spurious correlations, and generalization to unseen targets. &lt;/p>; &lt;p>; Here we show quantitative results on two representative datasets using SimPer pre-trained using various SSL methods and fine-tuned on the labeled data. First, we pre-train SimPer using the &lt;a href=&quot;https://sites.google.com/corp/view/ybenezeth/ubfcrppg&quot;>;Univ. Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy&lt;/a>; (UBFC) dataset, a human &lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram#Remote_photoplethysmography&quot;>;photoplethysmography&lt;/a>; and heart rate prediction dataset, and compare its performance to state-of-the-art SSL methods. We observe that SimPer outperforms SimCLR, MoCo v2, BYOL, and CVRL methods. The results on the human action counting dataset, &lt;a href=&quot;https://sites.google.com/corp/view/repnet&quot;>;Countix&lt;/a>;, further confirm the benefits of SimPer over others methods as it notably outperforms the supervised baseline. For the feature evaluation results and performance on other datasets, please refer to the &lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorce K9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s1999/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1 YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Univ 上 SimCLR、MoCo v2、BYOL、CVRL 和 SimPer 的结果。勃艮第弗朗什孔泰远程光电体积描记法 (UBFC) 和 Countix 数据集。心率和重复次数表现报告为&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;平均绝对误差&lt;/a>; (MAE)。&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和应用&lt;/h2>; &lt;p>; 我们提出 SimPer，一个用于学习数据中周期性信息的自监督对比框架。我们证明，通过结合时间自对比学习框架、周期性不变和周期性变化增强以及连续周期性特征相似性，SimPer 提供了一种直观且灵活的方法来学习周期性信号的强特征表示。此外，SimPer可以应用于从环境遥感到医疗保健的各个领域。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢杨宇哲、刘鑫、Ming-Zher Poh、Jiang Wu、Silviu Borac 和 Dina Katabi 对这项工作的贡献。 &lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8123471024413959829/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html#comment-form&quot; rel =&quot;回复&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;edit&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html&quot; rel=&quot;alternate&quot; title=&quot;SimPer：周期性目标的简单自监督学习&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com &lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded .gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN -dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR _u0zB0Qiabo_g92yjjDcc4SEhz/s72-c/SimPer%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2013323302512835157&lt;/id>;&lt;已发布>;2023-07-13T14:01 :00.001-07:00&lt;/已发布>;&lt;更新>;2023-07-13T14:01:18.428-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;机器智能&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;符号调整可改善语言模型中的上下文学习&lt;/stitle>;&lt;content type= &quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究中心学生研究员 Jerry Wei 和首席科学家 Denny Zhou&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRicUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiI j-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s1200/SymbolTuning.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人类智能的一个关键特征是，人类可以仅使用几个例子进行推理来学习执行新任务。扩展语言模型解锁了机器学习中的一系列新应用和范例，包括通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;执行具有挑战性的推理任务的能力>;情境学习&lt;/a>;。然而，语言模型仍然对给出提示的方式敏感，这表明它们没有以稳健的方式进行推理。例如，语言模型通常需要繁重的提示工程或措辞任务作为指令，并且它们会表现出意想不到的行为，例如 &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do- in-context.html&quot;>;即使显示不正确的标签，任务性能也不受影响&lt;/a>;。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2305.08298&quot;>;符号调整改善了语言模型中的上下文学习&lt; /a>;”，我们提出了一个简单的微调过程，称为&lt;em>;符号调整&lt;/em>;，它可以通过强调输入标签映射来改进上下文学习。我们在 &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalized.html&quot;>;Flan-PaLM&lt;/a>; 模型中尝试符号调整，并观察各种设置的好处。 &lt;/p>; &lt;ul>; &lt;li>;符号调整可以提高未见过的上下文学习任务的性能，并且对于未指定的提示（例如没有说明或没有自然语言标签的提示）更加稳健。 &lt;/li>;&lt;li>;符号调整模型在算法推理任务上要强大得多。 &lt;/li>;&lt;li>;最后，符号调整模型在上下文中呈现的翻转标签方面显示出巨大的改进，这意味着它们更有能力使用上下文信息来覆盖先验知识。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q 33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C /s1035/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;663&quot; data-original-width =“1035”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a 5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s16000/image6.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;符号调整概述，其中模型针对自然语言标签替换为任意符号的任务进行微调。符号调整依赖于这样的直觉：当指令和相关标签不可用时，模型必须使用上下文示例来学习任务。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line -height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;动机&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more -generalized.html&quot;>;指令调优&lt;/a>;是一种常见的微调方法，已被证明可以提高性能并允许模型更好地遵循上下文示例。然而，一个缺点是模型不会被迫学习使用示例，因为任务是通过指令和自然语言标签在评估示例中冗余定义的。例如，在上图左侧，虽然示例可以帮助模型理解任务（情感分析），但它们并不是绝对必要的，因为模型可以忽略示例，只阅读指示任务是什么的指令。 &lt;/p>; &lt;p>; 在符号调整中，模型对示例进行微调，其中删除了指令，并将自然语言标签替换为语义不相关的标签（例如“Foo”、“Bar”等）。在此设置中，如果不查看上下文示例，任务就不清楚。例如，在上图右侧，需要多个上下文示例才能弄清楚任务。由于符号调整教会模型对上下文中的示例进行推理，因此符号调整后的模型在需要在上下文中的示例与其标签之间进行推理的任务上应该具有更好的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpk HGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s1035 /image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;237&quot; data-original-width=&quot;第1035章vaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s16000/image7.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用于符号调整的数据集和任务类型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;符号调整过程&lt;/h2>; &lt;p>; 我们选择了 22 个公开可用的&lt;a href=&quot;https:// /en.wikipedia.org/wiki/Natural_language_processing&quot;>;我们用于符号调整过程的自然语言处理&lt;/a>; (NLP) 数据集。这些任务在过去已经被广泛使用，我们只选择分类类型的任务，因为我们的方法需要离散标签。然后，我们将标签重新映射到一组约 30K 任意标签中的随机标签，这些标签选自以下三个类别之一：整数、字符组合和单词。 &lt;/p>; &lt;p>; 对于我们的实验，我们对 &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>; 进行符号调优，这是 &lt;a href= 的指令调优变体“https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;。我们使用三种不同尺寸的 Flan-PaLM 型号：Flan-PaLM-8B、Flan-PaLM-62B 和 Flan-PaLM-540B。我们还测试了 &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;Flan-cont-PaLM-62B&lt;/a>;（Flan-PaLM-62B 为 1.3T 代币而不是 780B 代币），我们缩写为62B-c。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx -KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s861 /image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;168&quot; data-original-width=&quot;第861章MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们使用来自三个类别（整数、字符组合和单词）的一组 ∼300K 任意符号。 ∼30K 符号用于调优，其余符号用于评估。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt; /div>; &lt;h2>;实验设置&lt;/h2>; &lt;p>; 我们想要评估模型执行看不见的任务的能力，因此我们无法评估符号调整中使用的任务（22 个数据集）或指令调整期间使用的任务（1.8K 任务） ）。因此，我们选择了 11 个微调期间未使用的 NLP 数据集。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;上下文学习&lt;/h2>; &lt;p>; 在符号调整过程中，模型必须学会与上下文中的示例进行推理，以便成功执行任务，因为修改了提示以确保任务不能简单地从相关标签或说明中学习。经过符号调整的模型在任务不明确且需要在上下文示例及其标签之间进行推理的环境中应该表现得更好。为了探索这些设置，我们定义了四种上下文学习设置，这些设置改变了输入和标签之间所需的推理量，以便学习任务（基于指令/相关标签的可用性）&lt;/p>; &lt;tablealign=&quot;中心&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text -align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9 -LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s936/image2。 png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;936&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ld A-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s16000/image2.png&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据指令和相关自然语言标签的可用性，模型可能需要进行不同数量的推理并附有上下文示例。当这些功能不可用时，模型必须根据给定的上下文示例进行推理才能成功执行任务。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;符号调整可提高所有设置的性能模型 62B 及更大，具有相关自然语言标签的设置略有改进（+0.8% 至 +4.2%），而无相关自然语言标签的设置则有重大改进（+5.5% 至 +15.5%）。引人注目的是，当相关标签不可用时，符号调整的 Flan-PaLM-8B 优于 FlanPaLM-62B，符号调整的 Flan-PaLM-62B 优于 Flan-PaLM-540B。这种性能差异表明，符号调整可以允许更小的模型在这些任务上与大型模型一样执行（有效地节省约 10 倍的推理计算）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7 iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s900/image1.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;495&quot; data-original-width=&quot;900&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziu OIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;足够大的符号调整模型比基线更适合上下文学习，特别是在相关标签不可用的设置中。性能显示为十一项任务的平均模型准确度 (%)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div >; &lt;h2>;算法推理&lt;/h2>; &lt;p>; 我们还对 &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>; 的算法推理任务进行实验。主要有两组任务：1) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/list_functions&quot;>;列出函数&lt;/a>; - 识别转换包含非负整数的输入和输出列表之间的函数（例如，删除列表中的最后一个元素）； 2) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/simp_turing_concept&quot;>;简单的图灵概念&lt;/a>; - 使用二进制字符串进行推理，以了解以下概念：将输入映射到输出（例如，交换字符串中的 0 和 1）。 &lt;/p>; &lt;p>; 在列表函数和简单图灵概念任务上，符号调整的平均性能分别提高了 18.2% 和 15.3%。此外，具有符号调整功能的 Flan-cont-PaLM-62B 在列表函数任务上的平均性能优于 Flan-PaLM-540B，这相当于推理计算量减少了约 10 倍。这些改进表明符号调整增强了模型在上下文中学习未见过的任务类型的能力，因为符号调整不包含任何算法数据。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5 XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB -jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s908/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;496&quot; data-original -width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL _Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s16000/image3.png&quot;/>;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;符号调整模型在列表函数任务和简单图灵概念任务上实现更高的性能。 (A–E)：列表函数任务的类别。 (F)：简单的图灵概念任务。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;翻转标签&lt;/h2>; &lt;p>; 在翻转标签实验中，上下文和评估示例的标签被翻转，这意味着先验知识和输入标签映射不一致（例如，包含积极情绪的句子被标记为“消极情绪”） ，从而使我们能够研究模型是否可以覆盖先验知识。 &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;之前的工作&lt;/a>;表明，虽然预训练模型（没有指令调优）在某种程度上可以遵循上下文中呈现的翻转标签，指令调优降低了这种能力。 &lt;/p>; &lt;p>; 我们发现所有模型大小都存在类似的趋势 - 符号调整模型比指令调整模型更能遵循翻转标签。我们发现，经过符号调整后，Flan-PaLM-8B 在所有数据集上的平均改进为 26.5%，Flan-PaLM-62B 的改进为 33.7%，Flan-PaLM-540B 的改进为 34.0%。此外，经过符号调整的模型与仅预训练模型的平均性能相似或更好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOS K4VS -3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s914/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt; img border =“0”数据原始高度=“434”数据原始-width=&quot;914&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3Tlq SJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s16000/image5.png&quot; />;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;符号调整模型比上下文中呈现的翻转标签更擅长跟踪指令调整模型是。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2 >; &lt;p>; 我们提出了符号调优，这是一种在将自然语言标签重新映射到任意符号的任务中调整模型的新方法。符号调整基于这样的直觉：当模型无法使用指令或相关标签来确定所呈现的任务时，它必须通过从上下文示例中学习来实现这一点。我们使用符号调优程序调优了四种语言模型，利用 22 个数据集和大约 30K 任意符号作为标签的调优混合物。 &lt;/p>; &lt;p>; 我们首先表明，符号调整可以提高未见过的上下文学习任务的性能，特别是当提示不包含说明或相关标签时。我们还发现，尽管符号调整过程中缺乏数值或算法数据，但符号调整模型在算法推理任务方面表现得更好。最后，在输入具有翻转标签的上下文学习环境中，符号调整（对于某些数据集）可以恢复在指令调整期间丢失的遵循翻转标签的能力。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;未来的工作&lt;/h2>; &lt;p>;通过符号调整，我们的目标是提高模型的程度可以在上下文学习期间检查输入-标签映射并从中学习。我们希望我们的结果能够鼓励进一步努力提高语言模型对上下文中呈现的符号进行推理的能力。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者现已加入 Google DeepMind 。这项工作由 Jerry Wei、Le Hou、Andrew Lampinen、Xiangning Chen、Da Huang、Yi Tay、Xinyun Chen、Yifeng Lu、Denny Zhou、Tengyu Ma 和 Quoc V. Le 进行。我们要感谢 Google Research 和 Google DeepMind 的同事提供的建议和有益的讨论。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2013323302512835157 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/symbol-tuning- Improves-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626 /posts/default/2013323302512835157&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel= “self” type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/symbol-tuning-improves-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;符号调整改善了语言模型中的上下文学习&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https: //img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yK WoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s72-c/SymbolTuning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:缩略图>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8984134419460793359&lt;/id>;&lt;发布>;2023-07- 11T10:00:00.010-07:00&lt;/发布>;&lt;更新>;2023-07-11T10:44:15.111-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;开源&quot;>;&lt;/category>;&lt;category schema=&quot; http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;机器学习辅助计算机架构设计的开源体育馆&lt;/stitle>;&lt; content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Amir Yazdanbakhsh 和访问研究员 Vijay Janapa Reddi&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7St Ub4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s1200/ArchGym-animation2.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_architecture&quot;>;计算机体系结构&lt;/a>;研究在开发模拟器和工具来评估和塑造计算机系统的设计方面有着悠久的历史。例如，&lt;a href=&quot;https://ieeexplore.ieee.org/iel5/2/21180/00982917.pdf?casa_token=M_ZAQmgCbKkAAAAA:Y9wFTB9OQBwzXXpw7kNbq4asdlCCHYRaq7qsqcRBpYyh6734aHmr57ll4Vb1_zcG5ukNv NONNQ&quot;>;SimpleScalar&lt;/a>; 模拟器于 20 世纪 90 年代末推出并允许研究人员探索各种&lt;a href=&quot;https://en.wikipedia.org/wiki/Microarchitecture&quot;>;微架构&lt;/a>;想法。计算机架构模拟器和工具，例如 &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHg Med-f-zIkC1q6_OUX11TzJQ&quot;>;gem5&lt;/a>;, &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>; 等在推进计算机体系结构研究方面发挥了重要作用。从那时起，这些共享的资源和基础设施使工业界和学术界受益，并使研究人员能够系统地借鉴彼此的工作，从而导致该领域取得重大进展。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 尽管如此，计算机体系结构研究正在不断发展，行业和学术界转向机器学习 (ML) 优化，以满足严格的特定领域要求，例如 &lt; a href=&quot;https://ai.googleblog.com/2021/02/machine-learning-for-computer.html&quot;>;计算机架构机器学习&lt;/a>;、&lt;a href=&quot;https://arxiv.org /pdf/2201.01863.pdf&quot;>;用于 TinyML 加速的机器学习&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022/03/offline-optimization-for-architecting.html&quot;>;DNN &lt;/a>; &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9804604&quot;>;加速器&lt;/a>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/ 10.1145/3503222.3507767&quot;>;数据路径优化&lt;/a>;、&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1394608.1382172?casa_token=3g46kAHeN0QAAAAA:5pKdYXamA_vstsO5LqA0_4rRy5o2Z4 6Ks-OJLDUe7ZSLtDfkaSS5i0zLfMe3Y7gAuY2bFMZ1yGOEMA&quot;>;内存控制器&lt;/a>; 、&lt;a href=&quot;https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40&quot;>;功耗&lt;/a>;、&lt;a href= “https://ieeexplore.ieee.org/document/7430287&quot;>;安全&lt;/a>;，以及&lt;a href=&quot;https://www.sigarch.org/tag/privacy-preserving-computing/&quot;>;隐私&lt; /a>;.尽管之前的工作已经证明了机器学习在设计优化方面的优势，但缺乏强大的、可重复的基线阻碍了不同方法之间的公平和客观比较，并给它们的部署带来了一些挑战。为了确保稳步进展，必须共同理解和应对这些挑战。 &lt;/p>; &lt;p>; 为了缓解这些挑战，请参阅“&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3579371.3589049&quot;>;ArchGym：用于机器学习辅助架构设计的开源健身房&lt; /a>;”，在 &lt;a href=&quot;https://www.iscaconf.org/isca2023/program/&quot;>;ISCA 2023&lt;/a>; 上接受，我们引入了 ArchGym，其中包括各种计算机架构模拟器和 ML 算法。在 ArchGym 的支持下，我们的结果表明，只要有足够多的样本，任何不同的 ML 算法集合都能够为每个目标问题找到最佳的架构设计参数集； &lt;i>;没有一种解决方案一定比另一种更好&lt;/i>;。这些结果进一步表明，为给定的 ML 算法选择最佳超参数对于寻找最佳架构设计至关重要，但选择它们并非易事。我们&lt;a href=&quot;https://bit.ly/ArchGym&quot;>;发布&lt;/a>;跨多个计算机架构模拟和机器学习算法的代码和数据集。&lt;/p>; &lt;br />; &lt;h2>;机器学习中的挑战-辅助架构研究 &lt;/h2>; &lt;p>; 机器学习辅助架构研究提出了多项挑战，包括： &lt;/p>; &lt;ol>; &lt;li>;对于特定的机器学习辅助计算机架构问题（例如，为&lt; a href=&quot;https://en.wikipedia.org/wiki/Dynamic_random-access_memory&quot;>;DRAM&lt;/a>; 控制器）没有系统的方法来识别最佳机器学习算法或超参数（例如学习速率、预热步骤） ， ETC。）。机器学习和启发式方法的范围更广，从&lt;a href=&quot;https://arxiv.org/abs/2008.03639&quot;>;随机游走&lt;/a>;到&lt;a href=&quot;http://incompleteideas.net/ book/RLbook2020.pdf&quot;>;强化学习&lt;/a>; (RL)，可用于&lt;a href=&quot;https://en.wikipedia.org/wiki/Design_space_exploration&quot;>;设计空间探索&lt;/a>;（经济发展局）。虽然这些方法在其选择的基线上显示出显着的性能改进，但尚不清楚这些改进是由于优化算法还是超参数的选择所致。&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;因此，为了确保可重复性并促进机器学习辅助架构 DSE 的广泛采用，有必要概述系统的基准测试方法。&lt;/em>; &lt;br />;&lt;br />; &lt; /li>; &lt;li>;虽然计算机架构模拟器一直是架构创新的支柱，但越来越需要解决架构探索中的准确性、速度和成本之间的权衡。性能估计的准确性和速度因模拟器而异，具体取决于底层建模细节（例如，&lt;a href=&quot;https://biblio.ugent.be/publication/2968322/file/6776727.pdf&quot;>;周期&lt;/a>;-&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f -zIkC1q6_OUX11TzJQ&quot;>;准确&lt;/a>;与&lt;a href =&quot;https://arxiv.org/abs/2210.03894&quot;>;机器学习&lt;/a>;-&lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;基于&lt;/a>; &lt;a href=&quot;https ://arxiv.org/abs/1808.07412&quot;>;代理&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1803.02329&quot;>;模型&lt;/a>;）。虽然分析或基于机器学习的代理模型由于丢弃低级细节而非常灵活，但它们通常会出现较高的预测误差。此外，由于商业许可，&lt;a href=&quot;https://ieeexplore.ieee.org/document/7945172&quot;>;从模拟器收集的运行次数可能有严格的限制&lt;/a>;。总体而言，这些约束表现出明显的性能与样本效率权衡，影响架构探索的优化算法的选择。 &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;描述如何在这些约束下系统地比较各种机器学习算法的有效性具有挑战性。&lt;/ em>; &lt;br />; &lt;br />; &lt;/li>; &lt;li>;最后，机器学习算法的前景正在迅速发展，一些机器学习算法需要数据才能发挥作用。此外，将 DSE 的结果呈现为有意义的工件（例如数据集）对于深入了解设计空间至关重要。 &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;在这个快速发展的生态系统中，确保如何分摊架构搜索算法的开销至关重要勘探。目前尚不清楚，也没有系统地研究如何在不了解底层搜索算法的情况下利用探索数据。&lt;/em>; &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;ArchGym 设计&lt;/h2>; &lt;p>; ArchGym 通过提供一个统一的框架来公平地评估不同的基于 ML 的搜索算法来解决这些挑战。它包含两个主要组件：1) ArchGym 环境和 2) ArchGym 代理。环境是架构成本模型的封装，其中包括延迟、吞吐量、面积、能源等，在给定一组与目标工作负载配对的架构参数的情况下，确定运行工作负载的计算成本。代理是用于搜索的 ML 算法的封装，由超参数和指导策略组成。超参数是模型要优化的算法所固有的，可以显着影响性能。另一方面，策略决定代理如何迭代选择参数来优化目标。 &lt;/p>; &lt;p>; 值得注意的是，ArchGym 还包括一个连接这两个组件的标准化接口，同时还将探索数据保存为 ArchGym 数据集。该接口的核心包含三个主要信号：&lt;i>;硬件状态&lt;/i>;、&lt;i>;硬件参数&lt;/i>;和&lt;i>;指标&lt;/i>;。这些信号是在环境和代理之间建立有意义的通信通道的最低限度。使用这些信号，代理可以观察硬件的状态并建议一组硬件参数来迭代优化（用户定义的）奖励。奖励是硬件性能指标的函数，例如性能、能耗等。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE -k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s1226/ArchGym-animation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot;数据原始高度=“722”数据原始宽度=“1226”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32 -fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s16000/ArchGym -animation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ArchGym 包含两个主要组件：ArchGym环境和 ArchGym 代理。 ArchGym 环境封装了成本模型，代理是策略和超参数的抽象。通过连接这两个组件的标准化接口，ArchGym 提供了一个统一的框架来公平地评估不同的基于 ML 的搜索算法，同时还将探索数据保存为 ArchGym 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;br />; &lt;h2>;ML 算法同样有利于满足用户定义的目标规范&lt;/h2>; &lt;p>; 使用 ArchGym，我们凭经验证明，在不同的优化目标和 DSE 问题中，&lt;em>;至少有一组存在的超参数可以实现与其他机器学习算法相同的硬件性能&lt;/em>;。机器学习算法或其基线选择不当（随机选择）的超参数可能会导致错误的结论，即特定的机器学习算法系列比其他机器学习算法更好。我们表明，通过足够的超参数调整，不同的搜索算法，甚至是随机游走（RW），都能够识别出最好的可能报酬。然而，请注意，找到正确的超参数集可能需要详尽的搜索，甚至需要运气才能使其具有竞争力。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ 6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8 /s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1158&quot; data-original-width=&quot;1999&quot; 高度=“371”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3 Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/w640-h371/image1.png&quot;宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;有足够数量的样本，至少存在一组的超参数可以在一系列搜索算法中产生相同的性能。这里的虚线代表最大标准化奖励。 &lt;i>;Cloud-1&lt;/i>;、&lt;i>;cloud-2&lt;/i>;、&lt;i>;stream&lt;/i>; 和 &lt;i>;random&lt;/i>; 表示 &lt;a href= 的四种不同的内存跟踪&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>;（DRAM子系统设计空间探索框架）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;数据集构建和高保真代理模型训练&lt;/h2>; &lt;p>;使用 ArchGym 创建统一界面还可以创建数据集，这些数据集可用于设计更好的数据驱动的基于 ML 的代理架构成本模型，以改进架构模拟的速度。为了评估数据集在构建 ML 模型以估算架构成本方面的优势，我们利用 ArchGym 记录 DRAMSys 每次运行数据的能力来创建四个数据集变体，每个变体具有不同数量的数据点。对于每个变体，我们创建两个类别：（a）多样化数据集，代表从不同代理收集的数据（&lt;a href=&quot;https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms&quot;>;ACO&lt;/a>;， &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_algorithm&quot;>;GA&lt;/a>;，&lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;RW&lt;/a>; >; 和 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expexpcious%2Dto%2Devaluate%20functions.&quot;>;BO&lt;/ a>;) 和 (b) 仅 ACO，显示专门从 ACO 代理收集的数据，两者均随 ArchGym 一起发布。我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;>;随机森林回归&lt;/a>;在每个数据集上训练代理模型，目的是预测&lt;a href>;的设计延迟=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAM 模拟器&lt;/a>;。我们的结果表明：&lt;/p>; &lt;ol>; &lt;li>;随着数据集大小的增加，平均归一化&lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;根均方误差&lt;/a>; (RMSE) 略有下降。 &lt;/li>;&lt;li>;然而，当我们在数据集中引入多样性（例如，从不同的代理收集数据）时，我们观察到不同数据集大小的 RMSE 降低了 9 倍到 42 倍。 &lt;/li>; &lt;/ol>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right : auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp8 2jCJZQrqLc67DI -feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/s1826/ArchGym2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0 “数据原始高度=“1143”数据原始宽度= “1826”高度=“401”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67 DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/w640-h401/ArchGym2.jpg&quot;宽度=“640” />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用 ArchGym 界面跨不同代理收集不同的数据集。&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3- XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWT9d/s 1803/ArchGym1.jpg&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1803&quot; height=&quot;407&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_ q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/w640-h407/ArchGym1.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;不同数据集和数据集大小对标准化 RMSE 的影响。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt; h2>;需要一个社区驱动的生态系统来进行 ML 辅助架构研究&lt;/h2>; &lt;p>;ArchGym 是创建一个开源生态系统的初步努力，该生态系统 (1) 将广泛的搜索算法连接到计算机以统一且易于扩展的方式构建架构模拟器，(2) 促进 ML 辅助计算机架构的研究，(3) 形成开发可重复基线的支架，有许多开放挑战需要社区范围的支持。下面我们概述了机器学习辅助架构设计中的一些开放挑战。应对这些挑战需要良好协调的努力和社区驱动的生态系统。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left : auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl /AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKu RYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;969&quot;数据原始宽度=“1999”高度=“310”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzO Z1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/w640-h310/image2.png &quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;机器学习辅助架构中的主要挑战设计。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们称这个生态系统&lt;a href=&quot;https://www.sigarch.org/architecture-2-0-why-computer- Architectures-need-a-data-centric-ai-gymnasium/&quot;>;架构 2.0&lt;/a>;。我们概述了建立跨学科研究人员的包容性生态系统的主要挑战和愿景，以解决长期存在的开放性问题将机器学习应用于计算机架构研究。如果您有兴趣帮助塑造这个生态系统，请填写&lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSfIYeSBoEi-DIHizPx4-FTEcZUSY_uUcKe0rdHC0tkCWp3Gag/ viewform&quot;>;兴趣调查&lt;/a>;。&lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>;&lt;a href=&quot;https://bit.ly/ArchGym&quot;>;ArchGym&lt;/a>;是 ML 架构 DSE 的开源体育馆，支持标准化接口，可以轻松扩展以适应不同的用例。此外，ArchGym 可以在不同的 ML 算法之间进行公平且可重复的比较，并有助于为计算机架构研究问题建立更强的基线。 &lt;/p>; &lt;p>; 我们邀请计算机架构社区以及机器学习社区积极参与 ArchGym 的开发。我们相信，为计算机架构研究创建一个体育馆式的环境将是该领域向前迈出的重要一步，并为研究人员使用机器学习加速研究并带来新的创新设计提供一个平台。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>;&lt;i>;这篇博文基于 Google 和哈佛大学的几位合著者的共同努力。&lt;/i>;&lt;i>;我们感谢并强调 Srivatsan Krishnan（哈佛），他与 Shvetank Prakash（哈佛）、Jason Jabbour（哈佛）、Ikechukwu Uchendu（哈佛）、Susobhan Ghosh（哈佛）、Behzad Boroujerdian（哈佛）合作为本项目贡献了一些想法、Daniel Richins（哈佛）、Devashree Tripathy（哈佛）和 Thierry Thambe（哈佛）。此外，我们还要感谢 James Laudon、Douglas Eck、Cliff Young 和 Aleksandra Faust 对这项工作的支持、反馈和动力。我们还要感谢约翰·吉利亚德在这篇文章中使用的动画人物。 Amir Yazdanbakhsh 现在是 Google DeepMind 的研究科学家，Vijay Janapa Reddi 是哈佛大学的副教授。&lt;/i>;&lt;/p>;&lt;div>;&lt;br />;&lt;/div>;&lt;br />;&lt;br />;&lt;/内容>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8984134419460793359/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://ai.googleblog.com/2023/07/an-open-source-gymnasium-for-computer.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com /2023/07/an-open-source-gymnasium-for-computer.html&quot; rel=&quot;alternate&quot; title=&quot;机器学习辅助计算机架构设计的开源体育馆&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot; 16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/ gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1vo UVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s72-c/ArchGym-animation2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt; id>;标签：blogger.com，1999：blog-8474926331452026626.post-7093565002706757508&lt;/id>;&lt;发布>;2023-07-10T06:02:00.005-07:00&lt;/发布>;&lt;更新>;2023-07-21T13： 24:14.452-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category >;&lt;title type=&quot;text&quot;>;Google 在 ACL 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 项目经理 Malaya Jules&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cM huxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s1040/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; style=&quot;显示: 无;” />; &lt;p>; 本周，&lt;a href=&quot;https://www.aclweb.org/&quot;>;第 61 届年会&lt;/a>; &quot;>;计算语言学协会 (ACL) 是一次重要会议，涵盖与自然语言计算方法相关的广泛研究领域，正在不列颠哥伦比亚省温哥华举行。作为自然语言处理和理解领域的领导者，以及&lt;a href=&quot;https://2023.aclweb.org/sponsors/&quot;>;钻石级赞助商&lt;/a>;&amp;nbsp;&lt;a href=&quot;https: //2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;，Google 将通过 50 多篇出版物展示该领域的最新研究成果，并积极参与各种研讨会和教程。&lt;/p>; &lt;a name= &#39;更多&#39;>;&lt;/a>; &lt;p>;如果您已注册参加 ACL 2023，我们希望您能够参观 Google 展位，详细了解 Google 的项目，这些项目旨在为数十亿人解决有趣的问题。您还可以在下面了解有关 Google 参与的更多信息（Google 隶属关系以&lt;b>;粗体&lt;/b>;显示）。&lt;/p>; &lt;br />; &lt;h2>;董事会和组委会&lt;/h2>; &lt;div style=&quot;margin-left : 20px;&quot;>; &lt;p>; 区域椅子包括：&lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>; &lt;br />; 工作坊椅子包括：&lt;strong>;&lt;em>;Annie Louis&lt;/em>;&lt;/ strong>; &lt;br />; 出版主席包括：&lt;strong>;&lt;em>;雷舒&lt;/em>;&lt;/strong>; &lt;br />; 程序委员会包括：&lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>; 、&lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;聚光灯论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09648.pdf&quot;>;NusaCrowd：印度尼西亚 NLP 资源开源倡议&lt;/a>; &lt;br />; &lt;em>;Samuel Cahyawijaya、Holy Lovenia、Alham Fikri Aji、Genta Winata、Bryan Wilie、Fajri Koto、Rahmad Mahendra、Christian Wibisono、Ade Romadhony、Karissa Vincentio、Jennifer Santoso、David Moeljadi、Cahya Wirawan , Frederikus Hudi, Muhammad Satrio Wicaksono, Ivan Parmonangan, Ika Alfina, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh Dhole, Arie Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Adilazuarda、Ryan Hadiwijaya、Ryandito Diandaru、Tiezheng Yu、Vito Ghifari、戴文亮、Yan Xu、Dyah Damapuspita、Haryo Wibowo、Cuk Tho、Ichwanul Karo Karo、Tirana Fatyanosa、Ziwei Ji、Graham Neubig、Timothy Baldwin、&lt;strong>;Sebastian Ruder&lt;/strong>;、Pascale Fung、Herry Sujaini、Sakriani Sakti、Ayu Purwarianti&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2205.12680.pdf&quot;>;优化测试-密集检索的时间查询表示&lt;/a>; &lt;br />; &lt;em>;Mujeen Sung、Jungsoo Park、Jaewoo Kang、Danqi Chen、&lt;strong>;Jinhyuk Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10750.pdf&quot;>;PropSegmEnt：用于命题级分割和蕴涵识别的大型语料库&lt;/a>; &lt;br />; &lt;em>;Sihao Chen*, &lt;strong>;Senaka Buthpitiya&lt;/strong>;、&lt;strong>;Alex Fabrikant&lt;/strong>;、Dan Roth、&lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //arxiv.org/pdf/2305.02301.pdf&quot;>;逐步蒸馏！使用较少的训练数据和较小的模型规模超越较大的语言模型&lt;/a>; &lt;br />; &lt;em>;Cheng-Yu Hsieh*、&lt;strong>;Chun-Liang Li&lt;/strong>;、&lt;strong>;Chih-Kuan Yeh&lt;/ &lt;strong>;、&lt;strong>;Hootan Nakhost&lt;/strong>;、&lt;strong>;藤井康久&lt;/strong>;、Alex Ratner、Ranjay Krishna、&lt;strong>;李振宇&lt;/strong>;、&lt;strong>;托马斯·普菲斯特&lt;/strong>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05110.pdf&quot;>;具有可控工作记忆的大型语言模型&lt;/a>; &lt;br />; &lt;em>;&lt; b>;李大良&lt;/b>;、&lt;b>;Ankit Singh Rawat&lt;/b>;、&lt;b>;Manzil Zaheer&lt;/b>;、&lt;b>;王鑫&lt;/b>;、&lt;b>;Michal Lukasik&lt;/b>;、 &lt;b>;Andreas Veit&lt;/b>;、&lt;b>;Felix Yu&lt;/b>;、&lt;b>;Sanjiv Kumar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv .org/pdf/2212.10791.pdf&quot;>;OpineSum：基于蕴含的抽象意见总结自我训练&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Annie Louis&lt;/b>;，&lt;b>;Joshua Maynez&lt;/ b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08775.pdf&quot;>;RISE：利用检索技术进行摘要评估&lt;/a>; &lt;br />; &lt; em>;&lt;b>;David Uthus&lt;/b>;、&lt;b>;倪建模&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools -public-publication-data/pdf/b5b9bb4ec1e1d7416f88f8b3a1649d1235d747b8.pdf&quot;>;充满信心地跟随领导者（董事会）：利用项目和响应方差从单个测试集中估计 p 值&lt;/a>; &lt;br />; &lt;i>; Shira Wein *，克里斯托弗·霍曼，&lt;strong>;洛拉·阿罗约&lt;/strong>;，&lt;strong>;克里斯·韦尔蒂&lt;/strong>;&lt;/i>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/ 2306.02516.pdf&quot;>;SamToNe：改进具有同塔负数的双编码器检索模型的对比损失&lt;/a>; &lt;br />; &lt;i>;&lt;strong>;Fedor Moiseev&lt;/strong>;、&lt;strong>;Gustavo Hernandez Abrego&lt;/strong>; 、&lt;strong>;彼得·多恩巴赫&lt;/strong>;、&lt;strong>;伊梅德·齐图尼&lt;/strong>;、&lt;strong>;恩里克·阿方塞卡&lt;/strong>;、&lt;strong>;董哲&lt;/strong>;&lt;/i>; &lt;/p>; &lt;/ div>; &lt;br />; &lt;h2>;论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10266.pdf&quot;>;大海捞针：偶然双语在 PaLM 翻译能力中的作用&lt;/a>; &lt;br />; &lt;em>;Eleftheria Briakou、&lt;strong>;Colin Cherry&lt;/strong>;、&lt;strong>;George Foster&lt;/strong>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09102.pdf&quot;>;提示PaLM进行翻译：评估策略和性能&lt;/a>; &lt;br />; &lt;em >;&lt;b>;大卫·维拉尔&lt;/b>;、&lt;b>;马库斯·弗雷塔格&lt;/b>;、&lt;b>;科林·切里&lt;/b>;、&lt;b>;罗家明&lt;/b>;、&lt;b>;维雷什·拉特纳卡尔&lt;/b>; , &lt;b>;George Foster&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.17525.pdf&quot;>;闭卷长篇的查询细化提示-表格质量检查&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Reinald Kim Amplayo&lt;/b>;、&lt;b>;Kellie Webster&lt;/b>;、&lt;b>;Michael Collins&lt;/b>;、&lt;b>;Dipanjan Das&lt; /b>;, &lt;b>;Shashi Narayan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10381.pdf&quot;>;改编或注释：开放域问答中域适应的挑战和干预&lt;/a>; &lt;br />; &lt;em>;Dheeru Dua*、&lt;strong>;Emma Strubell&lt;/strong>;、Sameer Singh、&lt;strong>;Pat Verga&lt;/strong>;&lt; /em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00193.pdf&quot;>;FRMT：少样本区域感知机器翻译的基准&lt;/a>;（参见&lt; a href=&quot;https://ai.googleblog.com/2023/02/frmt-benchmark-for-few-shot-region.html&quot;>;博客文章&lt;/a>;）&lt;br />; &lt;em>;&lt;b>;帕克·莱利、&lt;b>;蒂莫西·多扎特&lt;/b>;、&lt;b>;Jan A. Botha&lt;/b>;、&lt;b>;泽维尔·加西亚&lt;/b>;、&lt;b>;丹·加勒特&lt;/b>;、&lt; b>;杰森·里萨&lt;/b>;，&lt;b>;奥尔罕·菲拉特&lt;/b>;，&lt;b>;诺亚·康斯坦特&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/2207.00397.pdf&quot;>;带有问答蓝图的条件生成&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Shashi Narayan&lt;/b>;、&lt;b>;Joshua Maynez&lt;/b>;、&lt;b >;雷纳尔德·金·安普莱奥&lt;/b>;、&lt;b>;库兹曼·甘切夫&lt;/b>;、&lt;b>;安妮·路易斯&lt;/b>;、&lt;b>;芳汀·胡特&lt;/b>;、&lt;b>;安德斯·桑德霍尔姆&lt;/b>;、&lt; b>;Dipanjan Das&lt;/b>;、&lt;b>;Mirella Lapata&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12142.pdf&quot;>;参考文献通过基于 Seq2Seq 转换的系统进行解析&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Bernd Bohnet&lt;/b>;、&lt;b>;Chris Alberti&lt;/b>;、&lt;b>;Michael Collins&lt;/b>;&lt;/ em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00106.pdf&quot;>;使用特定于语言的子网进行低资源依存解析的跨语言传输&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni、&lt;strong>;Dan Garrette&lt;/strong>;、Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08054.pdf&quot; >;DAMP：用于面向任务的对话的双重对齐多语言解析器&lt;/a>; &lt;br />; &lt;em>;William Held*、&lt;strong>;Christopher Hidey&lt;/strong>;、&lt;strong>;Fei Liu&lt;/strong>;、&lt;strong>; Eric Zhu&lt;/strong>;、&lt;strong>;Rahul Goel&lt;/strong>;、Diyi Yang、&lt;strong>;Rushin Shah&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv .org/pdf/2210.08726.pdf&quot;>;RARR：研究和修订语言模型所说的内容，使用语言模型&lt;/a>; &lt;br />;高鲁宇*，&lt;strong>;戴竹云&lt;/strong>;，&lt;strong>;Panupong Pasupat &lt;/strong>;、Anthony Chen*、&lt;strong>;Arun Tejasvi Chaganty&lt;/strong>;、&lt;strong>;范一成&lt;/strong>;、&lt;strong>;赵文山&lt;/strong>;、&lt;strong>;倪老&lt;/strong>; >;、&lt;strong>;李洪来&lt;/strong>;、&lt;strong>;胡安大成&lt;/strong>;、&lt;strong>;Kelvin Guu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv .org/pdf/2306.16793.pdf&quot;>;条件生成大型语言模型能力的基准测试&lt;/a>; &lt;br />; &lt;em>;Joshua Maynez、Priyanka Agrawal、&lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.01786.pdf&quot;>;通过多任务微调实现跨语言泛化&lt;/a>; &lt;br />; &lt;em>;Niklas Muennighoff、Thomas Wang、Lintang Sutawika、&lt;strong>;Adam Roberts&lt;/strong>;、Stella Biderman、Teven Le Scao、M. Saiful Bari、Sheng Shen、郑欣勇、Hailey Schoelkopf、Xiangru Tang、Dragomir Radev、Alham Fikri Aji、Khalid Almubarak、Samuel Albanie、 Zaid Alyafeai、Albert Webson、Edward Raff、Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05655.pdf&quot;>;DisentQA：用以下方法解开参数和上下文知识：反事实问答&lt;/a>; &lt;br />; &lt;em>;Ella Neeman、&lt;strong>;Roee Aharoni&lt;/strong>;、Or Honovich、Leshem Choshen、&lt;strong>;Idan Szpektor&lt;/strong>;、Omri Abend&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10933.pdf&quot;>;解析实体选择的间接引用表达式&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mohammad Javad Hosseini &lt;/b>;、&lt;b>;菲利普·拉德林斯基&lt;/b>;、&lt;b>;西尔维娅·帕雷蒂&lt;/b>;、&lt;b>;安妮·路易斯&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://arxiv.org/pdf/2305.11840.pdf&quot;>;SeeGULL：利用生成模型的具有广泛地理文化覆盖范围的刻板印象基准&lt;/a>; &lt;br />; &lt;em>;Akshita Jha*，&lt;strong>;Aida Mostafazadeh Davani &lt;/strong>;、Chandan K Reddy、&lt;strong>;Shachi Dave&lt;/strong>;、&lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;、&lt;strong>;Sunipa Dev&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://arxiv.org/pdf/2210.10040.pdf&quot;>;尾巴摇狗：社会偏见基准的数据集构建偏差&lt;/a>; &lt;br />; &lt;em>;Nikil Selvam，&lt;strong>;Sunipa Dev&lt;/strong>;、Daniel Khashabi、Tushar Khot、Kai-Wei Chang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10562.pdf&quot;>;角色感知模型改善视觉文本渲染&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Rosanne Liu&lt;/b>;、&lt;b>;Dan Garrette&lt;/b>;、&lt;b>;Chitwan Saharia&lt;/b>;、&lt;b>;William陈&lt;/b>;、&lt;b>;亚当·罗伯茨&lt;/b>;、&lt;b>;Sharan Narang&lt;/b>;、&lt;b>;伊琳娜·布洛克&lt;/b>;、&lt;b>;RJ Mical&lt;/b>;、&lt;b>;穆罕默德Norouzi&lt;/b>;，&lt;b>;Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06995.pdf&quot;>;冷启动数据更好的小样本语言模型微调的选择：一种基于提示的不确定性传播方法&lt;/a>; &lt;br />; &lt;em>;Yue Yu，Rongzhi Zhang，Ran Xu，Jieyu Zhu，&lt;strong>;Jiaming Shen&lt;/strong >;，张超&lt;/em>; &lt;/p>; &lt;p>; 涵盖不常见的领域：针对答案评估的差距聚焦问题生成&lt;br />; &lt;em>;&lt;b>;Roni Rabin&lt;/b>;，&lt;b>;Alexandre Djerbetian&lt; /b>;、&lt;b>;罗伊·英格伯格&lt;/b>;、&lt;b>;利丹·哈克蒙&lt;/b>;、&lt;b>;加尔·埃利丹&lt;/b>;、&lt;b>;罗伊特·沙法蒂&lt;/b>;、&lt;b>;阿米尔·格洛伯森&lt; /b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02549.pdf&quot;>;FormNetV2：表单文档信息提取的多模态图对比学习&lt;/a>; &lt; br />; &lt;em>;&lt;b>;李震宇&lt;/b>;、&lt;b>;李春亮&lt;/b>;、&lt;b>;张浩&lt;/b>;、&lt;b>;Timothy Dozat&lt;/b>;、 &lt;b>;文森特·佩罗&lt;/b>;、&lt;b>;苏国龙&lt;/b>;、&lt;b>;张翔&lt;/b>;、&lt;b>;基赫克·索恩&lt;/b>;、&lt;b>;尼古拉·格卢希涅夫&lt;/b>;、 &lt;b>;王仁深&lt;/b>;、&lt;b>;Joshua Ainslie&lt;/b>;、&lt;b>;龙尚邦&lt;/b>;、&lt;b>;秦思阳&lt;/b>;、&lt;b>;藤井泰久&lt;/b>;、 &lt;b>;南华&lt;/b>;，&lt;b>;托马斯·普菲斯特&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00922.pdf&quot;>;生成文本的方言鲁棒评估&lt;/a>; &lt;br />; &lt;em>;Jiao Sun*, &lt;strong>;Thibault Sellam&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Tu Vu*, &lt;strong>;Timothy多扎特&lt;/strong>;、&lt;strong>;丹·加勒特&lt;/strong>;、&lt;strong>;阿迪亚·西丹特&lt;/strong>;、&lt;strong>;雅各布·爱森斯坦&lt;/strong>;、&lt;strong>;塞巴斯蒂安·格尔曼&lt;/strong>;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.03950.pdf&quot;>;性别错误：大型语言模型在理解代词方面的局限性&lt;/a>; &lt;br />; &lt;em>;Tamanna Hossain， &lt;strong>;Sunipa Dev&lt;/strong>;，Sameer Singh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13894.pdf&quot;>;LAMBADA：用于自动推理的反向链接自然语言&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mehran Kazemi&lt;/b>;、&lt;b>;Najoung Kim&lt;/b>;、&lt;b>;Deepti Bhatia&lt;/b>;、&lt;b>;Xin Xu&lt; /b>;, &lt;b>;Deepak Ramachandran&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.19585.pdf&quot;>;LAIT：高效的多段通过层可调交互在变形金刚中进行编码&lt;/a>; &lt;br />; &lt;em>;Jeremiah Milbauer*、&lt;strong>;Annie Louis&lt;/strong>;、&lt;strong>;Mohammad Javad Hosseini&lt;/strong>;、&lt;strong>;Alex Fabrikant&lt; /strong>;、&lt;strong>;唐纳德·梅茨勒&lt;/strong>;、&lt;strong>;塔尔·舒斯特&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05392 .pdf&quot;>;通过代码生成进行模块化视觉问答&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot;>;博客帖子&lt;/a>;) &lt;br />; &lt;em>;Sanjay Subramanian、Medhini Narasimhan、Kushal Khangaonkar、Kevin Yang、&lt;strong>;Arsha Nagrani&lt;/strong>;、&lt;strong>;Cordelia Schmid&lt;/strong>;、&lt;strong>;Andy Zeng &lt;/strong>;，Trevor Darrell，Dan Klein&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10001.pdf&quot;>;理解思想链提示：对重要因素的实证研究&lt;/a>; &lt;br />; &lt;em>;Boshi Wang、Sewon Min、Xiang Deng、&lt;strong>;Jiaming Shen&lt;/strong>;、&lt;strong>;You Wu&lt;/strong>;、Luke Zettlemoyer 和 Huan Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14106.pdf&quot;>;通过自适应提示实现更好的零样本推理&lt;/a>; &lt;br />; &lt;em>;Xingchen Wan*、&lt;strong>;孙若曦&lt;/strong>;、Hanjun Dai、&lt;strong>;Sercan Ö。 Arik&lt;/strong>;、&lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.00186.pdf&quot;>;事实上一致的总结带有文本蕴涵反馈的强化学习&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Paul Roit&lt;/b>;、&lt;b>;Johan Ferret&lt;/b>;、&lt;b>;Lior Shani&lt;/b>;、&lt;b>;罗伊·阿哈罗尼&lt;/b>;、&lt;b>;杰弗里·西德龙&lt;/b>;、&lt;b>;罗伯特·达达什&lt;/b>;、&lt;b>;马蒂厄·盖斯特&lt;/b>;、&lt;b>;塞尔坦·吉尔金&lt;/b>;、&lt;b>;莱昂纳德·胡塞诺&lt;/b>;、&lt;b>;奥加德·凯勒&lt;/b>;、&lt;b>;尼古拉·莫切夫&lt;/b>;、&lt;b>;萨贝拉·拉莫斯&lt;/b>;、&lt;b>;皮奥特·斯坦奇克&lt;/b>;、&lt;b>;尼诺·维埃拉德&lt;/b>;、&lt;b>;奥利维尔·巴赫姆&lt;/b>;、&lt;b>;加尔·埃利丹&lt;/b>;、&lt;b>;阿维纳坦·哈西丁&lt;/b>;、&lt;b>;奥利维尔·皮特昆&lt;/b>;、&lt;b>; Idan Szpektor&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09248.pdf&quot;>;交互式数据科学笔记本中的自然语言到代码生成&lt;/a >; &lt;br />; &lt;em>;&lt;b>;殷鹏程&lt;/b>;、&lt;b>;李文鼎&lt;/b>;、&lt;b>;肖克凡&lt;/b>;、&lt;b>;Abhishek Rao&lt;/b>;、 &lt;b>;温业明&lt;/b>;、&lt;b>;石肯森&lt;/b>;、&lt;b>;约书亚·霍兰&lt;/b>;、&lt;b>;佩奇·贝利&lt;/b>;、&lt;b>;米歇尔·卡塔斯塔&lt;/b>;、 &lt;b>;亨利克·米哈勒夫斯基&lt;/b>;、&lt;b>;亚历山大·波洛佐夫&lt;/b>;、&lt;b>;查尔斯·萨顿&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv .org/pdf/2212.08410.pdf&quot;>;教授小语言模型进行推理&lt;/a>; &lt;br />; &lt;em>;Lucie Charlotte Magister*、&lt;strong>;Jonathan Mallinson&lt;/strong>;、&lt;strong>;Jakub Adamek&lt;/strong >;、&lt;strong>;埃里克·马尔米&lt;/strong>;、&lt;strong>;Aliaksei Severyn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/assets/pdf /papers/NeuPSL.pdf&quot;>;利用领域知识通过神经概率软逻辑引导对话结构归纳&lt;/a>; &lt;br />; &lt;em>;Connor Pryor*、&lt;strong>;Quan Yuan&lt;/strong>;、&lt;strong>;Jeremiah刘&lt;/strong>;、&lt;strong>;Mehran Kazemi&lt;/strong>;、&lt;strong>;Deepak Ramachandran&lt;/strong>;、&lt;strong>;Tania Bedrax-Weiss&lt;/strong>;、Lise Getoor&lt;/em>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://arxiv.org/pdf/2212.10397.pdf&quot;>;大海捞针：MTurk 上高共识工作者分析总结&lt;/a>; &lt;br />; &lt;em>;Lining张，Simon Mille，侯玉芳，&lt;strong>;Daniel Deutsch&lt;/strong>;，&lt;strong>;Elizabeth Clark&lt;/strong>;，刘一鑫，Saad Mahamood，&lt;strong>;Sebastian Gehrmann&lt;/strong>;，Miruna Clinciu，Khyathi Raghavi Chandu和 João Sedoc&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;行业跟踪论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot; https://arxiv.org/pdf/2305.18465.pdf&quot;>;具有差异隐私的Gboard语言模型联邦学习&lt;/a>; &lt;br />; &lt;em>;&lt;b>;徐峥&lt;/b>;，&lt;b>;张彦翔&lt;/b>;、&lt;b>;盖伦·安德鲁&lt;/b>;、&lt;b>;克里斯托弗·乔奎特&lt;/b>;、&lt;b>;彼得·凯鲁兹&lt;/b>;、&lt;b>;布兰登·麦克马汉&lt;/b>;、&lt;b>;杰西·罗森斯托克&lt;/b>;、&lt;b>;张远波&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18373.pdf&quot;>;KAFA：重新思考图片广告通过视觉语言模型的知识增强特征适应进行理解&lt;/a>; &lt;br />; &lt;em>;Zhiwei Jia*, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Arjun Akula&lt;/strong>;, &lt;strong>; Garima Pruthi&lt;/strong>;、Hao Su、&lt;strong>;Sugato Basu&lt;/strong>;、&lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;ACL 研究结果论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10622.pdf&quot;>;多语言摘要与事实一致性评估&lt;/a >; &lt;br />; &lt;em>;&lt;b>;罗伊·阿哈罗尼&lt;/b>;、&lt;b>;沙什·纳拉扬&lt;/b>;、&lt;b>;约书亚·梅内斯&lt;/b>;、&lt;b>;乔纳森·赫齐格&lt;/b>;、&lt;b >;伊丽莎白·克拉克&lt;/b>;，&lt;b>;米雷拉·拉帕塔&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06767.pdf&quot;>;参数-高效微调实现稳健的持续多语言学习&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Kartikeya Badola&lt;/b>;、&lt;b>;Shachi Dave&lt;/b>;、&lt;b>;Partha Talukdar&lt;/b>;&lt; /em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08153.pdf&quot;>;FiDO：融合解码器经过优化，可实现更强的性能和更快的推理&lt;/a>; &lt;br />; &lt;em>;Michiel de Jong*、&lt;strong>;Yury Zemlyanskiy&lt;/strong>;、&lt;strong>;Joshua Ainslie&lt;/strong>;、&lt;strong>;Nicholas FitzGerald&lt;/strong>;、&lt;strong>;Sumit Sanghai&lt;/strong>;、 &lt;strong>;飞沙&lt;/strong>;、&lt;strong>;威廉·科恩&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00609.pdf&quot;>;一种简单而有效的方法来查找代码生成中的偏差&lt;/a>; &lt;br />; &lt;em>;Spyridon Mouselinos、Mateusz Malinowski、&lt;strong>;Henryk Michalewski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://arxiv.org/pdf/2210.09261.pdf&quot;>;具有挑战性的大基准任务以及思想链是否可以解决它们&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mirac Suzgun &lt;/b>;、&lt;b>;内森·斯凯尔斯&lt;/b>;、&lt;b>;纳撒内尔·沙利&lt;/b>;、&lt;b>;塞巴斯蒂安·格尔曼&lt;/b>;、&lt;b>;Yi Tay&lt;/b>;、&lt;b>;Hyung Won钟&lt;/b>;、&lt;b>;Aakanksha Chowdhery&lt;/b>;、&lt;b>;Quoc Le&lt;/b>;、&lt;b>;Ed Chi&lt;/b>;、&lt;b>;Denny Zhou&lt;/b>;、&lt;b>;Jason Wei&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.07730.pdf&quot;>;QueryForm：一个简单的零样本实体查询框架&lt;/a >; &lt;br />; &lt;em>;王子峰*、&lt;strong>;张子钊&lt;/strong>;、&lt;strong>;Jacob Devlin&lt;/strong>;、&lt;strong>;李振宇&lt;/strong>;、&lt;strong>;苏国龙&lt; /strong>;、&lt;strong>;张浩&lt;/strong>;、Jennifer Dy、&lt;strong>;文森特·佩罗&lt;/strong>;、&lt;strong>;托马斯·菲斯特&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2305.10703.pdf&quot;>;ReGen：通过训练数据生成和渐进密集检索进行零样本文本分类&lt;/a>; &lt;br />; &lt;em>;Yue Yu，Yuchen Zhuang，Rongzhi张雨萌，&lt;strong>;沉家明&lt;/strong>;，张超&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09682.pdf&quot;>;多语言序列希伯来语 NLP 的序列模型&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Matan Eyal&lt;/b>;、&lt;b>;Hila Noga&lt;/b>;、&lt;b>;Roee Aharoni&lt;/b>;、&lt; b>;Idan Szpektor&lt;/b>;、&lt;b>;Reut Tsarfaty&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.04009.pdf&quot;>;触发使用软提示和随机游走的语言模型中问答的多跳推理&lt;/a>; &lt;br />; &lt;em>;Kanishka Misra*、&lt;strong>;Cicero Nogueira dos Santos&lt;/strong>;、Siamak Shakeri&lt;/em>; &lt; /p>; &lt;/div>; &lt;br />; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/ program/tutorials/&quot;>;自然语言中的复杂推理&lt;/a>;&lt;br />; &lt;em>;Wenting Zhao、Mor Geva、Bill Yuchen Lin、Michihiro Yasunaga、&lt;strong>;Aman Madaan&lt;/strong>;、Tao Yu&lt;/em >; &lt;/p>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;从语言模型生成文本&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Afra Amini &lt;/b>;、&lt;b>;瑞安·科特雷尔&lt;/b>;、&lt;b>;约翰·休伊特&lt;/b>;、&lt;b>;克拉拉·梅斯特&lt;/b>;、&lt;b>;蒂亚戈·皮门特尔&lt;/b>;&lt;/em>; &lt;/ p>; &lt;/div>; &lt;br />; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp /view/sustainlp2023&quot;>;简单高效的自然语言处理（SustaiNLP）&lt;/a>; &lt;br />;组织者包括：&lt;strong>;&lt;em>;Tal Schuster&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://www.workshopononlineabuse.com/&quot;>;网络虐待和伤害研讨会 (WOAH)&lt;/a>; &lt;br />; 组织者包括：&lt;strong>;&lt;em>;Aida Mostafazadeh Davani&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://doc2dial.github.io/workshop2023/&quot;>;基于文档的对话和对话式问答 (DialDoc)&lt;/a>; &lt;br />; 组织者包括：&lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/5thnlp4concai/home?authuser=0 &quot;>;对话式人工智能 NLP&lt;/a>; &lt;br />; 组织者包括：&lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cawl .wellformedness.com/&quot;>;计算和书面语言 (CAWL)&lt;/a>; &lt;br />; 组织者包括：&lt;em>;&lt;b>;Kyle Gorman&lt;/b>;、&lt;b>;Brian Roark&lt;/b>;、&lt;b >;理查德·斯普罗特&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sigmorphon.github.io/workshops/2023/&quot;>;计算形态学和音系学 (SIGMORPHON)&lt;/a>; &lt;br />; 演讲者包括：&lt;strong>;&lt;em>;凯尔·戈尔曼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/umass.edu/wnu2023 &quot;>;叙事理解研讨会（WNU）&lt;/a>; &lt;br />;组织者包括：&lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--脚注- ->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>;*&lt;/b>; 完成的工作在 Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7093565002706757508/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type= “application/atom+xml”/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-acl-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://ai.googleblog.com/2023/07/google-at-acl-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ACL 2023&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel= “http://schemas.google.com/g/2005#thumbnail” src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>; &lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYN zpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s72-c/Google%20ACL%202023% 20多伦多%20-%20xsmall.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total >;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6926843365781180400&lt;/id>;&lt;已发布>;2023-07-07T11:01:00.001-07:00&lt;/已发布>; &lt;更新>;2023-07-07T11:09:20.724-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category >;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns #&quot; term=&quot;video&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;通过代码生成进行模块化视觉问答&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;已发布作者：加州大学伯克利分校博士生 Sanjay Subramanian 和 Google 感知团队研究科学家 Arsha Nagrani&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7 HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382 -4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s320/hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://visualqa.org/&quot;>;视觉问答&lt;/a>; (VQA) 是一项机器学习任务，需要模型回答有关图像或&lt;a href =&quot;https://covr-dataset.github.io/&quot;>;一组图像&lt;/a>;。传统的 VQA 方法需要大量带标签的训练数据，其中包含数千个与图像相关的人工注释问答对。近年来，大规模预训练的进步促进了 VQA 方法的发展，这些方法在 &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a -single-visual-language-model&quot;>;少于 50 个训练示例&lt;/a>;（少量）和&lt;a href=&quot;https://ai.googleblog.com/2022/07/rewriting-image-captions- for-visual.html&quot;>;没有任何人工注释的 VQA 训练数据&lt;/a>;（零样本）。然而，这些方法与最先进的完全监督 VQA 方法之间仍然存在显着的性能差距，例如 &lt;a href=&quot;https://ai.googleblog.com/2023/05/mammut-simple- Vision-encoder-text.html&quot;>;MaMMUT&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;。特别是，few-shot 方法在空间推理、计数和多跳推理方面遇到了困难。此外，少样本方法通常仅限于回答有关单个图像的问题。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了提高涉及复杂推理的 VQA 示例的准确性，请参见“&lt;a href=&quot;https://arxiv.org/abs/2306.05392&quot;>;通过代码生成实现模块化视觉问答&lt;/a>;”，出现在 &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>; 上，我们推出了 CodeVQA，一个回答视觉问题的框架使用程序综合。具体来说，当给出有关图像或图像集的问题时，CodeVQA 会生成一个具有简单视觉功能的 Python 程序（代码），使其能够处理图像，并执行该程序以确定答案。我们证明，在少样本设置中，CodeVQA 在 &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; 数据集上的性能比之前的工作高约 3%，在&lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; 数据集。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;CodeVQA&lt;/h2>; &lt;p>; CodeVQA 方法使用代码编写大型语言模型（LLM ），例如 &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PALM&lt;/a>;，以生成 &lt;a href= “https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;Python&lt;/a>; 程序（代码）。我们通过制作由这些函数的描述和少于 15 个视觉问题的“上下文”示例以及相关 Python 代码组成的提示来指导法学硕士正确使用视觉函数。为了选择这些示例，我们计算输入问题以及我们已注释程序的所有问题（随机选择的 50 个）的嵌入。然后，我们选择与输入最相似的问题并将其用作上下文示例。给定我们想要回答的提示和问题，法学硕士会生成一个代表该问题的 Python 程序。 &lt;/p>; &lt;p>; 我们使用三个可视化函数实例化 CodeVQA 框架：(1) &lt;code>;query&lt;/code>;、(2) &lt;code>;get_pos&lt;/code>; 和 (3) &lt;code>;find_matching_image&lt; /代码>;。 &lt;/p>; &lt;ul>; &lt;li>;&lt;code>;Query&lt;/code>;，它回答有关单个图像的问题，是使用少镜头&lt;a href=&quot;https://arxiv.org/abs/ 实现的2210.08773&quot;>;即插即用 VQA&lt;/a>; (PnP-VQA) 方法。 PnP-VQA 使用 &lt;a href=&quot;https://arxiv.org/abs/2201.12086&quot;>;BLIP&lt;/a>; 生成字幕 - 图像字幕 &lt;a href=&quot;https://ai.googleblog.com/2017 /08/transformer-novel-neural-network.html&quot;>;transformer&lt;/a>; 在数百万个图像标题对上进行了预训练，并将它们输入到 LLM 中，输出问题的答案。 &lt;/li>;&lt;li>;&lt;code>;Get_pos&lt;/code>; 是一个对象定位器，它将对象的描述作为输入并返回其在图像中的位置，是使用 &lt;a href=&quot;https:// 实现的arxiv.org/abs/1610.02391&quot;>;GradCAM&lt;/a>;。具体来说，描述和图像通过 BLIP 联合文本图像编码器，该编码器预测图像文本匹配分数。 GradCAM 采用该分数相对于图像特征的梯度来查找与文本最相关的区域。 &lt;/li>;&lt;li>;&lt;code>;Find_matching_image&lt;/code>;，用于多图像问题中查找与给定输入短语最匹配的图像，是通过使用 BLIP 文本和图像编码器计算文本嵌入来实现的对于短语和每个图像的图像嵌入。然后文本嵌入与每个图像嵌入的点积表示每个图像与短语的相关性，我们选择最大化这种相关性的图像。 &lt;/li>; &lt;/ul>; &lt;p>; 这三个功能可以使用需要很少注释的模型来实现（例如，从网络收集的文本和图像文本对以及少量的 VQA 示例）。此外，除了这些功能之外，CodeVQA 框架还可以轻松推广到用户可能实现的其他功能（例如，对象检测、图像分割或知识库检索）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_Gs RRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s1024/image2 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;501&quot; data-original-width=&quot;1024&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW 6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;CodeVQA 方法的图示。首先，大型语言模型生成Python程序（代码），它调用代表问题的视觉函数。在此示例中，使用简单的 VQA 方法 (&lt;code>;query&lt;/code>;) 来回答问题的一部分，并使用对象定位器 (&lt;code>;get_pos&lt;/code>;) 来查找问题的位置提到的对象。然后程序通过组合这些函数的输出来生成原始问题的答案。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%; &quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; CodeVQA 框架不仅可以正确生成和执行单图像问题的 Python 程序，还可以正确生成和执行多图像问题的 Python 程序。例如，如果给定两张图像，每张图像显示两只熊猫，人们可能会问的一个问题是：“真的有四只熊猫吗？”在这种情况下，LLM 将有关图像对的计数问题转换为一个程序，其中为每个图像获取对象计数（使用 &lt;em>;query&lt;/em>; 函数）。然后将两个图像的计数相加来计算总计数，然后将其与原始问题中的数字进行比较以得出是或否的答案。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4F rp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s1080/image1.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UPuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGR4V9K4QXfHSm4w LFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们在三个视觉推理数据集上评估 CodeVQA：&lt;a href=&quot;https://cs.stanford.edu/people/ dorarad/gqa/about.html&quot;>;GQA&lt;/a>;（单图像）、&lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>;（多图像）、和 &lt;a href=&quot;https://lil.nlp.cornell.edu/nlvr/&quot;>;NLVR2&lt;/a>;（多图像）。对于 GQA，我们为每种方法提供 12 个上下文示例，对于 COVR 和 NLVR2，我们为每种方法提供 6 个上下文示例。下表显示，CodeVQA 在所有三个数据集上均比基线少样本 VQA 方法有了持续改进。 &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text -align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;方法&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;GQA&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;strong>;COVR&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;NLVR2&lt;/strong>; &lt;/td>; &lt; td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;少镜头 PnP-VQA &lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.56 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.06 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;63.37 &lt;/td>; &lt;td >;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/ td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.03 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;54.11 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;64.04 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt; /td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;GQA、COVR 和 NLVR2 数据集上的结果表明 CodeVQA 持续改进在少样本 PnP-VQA 上。衡量标准是精确匹配精度，即预测答案与真实答案完全匹配的示例的百分比。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;br />; &lt;p>; 我们发现，在 GQA 中，CodeVQA 在空间推理问题上的准确率比基线高出大约 30%，在“与”问题上高出 4%，在“或”问题上高出 3%。类别包括多跳问题，例如“图片中是否有盐瓶或滑板？”，生成的程序如下所示。&lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-左：40 像素；右边距：40px； white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;img = open_image(&quot;Image13.jpg&quot;) salt_shakers_exist = query(img, &quot;有盐瓶吗？&quot;)skateboards_exist = query(img, &quot;有滑板吗？&quot;) if salt_shakers_exist == &quot;yes&quot; 或skateboards_exist == &quot;yes&quot;:answer = &quot;yes&quot; else:answer = &quot;no&quot; &lt;/font>;&lt;/pre>; &lt;br />; &lt;p>;在COVR中，我们发现当输入图像数量较多时，CodeVQA相对于基线的增益较高，如下表所示。这种趋势表明将问题分解为单图像问题是有益的。&lt;/p>; &lt; br />; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”style=“margin-left：auto;右边距：自动；文本对齐：居中;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;图片数量&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;方法&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;1&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;3&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5&lt;br / >; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;少量 PnP-VQA&lt;/em>;&amp;nbsp ; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;91.7 &lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;51.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;48.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;47.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;75.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.2 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; /tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出CodeVQA，一个用于少量视觉问答的框架，依赖于代码生成来执行多步骤视觉推理。未来工作的令人兴奋的方向包括扩展所使用的模块集以及为 VQA 之外的视觉任务创建类似的框架。我们注意到，在考虑是否部署 CodeVQA 等系统时应谨慎，因为像我们的视觉函数中使用的视觉语言模型&lt;a href=&quot;https://arxiv.org/abs/2108.02818&quot;>;具有已被证明表现出社会偏见&lt;/a>;。同时，与整体模型相比，CodeVQA 提供了额外的可解释性（通过 Python 程序）和可控性（通过修改提示或视觉功能），这在生产系统中非常有用。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究是 &lt;a href= &quot;https://bair.berkeley.edu/baircommons.html&quot;>;加州大学伯克利分校人工智能研究实验室&lt;/a>; (BAIR) 和 Google Research，由 Sanjay Subramanian、Medin Narasimhan、Kushal Khangaonkar、Kevin Yang、Arsha 主持Nagrani、Cordelia Schmid、Andy Zeng、Trevor Darrell 和 Dan Klein。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6926843365781180400/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/modular-visual-question-answering-via .html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 6926843365781180400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;self&quot; type= &quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot; rel=&quot;alternate&quot; title=&quot;模块化视觉通过代码生成回答问题&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNRABjVYgqA9xmCanNWTtyYw 4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-561916049750681872&lt;/id>;&lt;发布>;2023-07-06T12：50：00.000- 07:00&lt;/发布>;&lt;更新>;2023-07-06T12:50:04.339-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;多模态学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category >;&lt;title type=&quot;text&quot;>;Pic2Word：将图片映射到文字以进行零样本合成图像检索&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Kuniaki Saito，学生Google 研究院云 AI 团队研究员和 Google 研究院研究科学家 Kihyuk Sohn&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIGz -JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW -hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s1100/Pic2Word.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 图像检索在搜索引擎中起着至关重要的作用。通常，他们的用户依靠图像或文本作为查询来检索所需的目标图像。然而，基于文本的检索有其局限性，因为使用文字准确描述目标图像可能具有挑战性。例如，当搜索时尚商品时，用户可能想要其特定属性（例如徽标的颜色或徽标本身）与他们在网站中找到的不同的商品。然而，在现有搜索引擎中搜索该商品并非易事，因为通过文本精确描述时尚商品可能具有挑战性。为了解决这个问题，&lt;a href=&quot;https://arxiv.org/pdf/1812.07119.pdf&quot;>;组合图像检索&lt;/a>; (CIR) 基于组合图像和文本样本的查询来检索图像它提供了有关如何修改图像以适合预期检索目标的说明。因此，CIR 可以通过图像和文本的结合来精确检索目标图像。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 然而，CIR 方法需要大量标记数据，即 1) 查询图像、2) 描述和 3) 目标图像的三元组。收集此类标记数据的成本很高，并且根据这些数据训练的模型通常是针对特定用例量身定制的，这限制了它们泛化到不同数据集的能力。 &lt;/p>; &lt;p>; 为了解决这些挑战，请参阅“&lt;a href=&quot;https://arxiv.org/abs/2302.03084&quot;>;Pic2Word：将图片映射到单词以进行零样本合成图像检索&lt;/a>;” ，我们提出了一个称为零样本 CIR (ZS-CIR) 的任务。在ZS-CIR中，我们的目标是构建一个执行各种CIR任务的单一CIR模型，例如&lt;a href=&quot;https://www.zheyuanliu.me/CIRR/&quot;>;对象组合，&lt;/a>; &lt; a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;属性编辑&lt;/a>;，或域转换，无需标记的三元组数据。相反，我们建议使用大规模图像标题对和未标记图像来训练检索模型，这些图像比大规模监督 CIR 数据集更容易收集。为了鼓励可重复性并进一步推进这一领域的发展，我们还&lt;a href=&quot;https://github.com/google-research/composed_image_retrieval&quot;>;发布代码&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyuc IiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s1062/image9.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;342&quot; data-original-width=&quot;1062&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD 79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s16000/image9.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;现有组合图像检索模型的描述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyE p3G9- DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCW​​i9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s949/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;476&quot; data-original-width=&quot;949&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9 ALCHCW​​i9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们仅使用图像标题数据训练组合图像检索模型。我们的模型检索与查询图像和文本的组合对齐的图像。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line -height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;方法概述&lt;/h2>; &lt;p>; 我们建议利用 &lt;a href=&quot;https://arxiv 中语言编码器的语言功能.org/abs/2103.00020&quot;>;对比语言图像预训练模型&lt;/a>;（CLIP），它擅长为各种文本概念和属性生成语义上有意义的语言嵌入。为此，我们使用了一个轻量级的模型CLIP中的映射子模块，被设计为将输入图片（例如，猫的照片）从图像嵌入空间映射到文本输入空间中的单词标记（例如，“猫”）。整个网络通过视觉语言对比损失进行优化，以再次确保视觉和文本嵌入空间在给定一对图像及其文本描述的情况下尽可能接近。然后，可以将查询图像视为一个单词。这使得语言编码器能够灵活、无缝地组合查询图像特征和文本描述。我们将我们的方法称为 Pic2Word 并在下图中概述了其训练过程。我们希望映射的标记&lt;em>;s&lt;/em>;以单词标记的形式表示输入图像。然后，我们训练映射网络来重建语言嵌入中的图像嵌入，&lt;em>;p&lt;/em>;。具体来说，我们优化了 CLIP 中提出的在视觉嵌入 &lt;em>;v&lt;/em>; 和文本嵌入 &lt;em>;p&lt;/em>; 之间计算的对比损失。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYH XfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDscoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s611 /image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;第611章0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用以下方法训练映射网络 (&lt;em>;f&lt;sub>;M&lt;/sub>;&lt;/em>;)仅限未标记的图像。我们仅使用冻结的视觉和文本编码器优化映射网络。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;给定经过训练的映射网络，我们可以将图像视为单词标记和对与文字描述灵活组合图文联合查询，如下图所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_i E9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s827/image3 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-width=&quot;827&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5St D1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过训练好的映射网络，我们将图像视为单词标记，并将其与文本描述配对以灵活地组成联合图像-文本查询。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;评估&lt;/h2>; &lt;p >; 我们进行了各种实验来评估 Pic2Word 在各种 CIR 任务上的性能。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;域转换&lt;/h3>; &lt;p>; 我们首先评估所提出的方法在域上的组合能力转换——给定图像和所需的新图像域（例如，雕塑、折纸、卡通、玩具），系统的输出应该是具有相同内容但具有新的所需图像域或风格的图像。如下图所示，我们评估分别以图像和文本形式给出的类别信息和领域描述的能力。我们使用 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; 和 &lt;a href=&quot;https://github.com/hendrycks 评估从真实图像到四个域的转换/imagenet-r&quot;>;ImageNet-R&lt;/a>;。 &lt;/p>; &lt;p>; 为了与不需要监督训练数据的方法进行比较，我们选择了三种方法：(i) &lt;em>;仅图像&lt;/em>;仅通过视觉嵌入执行检索，(ii) &lt;em>;文本only &lt;/em>;仅采用文本嵌入，并且 (iii)&lt;em>;图像 + 文本&lt;/em>;对视觉和文本嵌入进行平均以构成查询。与（iii）的比较显示了使用语言编码器组合图像和文本的重要性。我们还与 &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;Combiner&lt;/a>; 进行了比较，后者在 &lt;a href=&quot;https 上训练 CIR 模型://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRR&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdF FCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s965/image6.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;513&quot; data-original-width=&quot;965&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_ eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s16000/image6.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的目标是将输入查询图像的域转换为用文本描述的域，例如折纸。&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;p>; 如下图所示，我们提出的方法大大优于基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2 I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s754/image2.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;754&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxx YvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;结果（&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;召回&lt;/a>;@10，即，检索到的前 10 个图像中相关实例的百分比。）用于域转换的合成图像检索。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h3>;时尚属性构成&lt;/h3>; &lt;p>; 接下来，我们使用&lt;a href =&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; 数据集。下图说明了给定查询的所需输出。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1R yZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s818/image8.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;309&quot; data-original-width=&quot;818&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B _JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s16000/image8.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;时尚属性的 CIR 概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在下图中，我们与基线进行比较，包括利用三元组训练 CIR 模型的监督基线：(i) &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;CB &lt;/a>; 使用与我们的方法相同的架构，(ii) &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRPLANT&lt;/a>;，&lt;a href=&quot;https:// arxiv.org/pdf/2203.08101.pdf&quot;>;ALTEMIS&lt;/a>;、&lt;a href=&quot;https://arxiv.org/pdf/2007.00145.pdf&quot;>;MAAF&lt;/a>; 使用较小的主干网，例如 ResNet50 。与这些方法的比较将使我们了解零样本方法在此任务上的表现如何。 &lt;/p>; &lt;p>; 虽然 CB 优于我们的方法，但我们的方法比具有较小主干网的监督基线表现更好。这一结果表明，通过利用强大的 CLIP 模型，我们可以训练高效的 CIR 模型，而无需带注释的三元组。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXb FK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s807/image4.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;498&quot; data-original-width=&quot;807&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA 8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot;样式=&quot;text-align: center;&quot;>;结果（&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;召回&lt;/a>;@10，即第一个中相关实例的百分比检索了 10 个图像。）关于 Fashion-IQ 数据集的合成图像检索（越高越好）。浅蓝色条使用三元组训练模型。请注意，我们的方法与这些具有浅（较小）主干的监督基线的性能相当。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br >; &lt;/div>; &lt;h3>;定性结果&lt;/h3>; &lt;p>; 我们在下图中展示了几个示例。与不需要监督训练数据（文本+图像特征平均）的基线方法相比，我们的方法在正确检索目标图像方面做得更好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90 NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s1999/image7.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;1999&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZ zd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot;样式=&quot;text-align: center;&quot;>;不同查询图像和文本描述的定性结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论和未来工作&lt;/h2>; &lt;p>; 在本文中，我们介绍了 Pic2Word，一种用于 ZS-CIR 将图片映射到文字的方法。我们建议将图像转换为单词标记，以仅使用图像标题数据集来实现 CIR 模型。通过各种实验，我们验证了训练模型在各种 CIR 任务上的有效性，表明在图像字幕数据集上进行训练可以构建强大的 CIR 模型。未来一个潜在的研究方向是利用标题数据来训练映射网络，尽管我们在目前的工作中仅使用图像数据。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Kuniaki Saito、Kihyuk Sohn 进行，张翔、李春亮、李震宇、Kate Saenko 和 Tomas Pfister。同时感谢 Zizhao 张和 Sergey Ioffe 提供的宝贵反馈。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/561916049750681872/comments/default&quot; rel= &quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/pic2word-mapping-pictures-to-words-for .html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 561916049750681872&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;self&quot; type= “application/atom+xml”/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/pic2word-mapping-pictures-to-words-for.html&quot; rel=&quot;alternate&quot; title=&quot; Pic2Word：将图片映射到文字以进行零样本合成图像检索” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https: //img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4 BOPEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s72-c/Pic2Word.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/ &quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-667019077470952746&lt;/id>;&lt;发布>;2023-06-29T14:08:00.000-07:00&lt;/发布>;&lt;更新>;2023-06-29T14:08:24.386-07:00&lt;/更新>;&lt;category schema=&quot;http://www. blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;差异隐私&quot;>;&lt;/category >;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;安全与隐私&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;宣布首届机器遗忘挑战&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Fabian Pedregosa 和 Eleni Triantafillou&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-m NrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s900/Unlearning.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 深度学习最近在各种应用中取得了巨大进步，包括&lt;a href=&quot;https://imagen.research.google/&quot;>;真实图像生成&lt;/a>;和&lt;a href= “https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;令人印象深刻的检索系统&lt;/a>;到&lt;a href=&quot;https://blog.google/technology /ai/bard-google-ai-search-updates/&quot;>;可以进行类人对话的语言模型&lt;/a>;。虽然这一进展非常令人兴奋，但深度神经网络模型的广泛使用需要谨慎：在 Google 人工智能&lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;原则&lt;/a>;的指导下，我们寻求通过了解和减轻潜在风险（例如不公平偏见的传播和放大以及保护用户隐私）来负责任地开发人工智能技术。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 完全消除请求删除的数据的影响具有挑战性，因为除了简单地将其从存储的数据库中删除之外，还需要删除该数据对其他工件（例如训练有素的机器学习模型）的影响。此外，最近的研究 [&lt;a href=&quot;https://arxiv.org/abs/1610.05820&quot;>;1&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;2&lt;/ a>;] 表明，在某些情况下，可以使用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Adversarial_machine_learning#Model_extraction 高精度地推断示例是否用于训练机器学习模型&quot;>;成员推理攻击&lt;/a>; (MIA)。这可能会引起隐私问题，因为这意味着即使从数据库中删除个人数据，仍然可以推断该个人的数据是否用于训练模型。 &lt;/p>; &lt;p>; 鉴于上述情况，&lt;em>;机器遗忘&lt;/em>;是机器学习的一个新兴子领域，旨在消除训练样本的特定子集（“遗忘集”）的影响模型。此外，理想的去学习算法将消除某些示例的影响，同时保持其他有益的属性，例如训练集其余部分的准确性以及对保留示例的泛化。生成这种未学习模型的一种直接方法是在调整后的训练集上重新训练模型，该训练集排除了遗忘集中的样本。然而，这并不总是一个可行的选择，因为重新训练深度模型的计算成本可能很高。理想的去学习算法会使用已经训练的模型作为起点，并有效地进行调整以消除所请求数据的影响。 &lt;/p>; &lt;p>; 今天，我们很高兴地宣布，我们已经与众多学术和工业研究人员合作组织了&lt;a href=&quot;https://unlearning-challenge.github.io/&quot; >;首届机器遗忘挑战&lt;/a>;。比赛考虑了一个现实场景，在训练后，必须忘记训练图像的某个子集，以保护相关个人的隐私或权利。比赛将在 &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; 上举办，提交的作品将根据遗忘质量和模型实用性自动评分。我们希望这次竞赛将有助于推进机器去学习的最先进水平，并鼓励开发高效、有效和合乎道德的去学习算法。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;机器取消学习应用&lt;/h2>; &lt;p>; 机器取消学习除了保护用户隐私之外还有其他应用。例如，可以使用忘却来从训练模型中删除不准确或过时的信息（例如，由于标记错误或环境变化）或删除有害的、被操纵的或异常值的数据。 &lt;/p>; &lt;p>; 机器取消学习领域与机器学习的其他领域相关，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;差分隐私&lt;/a>;、&lt; a href=&quot;https://arxiv.org/abs/1802.07569&quot;>;终身学习&lt;/a>;，以及&lt;a href=&quot;https://en.wikipedia.org/wiki/Fairness_(machine_learning)&quot;>;公平&lt;/a>;。差分隐私旨在保证特定的训练样本不会对训练后的模型产生过大的影响；与忘却相比，这是一个更强大的目标，忘却只需要消除指定遗忘集的影响。终身学习研究旨在设计能够持续学习同时保持先前获得的技能的模型。随着消除学习工作的进展，它还可能开辟其他方法来提高模型的公平性，通过纠正不公平的偏见或对属于不同群体（例如人口统计、年龄组等）的成员的不同待遇。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPX IUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s720/image1.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K 21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s16000/image1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot;样式=&quot;text-align: center;&quot;>;&lt;b>;遗忘的剖析。&lt;/b>;遗忘算法将预训练的模型和要遗忘的训练集中的一个或多个样本（“遗忘集”）作为输入。根据模型、遗忘集和保留集，忘却算法会生成更新的模型。理想的忘却算法生成的模型与没有遗忘集的训练模型没有区别。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt; br>; &lt;/div>; &lt;h2>;机器遗忘的挑战&lt;/h2>; &lt;p>; 遗忘的问题是复杂且多方面的，因为它涉及几个相互冲突的目标：忘记所请求的数据、维持模型的效用（例如，保留的准确性）和保留的数据）和效率。因此，现有的去学习算法会做出不同的权衡。例如，完全再训练在不损害模型效用的情况下实现了成功的遗忘，但效率较低，而&lt;a href=&quot;https://arxiv.org/abs/2007.02923&quot;>;向权重添加噪声&lt;/a>;则在公用事业费用。 &lt;/p>; &lt;p>; 此外，迄今为止文献中对遗忘算法的评价高度不一致。虽然一些&lt;a href=&quot;https://arxiv.org/abs/1911.04933&quot;>;作品&lt;/a>;报告了要遗忘的样本的分类准确性，&lt;a href=&quot;https://proceedings.mlr.press/ v119/wu20b.html&quot;>;其他&lt;/a>;报告与完全重新训练的模型的距离，还有其他人使用成员推理攻击的错误率作为忘记质量的指标[&lt;a href=&quot;https://arxiv.org /abs/2302.09880&quot;>;4&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2010.10981&quot;>;5&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs /2005.02205&quot;>;6&lt;/a>;]。 &lt;/p>; &lt;p>; 我们认为，评估指标的不一致和缺乏标准化协议是该领域进展的严重障碍——我们无法对文献中不同的遗忘方法进行直接比较。这让我们对不同方法的相对优点和缺点以及开发改进算法所面临的挑战和机遇抱有短视的看法。为了解决评估不一致的问题并推进机器遗忘领域的最新技术，我们与众多学术和工业研究人员合作组织了首个遗忘挑战。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;宣布首届机器忘却挑战&lt;/h2>; &lt;p>; 我们很高兴地宣布&lt;a href =&quot;https://unlearning-challenge.github.io/&quot;>;首届机器学习挑战&lt;/a>;，该挑战将作为&lt;a href=&quot;https://neurips.cc/Conferences/2023/的一部分举行CompetitionTrack&quot;>;NeurIPS 2023 竞赛赛道。&lt;/a>; 竞赛的目标是双重的。首先，通过统一和标准化遗忘的评估指标，我们希望通过同类比较来识别不同算法的优缺点。其次，通过向所有人开放本次竞赛，我们希望培育出新颖的解决方案，并揭示开放的挑战和机遇。 &lt;/p>; &lt;p>; 比赛将在 &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; 上举办，并于 2023 年 7 月中旬至 2023 年 9 月中旬举行。为了迎接比赛，今天我们宣布推出&lt;a href=&quot;https://github.com/unlearning-challenge/starting-kit&quot;>;入门套件&lt;/a>;。该入门套件为参与者在玩具数据集上构建和测试他们的忘却模型提供了基础。 &lt;/p>; &lt;p>; 比赛考虑了一个现实场景，其中年龄预测器已经接受了人脸图像的训练，训练后，必须忘记训练图像的某个子集，以保护相关个人的隐私或权利。为此，我们将提供合成面部数据集（示例如下所示）作为起始套件的一部分，并且我们还将使用几个真实面部数据集来评估提交的内容。参与者被要求提交代码，该代码将经过训练的预测器、遗忘集和保留集作为输入，并输出已忘记指定遗忘集的预测器的权重。我们将根据遗忘算法的强度和模型效用来评估提交的内容。我们还将强制执行硬截止，拒绝运行速度比重新训练时间的一小部分慢的遗忘算法。本次竞赛的一个有价值的成果将是描述不同不学习算法的权衡。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ 9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s2000/image2.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;2000&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_ 67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;摘自&lt;a href=&quot;https://github.com/microsoft/FaceSynthetics&quot;>;人脸合成&lt;/a>;数据集的图像以及年龄注释。比赛考虑这样的场景：年龄预测器已经在上述人脸图像上进行了训练，并且在训练之后，必须忘记训练图像的某个子集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;p>; 为了评估遗忘，我们将使用受 MIA 启发的工具，例如 &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;LiRA&lt;/a>;。 MIA 最初是在隐私和安全文献中开发的，其目标是推断哪些示例是训练集的一部分。直观上，如果遗忘成功，则未学习的模型不包含被遗忘示例的痕迹，从而导致 MIA 失败：攻击者将&lt;em>;无法&lt;/em>;推断出遗忘集实际上是原始集的一部分训练集。此外，我们还将使用统计测试来量化未学习模型（由特定提交的未学习算法生成）的分布与从头开始重新训练的模型的分布有何不同。对于理想的去学习算法来说，这两者是无法区分的。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 机器遗忘是一个强大的工具，有潜力解决一些开放性问题。机器学习中的问题。随着这一领域研究的继续，我们希望看到更高效、更有效、更负责任的新方法。我们很高兴有机会通过这次比赛来激发人们对该领域的兴趣，我们期待与社区分享我们的见解和发现。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者现已加入 Google DeepMind 。我们代表遗忘竞赛的组织团队撰写这篇博文：Eleni Triantafillou*、Fabian Pedregosa*（*同等贡献）、Meghdad Kurmanji、Kairan Zhao、Gintare Karolina Dziugaite、Peter Triantafillou、Ioannis Mitliagkas、Vincent Dumoulin、Lisheng Sun Hosoya、Peter Kairouz、Julio CS Jacques Junior、Jun Wan、Sergio Escalera 和 Isabelle Guyon。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/667019077470952746 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/announcing-first- machine-unlearning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/667019077470952746&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;self &quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html&quot; rel=&quot;alternate&quot; title=&quot;宣布第一个机器遗忘挑战” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhleg tEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s72-c/Unlearning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-9161751123508054082&lt;/id>;&lt;发布>;2023-06-29T12:10:00.004- 07:00&lt;/发布>;&lt;更新>;2023-07-18T14:55:41.583-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=计算机视觉&quot;>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“机器学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”。 blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;用于条件文本到图像生成的设备上扩散插件&lt;/stitle>;&lt;内容类型=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Core ML 软件工程师 Yang Zhao 和 Tingbo Hou&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b /R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWS66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEossTiPIZKYPhZsnEe2O xNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s800/MediaPipeDiffusion-hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 近年来，&lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;扩散模型&lt;/a>;在文本到图像生成方面取得了巨大成功，实现了高图像质量，提高了推理性能，并扩展了我们的创作灵感。然而，有效控制生成仍然具有挑战性，特别是在难以用文本描述的条件下。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 今天，我们发布了 &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; 扩散插件，该插件使可控的文本到图像生成能够在设备上运行。扩展我们关于 GPU 推理的&lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;之前的工作&lt;/a>; -设备大型生成模型，我们引入了新的低成本解决方案，用于可控文本到图像的生成，可以插入现有的扩散模型及其低阶适应（&lt;a href=&quot;https://arxiv.org/abs /2106.09685&quot;>;LoRA&lt;/a>;) 变体。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759us Jt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s1080 /image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;780&quot; data-original-width=&quot;1080&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x 6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s16000/image6.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过在设备上运行的控制插件生成文本到图像。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;br />; &lt;h2>;背景&lt;/h2>; &lt;p>; 使用扩散模型，图像生成被建模为迭代去噪过程。从噪声图像开始，在每一步中，扩散模型逐渐对图像进行去噪，以揭示目标概念的图像。研究表明，通过文本提示利用语言理解可以极大地改善图像生成。对于文本到图像的生成，文本嵌入通过交叉注意层连接到模型。然而，有些信息很难通过文本提示来描述，例如物体的位置和姿态。为了解决这个问题，研究人员在扩散中添加了额外的模型，以从条件图像中注入控制信息。 &lt;/p>; &lt;p>; 受控文本到图像生成的常见方法包括&lt;a href=&quot;https://arxiv.org/abs/2211.12572&quot;>;即插即用&lt;/a>;、&lt;a href= “https://github.com/lllyasviel/ControlNet&quot;>;ControlNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2302.08453&quot;>;T2I 适配器&lt;/a>;。即插即用应用了广泛使用的去噪扩散隐式模型 (&lt;a href=&quot;https://arxiv.org/abs/2010.02502&quot;>;DDIM&lt;/a>;) 反演方法，该方法从输入图像开始反转生成过程导出初始噪声输入，然后使用扩散模型的副本（稳定扩散 1.5 的 860M 参数）对输入图像中的条件进行编码。即插即用从复制的扩散中提取具有&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;自我注意&lt;/a>;的空间特征，并将它们注入到文本到图像的扩散中。 ControlNet 创建扩散模型编码器的可训练副本，该副本通过卷积层与零初始化参数连接，以对传送到解码器层的调节信息进行​​编码。然而，其结果是尺寸很大，是扩散模型的一半（Stable Diffusion 1.5 的参数为 430M）。 T2I Adapter是一个较小的网络（77M参数），在可控生成方面实现了类似的效果。 T2I Adapter 仅将条件图像作为输入，其输出在所有扩散迭代中共享。然而，适配器模型并不是为便携式设备设计的。 &lt;/p>; &lt;br />; &lt;h2>;MediaPipe 扩散插件&lt;/h2>; &lt;p>; 为了使条件生成高效、可定制且可扩展，我们将 MediaPipe 扩散插件设计为一个单独的网络，即： &lt;/p>; &lt;ul>; &lt;li>;&lt;i>;可插拔&lt;/i>;：它可以轻松连接到预先训练的基础模型。 &lt;/li>;&lt;li>;&lt;i>;从头开始训练&lt;/i>;：它不使用基础模型中预先训练的权重。 &lt;/li>;&lt;li>;&lt;i>;便携式&lt;/i>;：它在移动设备上的基本模型之外运行，与基本模型推理相比，成本可以忽略不计。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;方法&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;参数大小&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;可插拔&lt;/strong>; &lt;/td>; &lt;td>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;从头开始&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td style=&quot;text-align: center;&quot;>;&lt;strong>;便携式&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;即插即用&lt;/td>; &lt;td>; &amp;nbsp; &amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;860M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align : center;&quot;>;430M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp ;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;T2I 适配器&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;77M &lt;/td>; &lt; td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-对齐：中心;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt; td>;MediaPipe 插件 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt; td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height : 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;即插即用、ControlNet、T2I 适配器和 MediaPipe 的比较扩散插件。&lt;br />;* 数量根据扩散模型的具体情况而有所不同。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MediaPipe 扩散插件是一个便携式设备上模型用于文本到图像的生成。它从条件图像中提取多尺度特征，这些特征被添加到相应级别的扩散模型的编码器中。当连接到文本到图像的扩散模型时，插件模型可以提供额外的调节信号到图像生成。我们将插件网络设计为仅具有 6M 参数的轻量级模型。它使用来自 &lt;a href=&quot;https://ai.googleblog.com/2018/04 的深度卷积和反向瓶颈/mobilenetv2-next- Generation-of-on.html&quot;>;MobileNetv2&lt;/a>; 用于在移动设备上进行快速推理。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot; tr-caption-container&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL- GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOK wzbkoOk0lxTYmxZLg4/s1724/image1.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;628&quot; data-original-width=&quot;1724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XV VWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;MediaPipe 扩散模型插件概述。该插件是一个单独的网络，其输出可以插入到预先训练的文本到图像生成模型中。插件提取的特征将应用于关联的网络扩散模型的下采样层（蓝色）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;与ControlNet不同，我们在所有扩散迭代中注入相同的控制特征。也就是说，我们只运行一次生成一张图像的插件，这节省了计算量。下面我们说明了扩散过程的一些中间结果。该控​​制在每个扩散步骤中都有效，并且即使在早期步骤也可以实现受控生成。更多迭代可以改善图像与文本提示的对齐情况并生成更多细节。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z- irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUniBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s1280/image5 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1280&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemE jJ9WZ4fPbvaUniBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用 MediaPipe 扩散插件的生成过程图示。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2 >;示例&lt;/h2>; &lt;p>; 在这项工作中，我们使用 MediaPipe 开发了用于基于扩散的文本到图像生成模型的插件 &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision /face_landmarker&quot;>;面部地标&lt;/a>;、MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;整体地标&lt;/a>;、&lt;a href=&quot;https: //en.wikipedia.org/wiki/Depth_map&quot;>;深度图&lt;/a>;和&lt;a href=&quot;https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html&quot;>;Canny边缘&lt;/a>;。对于每个任务，我们从 &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;网络规模图像文本数据集&lt;/a>;中选择大约 100K 图像，并使用相应的 MediaPipe 解决方案计算控制信号。我们使用 &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; 的精炼字幕来训练插件。 &lt;/p>; &lt;br />; &lt;h3>;面部地标&lt;/h3>; &lt;p>; MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;面部地标&lt;/ a>; 任务计算人脸的 478 个特征点（注意）。我们使用 MediaPipe 中的&lt;a href=&quot;https://github.com/google/mediapipe/blob/26a7ca5c64cd885978677931a7218d33cd7d1dec/mediapipe/python/solutions/drawing_utils.py&quot;>;绘图工具&lt;/a>;来渲染脸部，包括脸部轮廓、嘴巴、眼睛、眉毛和虹膜，具有不同的颜色。下表显示了通过调整面部网格和提示而随机生成的样本。作为比较，ControlNet 和 Plugin 都可以在给定条件下控制文本到图像的生成。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG- IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s1346/image7 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1018&quot; data-original-width=&quot;1346&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF1 7_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用于文本到图像生成的人脸地标插件，与 ControlNet 相比。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; br />; &lt;h3>;整体地标&lt;/h3>; &lt;p>; MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;整体地标&lt;/a>;任务包括身体姿势、手部和面部网格。下面，我们通过调节整体特征来生成各种风格化图像。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ 788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s1351/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;970&quot; data-original-width=&quot;1351&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h 8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;用于生成文本到图像的整体地标插件。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;深度&lt; /h3>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA- 7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s1344/image2.png&quot; style=&quot;margin-left:汽车； margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D 6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;用于生成文本到图像的深度插件。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Canny Edge&lt;/h3>; &lt;tablealign=&quot;中心&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcM Vi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh- 9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s1345/image3.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;955&quot; data-original-width=&quot;1345&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpST eRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;精明-edge 用于生成文本到图像的插件。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;评估&lt;/h2>; &lt;p>; 我们对&lt; em>;人脸地标&lt;/em>;插件来演示模型的性能。评估数据集包含 5K 人类图像。我们比较了通过广泛使用的指标来衡量的生成质量，&lt;a href=&quot;https://en.wikipedia.org /wiki/Fr%C3%A9chet_inception_distance&quot;>;Fréchet 起始距离&lt;/a>; (FID) 和 &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>; 分数。基本模型是预先训练的文本到图像扩散模型。我们在这里使用 &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>; v1.5。 &lt;/p>; &lt;p>; 如下表所示，就 FID 和 CLIP 分数而言，ControlNet 和 MediaPipe 扩散插件产生的样本质量比基本模型好得多。与需要在每个扩散步骤运行的 ControlNet 不同，MediaPipe 插件仅针对生成的每个图像运行一次。我们在服务器机器（配备 Nvidia V100 GPU）和手机（Galaxy S23）上测量了这三个模型的性能。在服务器上，我们使用 50 个扩散步骤运行所有三个模型，在移动设备上，我们使用 &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/image_generator&quot;>;MediaPipe 运行 20 个扩散步骤图像生成应用程序&lt;/a>;。与ControlNet相比，MediaPipe插件在保持样本质量的同时，在推理效率上表现出明显的优势。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;型号&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style= &quot;text-align: center;&quot;>;&lt;strong>;FID↓&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot; text-align: center;&quot;>;&lt;strong>;CLIP↑&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text -align: center;&quot;>;&lt;strong>;推理时间（秒）&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Nvidia V100&lt;/强>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;/tr>; &lt; tr>; &lt;td>;基础 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;10.32 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.26 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 &lt; /td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.5 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + ControlNet &lt;/td >; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.51 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot; text-align: center;&quot;>;0.31 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.4 (+48%) &lt;/td>; &lt; td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;18.2 (+58.3%) &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;基础 + MediaPipe 插件 &lt; /td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td 样式=&quot;text-align: center;&quot;>;0.30 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 (+0.2%) &lt;/td >; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.8 (+2.6%) &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;左边距：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;FID、CLIP、推理时间的定量比较。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们在从中端到高端的各种移动设备上测试了插件的性能。下表列出了一些代表性设备上的结果，涵盖 Android 和 iOS。&lt;/p>; &lt;p>; &lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left:汽车; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;设备&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan =&quot;7&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Android&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot; text-align: center;&quot;>;&lt;strong>;iOS&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center ;&quot;>;&lt;strong>;Pixel 4&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 6&lt;/strong >; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 7&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style= &quot;text-align: center;&quot;>;&lt;strong>;iPhone 12 Pro&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt; strong>;iPhone 13 Pro&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;strong>;时间&lt;/strong>;（毫秒）&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;128 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;68 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text -align: center;&quot;>;48 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;73 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;63 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;插件在不同移动设备上的推理时间（毫秒）&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; 在这项工作中，我们提出了 MediaPipe，一个用于条件文本到图像生成的便携式插件。它注入从条件图像中提取的特征到扩散模型，从而控制图像生成。便携式插件可以连接到在服务器或设备上运行的预先训练的扩散模型。通过完全在设备上运行文本到图像生成和插件，我们使生成式人工智能的应用更加灵活。&lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢所有为这项工作做出贡献的团队成员：Raman Sarokin 和 Juhyun Lee 提供 GPU 推理解决方案；Khanh LeViet、Chuo-Ling Chang、Andrei Kulik 和 Matthias Grundmann 提供领导。特别感谢 Jiuqiang Tang&lt;/em>;&lt;i>;、Joe Zou 和 Lu wang&lt;/i>;&lt;em>;谁发明了这项技术以及在设备上运行的所有演示。&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/9161751123508054082/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/on-device-diffusion-plugins-for .html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 9161751123508054082&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;self&quot; type= “application/atom+xml”/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/on-device-diffusion-plugins-for.html&quot; rel=&quot;alternate&quot; title=&quot;On-用于条件文本到图像生成的设备扩散插件” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt; /uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https:// img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWS66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEossTiPIZKYPh ZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s72-c/MediaPipeDiffusion-hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>; &lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6036162561497757163&lt;/id>;&lt;已发布>; 2023-06-27T14:19:00.000-07:00&lt;/发布>;&lt;更新>;2023-06-27T14:19:04.029-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger. com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category方案=“http://www.blogger.com/atom/ns#”术语=“多模式学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;通过前缀条件统一图像标题和图像分类数据集&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-作者&quot;>;由 Cloud AI 团队学生研究员 Kuniaki Saito 和感知团队研究科学家 Kihyuk Sohn 发布&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD 21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9 -Bc/s320/Prefix%20conditioning%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 在网络规模图像字幕数据集上预训练视觉语言 (VL) 模型最近已成为一种强大的替代方案与传统的图像分类数据预训练相比。图像字幕数据集被认为更“开放域”，因为它们包含更广泛的场景类型和词汇，这导致模型在以下方面具有强大的性能：少样本和零样本识别任务&lt;/a>;。然而，具有细粒度类描述的图像可能很少见，并且由于图像标题数据集不经过手动管理，类分布可能不平衡。相比之下，大规模分类数据集，例如 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>;，通常是精心策划的，因此可以提供具有平衡性的细粒度类别。标签分布。虽然听起来很有希望，但直接结合标题和分类数据集进行预训练通常是不成功的，因为它可能会导致有偏差的表示，而这些表示不能很好地推广到各种下游任务。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2206.01125&quot;>;前缀条件统一语言和标签监管&lt;/a>;” ”，在 &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>; 上提出，我们演示了一种预训练策略，该策略使用分类和描述数据集来提供互补的优势。首先，我们表明，天真地统一数据集会导致下游零样本识别任务的性能次优，因为模型受到数据集偏差的影响：每个数据集中图像域和词汇的覆盖范围不同。我们在训练过程中通过&lt;em>;前缀调节&lt;/em>;解决了这个问题，这是一种新颖的简单而有效的方法，它使用前缀标记来消除数据集偏差与视觉概念。这种方法允许语言编码器从两个数据集学习，同时还为每个数据集定制特征提取。前缀条件是一种通用方法，可以轻松集成到现有的 VL 预训练目标中，例如&lt;a href=&quot;https://openai.com/research/clip&quot;>;对比语言-图像预训练&lt;/a>; (CLIP) 或&lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;统一对比学习&lt;/a>; (UniCL)。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;高级思想&lt;/h2>; &lt;p>;我们注意到分类数据集往往在以下方面存在偏差：至少有两种方式：（1）图像大多包含来自受限领域的单个对象，（2）词汇量有限并且缺乏零样本学习所需的语言灵活性。例如，针对 ImageNet 优化的“狗的照片”的类嵌入通常会导致从 ImageNet 数据集拉取的图像中心有一张狗的照片，这不能很好地推广到包含多只狗的图像的其他数据集在不同的空间位置或狗与其他主体。 &lt;/p>; &lt;p>; By contrast, caption datasets contain a wider variety of scene types and vocabularies. As shown below, if a model simply learns from two datasets, the language embedding can entangle the bias from the image classification and caption dataset, which can decrease the generalization in zero-shot classification. If we can disentangle the bias from two datasets, we can use language embeddings that are tailored for the caption dataset to improve generalization. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/s1150/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1150&quot; data-original-width=&quot;856&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/w476-h640/image1.png&quot; width=&quot;476&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Top&lt;/strong>;: Language embedding entangling the bias from image classification and caption dataset. &lt;strong>;Bottom&lt;/strong>;: Language embeddings disentangles the bias from two datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prefix conditioning&lt;/h2>; &lt;p>; Prefix conditioning is partially inspired by &lt;a href=&quot;https://ai.googleblog.com/2022/02/guiding-frozen-language-models-with.html&quot;>;prompt tuning&lt;/a>;, which prepends learnable tokens to the input token sequences to instruct a pre-trained model backbone to learn task-specific knowledge that can be used to solve downstream tasks. The prefix conditioning approach differs from prompt tuning in two ways: (1) it is designed to unify image-caption and classification datasets by disentangling the dataset bias, and (2) it is applied to VL pre-training while the standard prompt tuning is used to fine-tune models. Prefix conditioning is an explicit way to specifically steer the behavior of model backbones based on the type of datasets provided by users. This is especially helpful in production when the number of different types of datasets is known ahead of time. &lt;/p>; &lt;p>; During training, prefix conditioning learns a text token (prefix token) for each dataset type, which absorbs the bias of the dataset and allows the remaining text tokens to focus on learning visual concepts. Specifically, it prepends prefix tokens for each dataset type to the input tokens that inform the language and visual encoder of the input data type (eg, classification vs. caption). Prefix tokens are trained to learn the dataset-type-specific bias, which enables us to disentangle that bias in language representations and utilize the embedding learned on the image-caption dataset during test time, even without an input caption. &lt;/p>; &lt;p>; We utilize prefix conditioning for CLIP using a language and visual encoder. During test time, we employ the prefix used for the image-caption dataset since the dataset is supposed to cover broader scene types and vocabulary words, leading to better performance in zero-shot recognition. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s1038/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;1038&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the Prefix Conditioning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; We apply prefix conditioning to two types of contrastive loss, &lt;a href=&quot;http://proceedings.mlr.press/v139/radford21a&quot;>;CLIP&lt;/a>; and &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf&quot;>;UniCL,&lt;/a>; and evaluate their performance on zero-shot recognition tasks compared to models trained with &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet21K&lt;/a>; (IN21K) and &lt;a href=&quot;https://github.com/google-research-datasets/conceptual-12m&quot;>;Conceptual 12M&lt;/a>; (CC12M). CLIP and UniCL models trained with two datasets using prefix conditioning show large improvements in zero-shot classification accuracy. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s440/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;440&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Zero-shot classification accuracy of models trained with only IN21K or CC12M compared to CLIP and UniCL models trained with both two datasets using prefix conditioning (“Ours”). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on test-time prefix&lt;/h3>; &lt;p>; The table below describes the performance change by the prefix used during test time. We demonstrate that by using the same prefix used for the classification dataset (“Prompt”), the performance on the classification dataset (&lt;a href=&quot;https://www.image-net.org/&quot;>;IN-1K&lt;/a>;) improves. When using the same prefix used for the image-caption dataset (“Caption”), the performance on other datasets (Zero-shot AVG) improves. This analysis illustrates that if the prefix is tailored for the image-caption dataset, it achieves better generalization of scene types and vocabulary words. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/w640-h396/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis of the prefix used for test-time. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on robustness to image distribution shift &lt;/h3>; &lt;p>; We study the shift in image distribution using ImageNet variants. We see that the “Caption” prefix performs better than “Prompt” in &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>; (IN-R) and &lt;a href=&quot;https://github.com/HaohanWang/ImageNet-Sketch&quot;>;ImageNet-Sketch&lt;/a>; (IN-S), but underperforms in &lt;a href=&quot;https://github.com/modestyachts/ImageNetV2&quot;>;ImageNet-V2&lt;/a>; (IN-V2). This indicates that the “Caption” prefix achieves generalization on domains far from the classification dataset. Therefore, the optimal prefix probably differs by how far the test domain is from the classification dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis on the robustness to image-level distribution shift. IN: ImageNet, IN-V2: ImageNet-V2, IN-R: Art, Cartoon style ImageNet, IN-S: ImageNet Sketch. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We introduce prefix conditioning, a technique for unifying image caption and classification datasets for better zero-shot classification. We show that this approach leads to better zero-shot classification accuracy and that the prefix can control the bias in the language embedding. One limitation is that the prefix learned on the caption dataset is not necessarily optimal for the zero-shot classification. Identifying the optimal prefix for each test dataset is an interesting direction for future work. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Thanks to Zizhao Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6036162561497757163/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html&quot; rel=&quot;alternate&quot; title=&quot;Unifying image-caption and image-classification datasets with prefix conditioning&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s72-c/Prefix%20conditioning%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8471212041038116532&lt;/id>;&lt;published>;2023-06-23T12:24:00.001-07:00&lt;/published>;&lt;updated>;2023-06-23T12:34:21.337-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Preference learning with automated feedback for cache eviction&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ramki Gummadi, Software Engineer, Google and Kevin Chen, Software Engineer, YouTube&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeCdR0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s320/Halp%20hero%20gif.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Caching is a ubiquitous idea in computer science that significantly improves the performance of storage and retrieval systems by storing a subset of popular items closer to the client based on request patterns. An important algorithmic piece of cache management is the decision policy used for dynamically updating the set of items being stored, which has been extensively optimized over several decades, resulting in several &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;>;efficient and robust heuristics&lt;/a>;. While applying machine learning to cache policies has shown promising results in recent years (eg, &lt;a href=&quot;https://www.usenix.org/conference/nsdi20/presentation/song&quot;>;LRB&lt;/a>;, &lt;a href=&quot;https://www.usenix.org/system/files/conference/nsdi18/nsdi18-beckmann.pdf&quot;>;LHD&lt;/a>;, &lt;a href=&quot;https://www.pdl.cmu.edu/PDL-FTP/Storage/MLSys2021-zhou.pdf&quot;>;storage applications&lt;/a>;), it remains a challenge to outperform robust heuristics in a way that can generalize reliably beyond benchmarks to production settings, while maintaining competitive compute and memory overheads. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c9ebaf640152028a6cdedb684577a7c9e52b6f10.pdf&quot;>;HALP: Heuristic Aided Learned Preference Eviction Policy for YouTube Content Delivery Network&lt;/a>;”, presented at &lt;a href=&quot;https://www.usenix.org/conference/nsdi23&quot;>;NSDI 2023&lt;/a>;, we introduce a scalable state-of-the-art cache eviction framework that is based on learned rewards and uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;preference learning&lt;/a>; with automated feedback. The Heuristic Aided Learned Preference (HALP) framework is a meta-algorithm that uses randomization to merge a lightweight heuristic baseline eviction rule with a learned reward model. The reward model is a lightweight neural network that is continuously trained with ongoing automated feedback on preference comparisons designed to mimic the &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0020019006003449&quot;>;offline oracle&lt;/a>;. We discuss how HALP has improved infrastructure efficiency and user video playback latency for YouTube&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Content_delivery_network&quot;>;content delivery network&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learned preferences for cache eviction decisions&lt;/h2>; &lt;p>; The HALP framework computes cache eviction decisions based on two components: (1) a neural reward model trained with automated feedback via preference learning, and (2) a meta-algorithm that combines a learned reward model with a fast heuristic. As the cache observes incoming requests, HALP continuously trains a small neural network that predicts a scalar reward for each item by formulating this as a preference learning method via &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_to_rank#Pairwise_approach&quot;>;pairwise&lt;/a>; preference feedback. This aspect of HALP is similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback&quot;>;reinforcement learning from human feedback&lt;/a>; (RLHF) systems, but with two important distinctions: &lt;/p>; &lt;ul>; &lt;li>;Feedback is &lt;em>;automated&lt;/em>; and leverages well-known results about the structure of offline &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0020019006003449&quot;>;optimal&lt;/a>; cache eviction policies. &lt;/li>;&lt;li>;The model is &lt;em>;learned continuously&lt;/em>; using a transient buffer of training examples constructed from the automated feedback process. &lt;/li>; &lt;/ul>; &lt;p>; The eviction decisions rely on a filtering mechanism with two steps. First, a small subset of candidates is selected using a heuristic that is efficient, but suboptimal in terms of performance. Then, a re-ranking step optimizes from within the baseline candidates via the sparing use of a neural network scoring function to “boost” the quality of the final decision. &lt;/p>; &lt;p>; As a production ready cache policy implementation, HALP not only makes eviction decisions, but also subsumes the end-to-end process of sampling pairwise preference queries used to efficiently construct relevant feedback and update the model to power eviction decisions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A neural reward model&lt;/h2>; &lt;p>; HALP uses a light-weight two-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) as its reward model to selectively score individual items in the cache. The features are constructed and managed as a metadata-only “ghost cache” (similar to classical policies like &lt;a href=&quot;https://en.wikipedia.org/wiki/Adaptive_replacement_cache&quot;>;ARC&lt;/a>;). After any given lookup request, in addition to regular cache operations, HALP conducts the book-keeping (eg, tracking and updating feature metadata in a capacity-constrained key-value store) needed to update the dynamic internal representation. This includes: (1) externally tagged features provided by the user as input, along with a cache lookup request, and (2) internally constructed dynamic features (eg, time since last access, average time between accesses) constructed from lookup times observed on each item. &lt;/p>; &lt;p>; HALP learns its reward model fully online starting from a random weight initialization. This might seem like a bad idea, especially if the decisions are made exclusively for optimizing the reward model. However, the eviction decisions rely on both the learned reward model and a suboptimal but simple and robust heuristic like &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU&quot;>;LRU&lt;/a>;. This allows for optimal performance when the reward model has fully generalized, while remaining robust to a temporarily uninformative reward model that is yet to generalize, or in the process of catching up to a changing environment. &lt;/p>; &lt;p>; Another advantage of online training is specialization. Each cache server runs in a potentially different environment (eg, geographic location), which influences local network conditions and what content is locally popular, among other things. Online training automatically captures this information while reducing the burden of generalization, as opposed to a single offline training solution.&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scoring samples from a randomized priority queue &lt;/h2>; &lt;p>; It can be impractical to optimize for the quality of eviction decisions with an exclusively learned objective for two reasons. &lt;/p>; &lt;ol>; &lt;li>;Compute efficiency constraints: Inference with a learned network can be significantly more expensive than the computations performed in practical cache policies operating at scale. This limits not only the expressivity of the network and features, but also how often these are invoked during each eviction decision. &lt;/li>;&lt;li>;Robustness for generalizing out-of-distribution: HALP is deployed in a setup that involves continual learning, where a quickly changing workload might generate request patterns that might be temporarily out-of-distribution with respect to previously seen data. &lt;/li>; &lt;/ol>; &lt;p>; To address these issues, HALP first applies an inexpensive heuristic scoring rule that corresponds to an eviction priority to identify a small candidate sample. This process is based on efficient random sampling that &lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;approximates&lt;/a>; exact &lt;a href=&quot;https://en.wikipedia.org/wiki/Priority_queue&quot;>;priority queues&lt;/a>;. The priority function for generating candidate samples is intended to be quick to compute using existing manually-tuned algorithms, eg, LRU. However, this is configurable to approximate other cache replacement heuristics by editing a simple cost function. Unlike &lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;prior work&lt;/a>;, where the randomization was used to tradeoff approximation for efficiency, HALP also &lt;em>;relies&lt;/em>; on the inherent randomization in the sampled candidates across time steps for providing the necessary &lt;a href=&quot;https://lilianweng.github.io/posts/2020-06-07-exploration-drl/&quot;>;exploratory&lt;/a>; diversity in the sampled candidates for both training and inference. &lt;/p>; &lt;p>; The final evicted item is chosen from among the supplied candidates, equivalent to the &lt;a href=&quot;https://openai.com/research/measuring-goodharts-law&quot;>;best-of-n&lt;/a>; reranked sample, corresponding to maximizing the predicted&lt;em>; &lt;/em>;preference score according to the neural reward model. The same pool of candidates used for eviction decisions is also used to construct the pairwise preference queries for automated feedback, which helps minimize the training and inference skew between samples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEEvfAA7aieN2cZjyMIm7_MVPockLfhOAvIp7HEiF60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s1470/Halp.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;523&quot; data-original-width=&quot;1470&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEEvfAA7aieN2cZjyMIm7_MVPockLfhOAvIp7HEiF60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s16000/Halp.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the two-stage process invoked for each eviction decision.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Online preference learning with automated feedback&lt;/h2>; &lt;p>; The reward model is learned using online feedback, which is based on automatically assigned preference labels that indicate, wherever feasible, the ranked preference ordering for the time taken to receive future re-accesses, starting from a given snapshot in time among each queried sample of items. This is similar to the oracle optimal policy, which, at any given time, evicts an item with the farthest future access from all the items in the cache. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3JlTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbVHH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s1200/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3JlTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbVHH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Generation of the automated feedback for learning the reward model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To make this feedback process informative, HALP constructs pairwise preference queries that are most likely to be relevant for eviction decisions. In sync with the usual cache operations, HALP issues a small number of pairwise preference queries while making each eviction decision, and appends them to a set of &lt;em>;pending comparisons. &lt;/em>;The labels for these pending comparisons can only be resolved at a random future time. To operate online, HALP also performs some additional book-keeping after each lookup request to process any pending comparisons that can be labeled incrementally after the current request. HALP indexes the pending comparison buffer with each element involved in the comparison, and recycles the memory consumed by stale comparisons (neither of which may ever get a re-access) to ensure that the memory overhead associated with feedback generation remains bounded over time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS89UV1sxOsECA8H4PvInlUTLeB7X5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/s1468/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1032&quot; data-original-width=&quot;1468&quot; height=&quot;450&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS89UV1sxOsECA8H4PvInlUTLeB7X5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/w640-h450/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of all main components in HALP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results: Impact on the YouTube CDN&lt;/h2>; &lt;p>; Through empirical analysis, we show that HALP compares favorably to state-of-the-art cache policies on public benchmark traces in terms of cache miss rates. However, while public benchmarks are a useful tool, they are rarely sufficient to capture all the usage patterns across the world over time, not to mention the diverse hardware configurations that we have already deployed. &lt;/p>; &lt;p>; Until recently, YouTube servers used an optimized LRU-variant for memory cache eviction. HALP increases YouTube&#39;s memory egress/ingress — the ratio of the total bandwidth egress served by the CDN to that consumed for retrieval (ingress) due to cache misses — by roughly 12% and memory hit rate by 6%. This reduces latency for users, since memory reads are faster than disk reads, and also improves egressing capacity for disk-bounded machines by shielding the disks from traffic. &lt;/p>; &lt;p>; The figure below shows a visually compelling reduction in the &lt;a href=&quot;https://www.usenix.org/legacy/publications/library/proceedings/usits97/full_papers/cao/cao_html/node12.html&quot;>;byte miss ratio&lt;/a>; in the days following HALP&#39;s final rollout on the YouTube CDN, which is now serving significantly more content from within the cache with lower latency to the end user, and without having to resort to more expensive retrieval that increases the operating costs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1Tl2wPMFShOcCQm8rfTRwa0CrNYhvDhrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s505/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;257&quot; data-original-width=&quot;505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1Tl2wPMFShOcCQm8rfTRwa0CrNYhvDhrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Aggregate worldwide YouTube byte miss ratio before and after rollout (vertical dashed line).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; An aggregated performance improvement could still hide important regressions. In addition to measuring overall impact, we also conduct an analysis in the paper to understand its impact on different racks using a machine level analysis, and find it to be overwhelmingly positive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduced a scalable state-of-the-art cache eviction framework that is based on learned rewards and uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;preference learning&lt;/a>; with automated feedback. Because of its design choices, HALP can be deployed in a manner similar to any other cache policy without the operational overhead of having to separately manage the labeled examples, training procedure and the model versions as additional offline pipelines common to most machine learning systems. Therefore, it incurs only a small extra overhead compared to other classical algorithms, but has the added benefit of being able to take advantage of additional features to make its eviction decisions and continuously adapt to changing access patterns. &lt;/p>; &lt;p>; This is the first large-scale deployment of a learned cache policy to a widely used and heavily trafficked CDN, and has significantly improved the CDN infrastructure efficiency while also delivering a better quality of experience to users. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Ramki Gummadi is now part of Google DeepMind. We would like to thank John Guilyard for help with the illustrations and Richard Schooler for feedback on this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8471212041038116532/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning-with-automated.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8471212041038116532&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8471212041038116532&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning-with-automated.html&quot; rel=&quot;alternate&quot; title=&quot;Preference learning with automated feedback for cache eviction&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeCdR0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s72-c/Halp%20hero%20gif.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3798161588273442525&lt;/id>;&lt;published>;2023-06-22T11:33:00.000-07:00&lt;/published>;&lt;updated>;2023-06-22T11:33:53.578-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Audio&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SoundStorm: Efficient parallel audio generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zalán Borsos, Research Software Engineer, and Marco Tagliasacchi, Senior Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZaYunzxOsmQf8vZZYzCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s2400/SoundStorm.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The recent progress in generative AI unlocked the possibility of creating new content in several different domains, including text, vision and audio. These models often rely on the fact that raw data is first converted to a compressed format as a sequence of tokens. In the case of audio, neural audio codecs (eg, &lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;>;SoundStream&lt;/a>; or &lt;a href=&quot;https://github.com/facebookresearch/encodec&quot;>;EnCodec&lt;/a>;) can efficiently compress waveforms to a compact representation, which can be inverted to reconstruct an approximation of the original audio signal. Such a representation consists of a sequence of discrete audio tokens, capturing the local properties of sounds (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Phoneme&quot;>;phonemes&lt;/a>;) and their temporal structure (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Prosody_(linguistics)&quot;>;prosody&lt;/a>;). By representing audio as a sequence of discrete tokens, audio generation can be performed with &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>;-based sequence-to-sequence models — this has unlocked rapid progress in speech continuation (eg, with &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;), text-to-speech (eg, with &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>;), and general audio and music generation (eg, &lt;a href=&quot;https://felixkreuk.github.io/audiogen/&quot;>;AudioGen&lt;/a>; and &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;). Many generative audio models, including AudioLM, rely on auto-regressive decoding, which produces tokens one by one. While this method achieves high acoustic quality, inference (ie, calculating an output) can be slow, especially when decoding long sequences. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To address this issue, in “&lt;a href=&quot;https://google-research.github.io/seanet/soundstorm/examples/&quot;>;SoundStorm: Efficient Parallel Audio Generation&lt;/a>;”, we propose a new method for efficient and high-quality audio generation. SoundStorm addresses the problem of generating long audio token sequences by relying on two novel elements: 1) an architecture adapted to the specific nature of audio tokens as produced by the SoundStream neural codec, and 2) a decoding scheme inspired by &lt;a href=&quot;https://arxiv.org/abs/2202.04200&quot;>;MaskGIT&lt;/a>;, a recently proposed method for image generation, which is tailored to operate on audio tokens. Compared to the autoregressive decoding approach of AudioLM, SoundStorm is able to generate tokens in parallel, thus decreasing the inference time by 100x for long sequences, and produces audio of the same quality and with higher consistency in voice and acoustic conditions. Moreover, we show that SoundStorm, coupled with the text-to-semantic modeling stage of &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>;, can synthesize high-quality, natural dialogues, allowing one to control the spoken content (via transcripts), speaker voices (via short voice prompts) and speaker turns (via transcript annotations), as demonstrated by the examples below: &lt;/p>; &lt;br>; &lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/SoundStorm_promo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;br>; &lt;br>; &lt;table>; &lt;tr>; &lt;td>;&lt;b>;Input: Text&lt;em>; (transcript used to drive the audio generation in bold)&lt;/em>;&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical-align: top&quot;>;&lt;span style=&quot;font-size: small;&quot;>;&lt;em>;Something really funny happened to me this morning. |哦，哇，什么？ | &lt;b>;Well, uh I woke up as usual. |呃呃|下楼去吃早餐。 |是啊|开始吃饭了。然后呃 10 分钟后我意识到现在是半夜了。 | Oh no way, that&#39;s so funny!&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical-align: top&quot;>;&lt;span style=&quot;font-size: small;&quot;>;&lt;em>;I didn&#39;t sleep well last night. |不好了。发生了什么？ | &lt;b>;I don&#39;t know.我就是无法入睡，整个晚上我都翻来覆去。 |这太糟糕了。也许你今晚应该尝试早点睡觉，或者你可以尝试读书。 |是的，谢谢你的建议，我希望你是对的。 |没问题。 II hope you get a good night&#39;s sleep&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;Input: Audio prompt&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/mp_joke_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/jm_sleep_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;Output: Audio prompt + generated audio&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/mp_joke_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/jm_sleep_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;SoundStorm design&lt;/h2>; &lt;p>; In our previous work on AudioLM, we showed that audio generation can be decomposed into two steps: 1) semantic modeling, which generates semantic tokens from either previous semantic tokens or a conditioning signal (eg, a transcript as in SPEAR-TTS, or a text prompt as in MusicLM), and 2) acoustic modeling, which generates acoustic tokens from semantic tokens. With SoundStorm we specifically address this second, acoustic modeling step, replacing slower autoregressive decoding with faster parallel decoding. &lt;/p>; &lt;p>; SoundStorm relies on a bidirectional attention-based &lt;a href=&quot;https://arxiv.org/abs/2005.08100&quot;>;Conformer&lt;/a>;, a model architecture that combines a Transformer with convolutions to capture both local and global structure of a sequence of tokens. Specifically, the model is trained to predict audio tokens produced by SoundStream given a sequence of semantic tokens generated by AudioLM as input. When doing this, it is important to take into account the fact that, at each time step &lt;em>;t&lt;/em>;, SoundStream uses up to &lt;em>;Q&lt;/em>; tokens to represent the audio using a method known as &lt;em>;residual vector quantization&lt;/em>; (RVQ), as illustrated below on the right. The key intuition is that the quality of the reconstructed audio progressively increases as the number of generated tokens at each step goes from 1 to &lt;em>;Q&lt;/em>;. &lt;/p>; &lt;p>; At inference time, given the semantic tokens as input conditioning signal, SoundStorm starts with all audio tokens masked out, and fills in the masked tokens over multiple iterations, starting from the coarse tokens at RVQ level &lt;em>;q = 1&lt;/em>; and proceeding level-by-level with finer tokens until reaching level &lt;em>;q = Q&lt;/em>;. &lt;/p>; &lt;p>; There are two crucial aspects of SoundStorm that enable fast generation: 1) tokens are predicted in parallel during a single iteration within a RVQ level and, 2) the model architecture is designed in such a way that the complexity is only mildly affected by the number of levels &lt;em>;Q&lt;/em>;. To support this inference scheme, during training a carefully designed masking scheme is used to mimic the iterative process used at inference. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOtw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7zSt2d985P6OC9On0xDwrLfj9OzJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;951&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOtw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7zSt2d985P6OC9On0xDwrLfj9OzJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SoundStorm model architecture. &lt;em>;T&lt;/em>; denotes the number of time steps and &lt;em>;Q&lt;/em>; the number of RVQ levels used by SoundStream. The semantic tokens used as conditioning are time-aligned with the SoundStream frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Measuring SoundStorm performance&lt;/h2>; &lt;p>; We demonstrate that SoundStorm matches the quality of AudioLM&#39;s acoustic generator, replacing both AudioLM&#39;s stage two (coarse acoustic model) and stage three (fine acoustic model). Furthermore, SoundStorm produces audio 100x faster than AudioLM&#39;s hierarchical autoregressive acoustic generator (top half below) with matching quality and improved consistency in terms of speaker identity and acoustic conditions (bottom half below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZgYL6-0bZWo8aLWAuYQrOJPf_vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s1999/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1122&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZgYL6-0bZWo8aLWAuYQrOJPf_vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Runtimes of SoundStream decoding, SoundStorm and different stages of AudioLM on a TPU-v4.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C-XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cpH9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCefUib8dFmGUm/s1312/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;702&quot; data-original-width=&quot;1312&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C-XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cpH9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCefUib8dFmGUm/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Acoustic consistency between the prompt and the generated audio. The shaded area represents the inter-quartile range.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Safety and risk mitigation&lt;/h2>; &lt;p>; We acknowledge that the audio samples produced by the model may be influenced by the unfair biases present in the training data, for instance in terms of represented accents and voice characteristics. In our generated samples, we demonstrate that we can reliably and responsibly control speaker characteristics via prompting, with the goal of avoiding unfair biases. A thorough analysis of any training data and its limitations is an area of future work in line with our responsible &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. &lt;/p>; &lt;p>; In turn, the ability to mimic a voice can have numerous malicious applications, including bypassing biometric identification and using the model for the purpose of impersonation. Thus, it is crucial to put in place safeguards against potential misuse: to this end, we have verified that the audio generated by SoundStorm remains detectable by a dedicated classifier using the same classifier as described in our original AudioLM paper. Hence, as a component of a larger system, we believe that SoundStorm would be unlikely to introduce additional risks to those discussed in our earlier papers on AudioLM and SPEAR-TTS. At the same time, relaxing the memory and computational requirements of AudioLM would make research in the domain of audio generation more accessible to a wider community. In the future, we plan to explore other approaches for detecting synthesized speech, eg, with the help of audio watermarking, so that any potential product usage of this technology strictly follows our responsible AI Principles. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have introduced SoundStorm, a model that can efficiently synthesize high-quality audio from discrete conditioning tokens. When compared to the acoustic generator of &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;, SoundStorm is two orders of magnitude faster and achieves higher temporal consistency when generating long audio samples. By combining a text-to-semantic token model similar to &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>; with SoundStorm, we can scale text-to-speech synthesis to longer contexts and generate natural dialogues with multiple speaker turns, controlling both the voices of the speakers and the generated content. SoundStorm is not limited to generating speech. For example, MusicLM uses SoundStorm to synthesize longer outputs efficiently (&lt;a href=&quot;https://ai.googleblog.com/2023/05/google-research-at-io-2023.html&quot;>;as seen at I/O&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The work described here was authored by Zalán Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour and Marco Tagliasacchi. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3798161588273442525/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient-parallel-audio.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3798161588273442525&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3798161588273442525&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient-parallel-audio.html&quot; rel=&quot;alternate&quot; title=&quot;SoundStorm: Efficient parallel audio generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZaYunzxOsmQf8vZZYzCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s72-c/SoundStorm.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3542229559799425219&lt;/id>;&lt;published>;2023-06-21T13:57:00.004-07:00&lt;/published>;&lt;updated>;2023-06-22T11:02:35.392-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Automatic Speech Recognition&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: AI for Social Good&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jimmy Tobin and Katrin Tomanek, Software Engineers, Google Research, AI for Social Good&lt;/span>; &lt;p>; Google&#39;s &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI for Social Good&lt;/a>; team consists of researchers, engineers, volunteers, and others with a shared focus on positive social impact. Our mission is to demonstrate AI&#39;s societal benefit by enabling real-world value, with projects spanning work in &lt;a href=&quot;https://blog.google/technology/health/making-data-useful-public-health/&quot;>;public health&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/making-android-more-accessible/&quot;>;accessibility&lt;/a>;, &lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;crisis response&lt;/a>;, &lt;a href=&quot;https://sustainability.google/&quot;>;climate and energy,&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;nature and society&lt;/a>;. We believe that the best way to drive positive change in underserved communities is by partnering with change-makers and the organizations they serve. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this blog post we discuss work done by &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;>;Project Euphonia&lt;/a>;, a team within &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI for Social Good&lt;/a>;, that aims to &lt;a href=&quot;https://ai.googleblog.com/2019/08/project-euphonias-personalized-speech.html&quot;>;improve automatic speech recognition&lt;/a>; (ASR) for people with disordered speech. For people with typical speech, an ASR model&#39;s word error rate (WER) can be less than 10%. But for people with disordered speech patterns, such as stuttering, &lt;a href=&quot;https://www.mayoclinic.org/diseases-conditions/dysarthria/symptoms-causes/syc-20371994&quot;>;dysarthria&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Apraxia&quot;>;apraxia&lt;/a>;, the WER could reach 50% or even 90% depending on the etiology and severity. To help address this problem, we worked with more than 1,000 participants to &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/project-euphonia-1000-hours-speech-recordings/&quot;>;collect over 1,000 hours of disordered speech samples&lt;/a>; and used the data to show that ASR &lt;a href=&quot;https://ai.googleblog.com/2021/09/personalized-asr-models-from-large-and.html&quot;>;personalization&lt;/a>; is a viable avenue for bridging the performance gap for users with disordered speech. We&#39;ve shown that personalization can be successful with as little as &lt;a href=&quot;https://arxiv.org/abs/2110.04612&quot;>;3-4 minutes of training speech&lt;/a>;&amp;nbsp;using &lt;a href=&quot;https://arxiv.org/abs/1907.13511&quot;>;layer freezing techniques&lt;/a>;. &lt;/p>; &lt;p>; This work led to the development of &lt;a href=&quot;https://sites.research.google/relate/&quot;>;Project Relate&lt;/a>;&amp;nbsp;for anyone with atypical speech who could benefit from a personalized speech model. Built in partnership with &lt;a href=&quot;https://research.google/research-areas/speech-processing/&quot;>;Google&#39;s Speech team&lt;/a>;, Project Relate enables people who find it hard to be understood by other people and technology to train their own models. People can use these personalized models to communicate more effectively and gain more independence. To make ASR more accessible and usable, we describe how we fine-tuned Google&#39;s &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; (USM) to better understand disordered speech out of the box, without personalization, for use with digital assistant technologies, dictation apps, and in conversations. &lt;/p>; &lt;br />; &lt;h2>;Addressing the challenges&lt;/h2>; &lt;p>; Working closely with Project Relate users, it became clear that personalized models can be very useful, but for many users, recording dozens or hundreds of examples can be challenging. In addition, the personalized models did not always perform well in freeform conversation. &lt;/p>; &lt;p>; To address these challenges, Euphonia&#39;s research efforts have been focusing on &lt;em>;speaker independent&lt;/em>; ASR (SI-ASR) to make models work better out of the box for people with disordered speech so that no additional training is necessary. &lt;/p>; &lt;br />; &lt;h2>;Prompted Speech dataset for SI-ASR&lt;/h2>; &lt;p>; The first step in building a robust SI-ASR model was to create representative dataset splits. We created the Prompted Speech dataset by splitting the &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2021/macdonald21_interspeech.pdf&quot;>;Euphonia corpus&lt;/a>; into train, validation and test portions, while ensuring that each split spanned a range of speech impairment severity and underlying etiology and that no speakers or phrases appeared in multiple splits. The training portion consists of over 950k speech utterances from over 1,000 speakers with disordered speech. The test set contains around 5,700 utterances from over 350 speakers. Speech-language pathologists manually reviewed all of the utterances in the test set for transcription accuracy and audio quality. &lt;/p>; &lt;br />; &lt;h2>;Real Conversation test set&lt;/h2>; &lt;p>; Unprompted or conversational speech differs from prompted speech in several ways. In conversation, people speak faster and enunciate less. They repeat words, repair misspoken words, and use a more expansive vocabulary that is specific and personal to themselves and their community. To improve a model for this use case, we created the Real Conversation test set to benchmark performance. &lt;/p>; &lt;p>; The Real Conversation test set was created with the help of trusted testers who recorded themselves speaking during conversations. The audio was reviewed, any personally identifiable information (PII) was removed, and then that data was transcribed by speech-language pathologists. The Real Conversation test set contains over 1,500 utterances from 29 speakers. &lt;/p>; &lt;br />; &lt;h2>;Adapting USM to disordered speech&lt;/h2>; &lt;p>; We then tuned USM on the training split of the Euphonia Prompted Speech set to improve its performance on disordered speech. Instead of fine-tuning the full model, our tuning was based on &lt;a href=&quot;http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf&quot;>;residual adapters&lt;/a>;, a parameter-efficient tuning approach that adds tunable bottleneck layers as residuals between the transformer layers. Only these layers are tuned, while the rest of the model weights are untouched. We have &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.541/&quot;>;previously shown&lt;/a>; that this approach works very well to adapt ASR models to disordered speech. Residual adapters were only added to the encoder layers, and the bottleneck dimension was set to 64. &lt;/p>; &lt;br />; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate the adapted USM, we compared it to older ASR models using the two test sets described above. For each test, we compare adapted USM to the pre-USM model best suited to that task: (1) For short prompted speech, we compare to Google&#39;s production ASR model optimized for short form ASR; (2) for longer Real Conversation speech, we compare to a model &lt;a href=&quot;https://arxiv.org/abs/2005.03271&quot;>;trained for long form ASR&lt;/a>;. USM improvements over pre-USM models can be explained by USM&#39;s relative size increase, 120M to 2B parameters, and other improvements discussed in the &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;USM blog post&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model word error rates (WER) for each test set (lower is better).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We see that the USM adapted with disordered speech significantly outperforms the other models. The adapted USM&#39;s WER on Real Conversation is 37% better than the pre-USM model, and on the Prompted Speech test set, the adapted USM performs 53% better. &lt;/p>; &lt;p>; These findings suggest that the adapted USM is significantly more usable for an end user with disordered speech. We can demonstrate this improvement by looking at transcripts of Real Conversation test set recordings from a trusted tester of Euphonia and Project Relate (see below). &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; -->; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example audio and transcriptions of a trusted tester&#39;s speech from the Real Conversation test set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A comparison of the Pre-USM and adapted USM transcripts revealed some key advantages: &lt;/p>; &lt;ul>;&lt;li>; The first example shows that Adapted USM is better at recognizing disordered speech patterns. The baseline misses key words like “XBox” and “controller” that are important for a listener to understand what they are trying to say. &lt;/li>; &lt;li>; The second example is a good example of how deletions are a primary issue with ASR models that are not trained with disordered speech. Though the baseline model did transcribe a portion correctly, a large part of the utterance was not transcribed, losing the speaker&#39;s intended message. &lt;/li>;&lt;/ul>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We believe that this work is an important step towards making speech recognition more accessible to people with disordered speech. We are continuing to work on improving the performance of our models. With the rapid advancements in ASR, we aim to ensure people with disordered speech benefit as well. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Key contributors to this project include Fadi Biadsy, Michael Brenner, Julie Cattiau, Richard Cave, Amy Chung-Yu Chou, Dotan Emanuel, Jordan Green, Rus Heywood, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Bob MacDonald, Philip Nelson, Katie Seaver, Joel Shor, Jimmy Tobin, Katrin Tomanek, and Subhashini Venugopalan. We gratefully acknowledge the support Project Euphonia received from members of the USM research team including Yu Zhang, Wei Han, Nanxin Chen, and many others. Most importantly, we wanted to say a huge thank you to the 2,200+ participants who recorded speech samples and the many &lt;a href=&quot;https://sites.research.google/euphonia/about/#thank-partners&quot;>;advocacy groups&lt;/a>; who helped us connect with these participants.&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;Audio volume has been adjusted for ease of listening, but the original files would be more consistent with those used in training and would have pauses, silences, variable volume, etc.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3542229559799425219/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: AI for Social Good&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s72-w640-h396-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4041992163804186827&lt;/id>;&lt;published>;2023-06-21T10:29:00.000-07:00&lt;/published>;&lt;updated>;2023-06-21T10:29:53.804-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;The world&#39;s first braiding of non-Abelian anyons&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Trond Andersen and Yuri Lensky, Research Scientists, Google Quantum AI Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s320/Non-Abelian%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Imagine you&#39;re shown two identical objects and then asked to close your eyes. When you open your eyes, you see the same two objects in the same position. How can you determine if they have been swapped back and forth? Intuition and the laws of &lt;a href=&quot;https://en.wikipedia.org/wiki/Identical_particles#Quantum_mechanical_description_of_identical_particles&quot;>;quantum mechanics agree&lt;/a>;: If the objects are truly identical, there is no way to tell. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While this sounds like common sense, it only applies to our familiar three-dimensional world. Researchers have predicted that for a special type of particle, called an &lt;a href=&quot;https://en.wikipedia.org/wiki/Anyon&quot;>;anyon&lt;/a>;, that is restricted to move only in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Plane_(mathematics)&quot;>;two-dimensional&lt;/a>; (2D) plane, quantum mechanics allows for something quite different. Anyons are indistinguishable from one another and some, non-Abelian anyons, have a special property that causes observable differences in the shared quantum state under exchange, making it possible to tell when they have been exchanged, despite being fully indistinguishable from one another. While researchers have managed to detect their relatives, Abelian anyons, whose change under exchange is more subtle and impossible to directly detect, realizing “non-Abelian exchange behavior” has proven more difficult due to challenges with both control and detection. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-05954-4&quot;>;Non-Abelian braiding of graph vertices in a superconducting processor&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;Nature&lt;/a>;&lt;/em>;, we report the observation of this non-Abelian exchange behavior for the first time. Non-Abelian anyons could open a new avenue for quantum computation, in which quantum operations are achieved by swapping particles around one another like strings are swapped around one another to create braids. Realizing this new exchange behavior on our &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;superconducting quantum processor&lt;/a>; could be an alternate route to so-called &lt;a href=&quot;https://arxiv.org/abs/quant-ph/9707021&quot;>;topological quantum computation&lt;/a>;, which benefits from being robust against environmental noise. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Exchange statistics and non-Abelian anyons&lt;/h2>; &lt;p>; In order to understand how this strange non-Abelian behavior can occur, it&#39;s helpful to consider an analogy with the braiding of two strings. Take two identical strings and lay them parallel next to one another. Swap their ends to form a double-helix shape. The strings are identical, but because they wrap around one another when the ends are exchanged, it is very clear when the two ends are swapped. &lt;/p>; &lt;p>; The exchange of non-Abelian anyons can be visualized in a similar way, where the strings are made from extending the particles&#39; positions into the time dimension to form “world-lines.” Imagine plotting two particles&#39; locations vs. time. If the particles stay put, the plot would simply be two parallel lines, representing their constant locations. But if we exchange the locations of the particles, the world lines wrap around one another. Exchange them a second time, and you&#39;ve made a &lt;a href=&quot;https://en.wikipedia.org/wiki/Knot_theory&quot;>;knot&lt;/a>;. &lt;/p>; &lt;p>; While a bit difficult to visualize, knots in four dimensions (three spatial plus one time dimension) can always easily be undone. They are trivial — like a shoelace, simply pull one end and it unravels. But when the particles are restricted to two spatial dimensions, the knots are in three total dimensions and — as we know from our everyday 3D lives — cannot always be easily untied. The braiding of the non-Abelian anyons&#39; world lines can be used as quantum computing operations to transform the state of the particles. &lt;/p>; &lt;p>; A key aspect of non-Abelian anyons is “degeneracy”: the full state of several separated anyons is not completely specified by local information, allowing the same anyon configuration to represent superpositions of several quantum states. Winding non-Abelian anyons about each other can change the encoded state. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;How to make a non-Abelian anyon&lt;/h2>; &lt;p>; So how do we realize non-Abelian braiding with one of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sycamore_processor&quot;>;Google&#39;s quantum processors&lt;/a>;? We start with the familiar surface code, which we recently used to achieve a &lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;milestone in quantum error correction&lt;/a>;, where qubits are arranged on the vertices of a checkerboard pattern. Each color square of the checkerboard represents one of two possible joint measurements that can be made of the qubits on the four corners of the square. These so-called “stabilizer measurements” can return a value of either + or – 1. The latter is referred to as a plaquette violation, and can be created and moved diagonally — just like bishops in chess — by applying single-qubit &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_logic_gate&quot;>;X- and Z-gates&lt;/a>;. Recently, we showed that these bishop-like &lt;a href=&quot;https://arxiv.org/abs/2104.01180&quot;>;plaquette violations are Abelian anyons&lt;/a>;. In contrast to non-Abelian anyons, the state of Abelian anyons changes only subtly when they are swapped — so subtly that it is impossible to directly detect. While Abelian anyons are interesting, they do not hold the same promise for topological quantum computing that non-Abelian anyons do. &lt;/p>; &lt;p>; To produce non-Abelian anyons, we need to control the degeneracy (ie, the number of &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function&quot;>;wavefunctions&lt;/a>; that causes all &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;stabilizer&lt;/a>; measurements to be +1). Since a stabilizer measurement returns two possible values, each stabilizer cuts the degeneracy of the system in half, and with sufficiently many stabilizers, only one wave function satisfies the criterion. Hence, a simple way to increase the degeneracy is to merge two stabilizers together. In the process of doing so, we remove one edge in the stabilizer grid, giving rise to two points where only three edges intersect. These points, referred to as “&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;degree-3 vertices&lt;/a>;” (D3Vs), are predicted to be non-Abelian anyons. &lt;/p>; &lt;p>; In order to braid the D3Vs, we have to move them, meaning that we have to stretch and squash the stabilizers into new shapes. We accomplish this by implementing two-qubit gates between the anyons and their neighbors (middle and right panels shown below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/s661/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;661&quot; height=&quot;529&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/w640-h529/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Non-Abelian anyons in stabilizer codes.&lt;strong>; a:&lt;/strong>; Example of a knot made by braiding two anyons&#39; world lines. &lt;strong>;b: &lt;/strong>;Single-qubit gates can be used to create and move stabilizers with a value of –1 (red squares). Like bishops in chess, these can only move diagonally and are therefore constrained to one sublattice in the regular surface code. This constraint is broken when D3Vs (yellow triangles) are introduced.&lt;strong>; c: &lt;/strong>;Process to form and move D3Vs (predicted to be non-Abelian anyons). We start with the surface code, where each square corresponds to a joint measurement of the four qubits on its corners (&lt;strong>;left&lt;/strong>; panel). We remove an edge separating two neighboring squares, such that there is now a single joint measurement of all six qubits (&lt;strong>;middle&lt;/strong>; panel). This creates two D3Vs, which are non-Abelian anyons. We move the D3Vs by applying two-qubit gates between neighboring sites (right panel). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Now that we have a way to create and move the non-Abelian anyons, we need to verify their anyonic behavior. For this we examine three characteristics that would be expected of non-Abelian anyons: &lt;/p>; &lt;ol>; &lt;li>;The “&lt;a href=&quot;https://en.wikipedia.org/wiki/Fusion_of_anyons&quot;>;fusion rules&lt;/a>;” — What happens when non-Abelian anyons collide with each other? &lt;/li>;&lt;li>;Exchange statistics — What happens when they are braided around one another? &lt;/li>;&lt;li>;Topological quantum computing primitives — Can we encode qubits in the non-Abelian anyons and use braiding to perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;>;two-qubit entangling operations&lt;/a>;? &lt;/li>; &lt;/ol>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The fusion rules of non-Abelian anyons&lt;/h2>; &lt;p>; We investigate fusion rules by studying how a pair of D3Vs interact with the bishop-like plaquette violations introduced above. In particular, we create a pair of these and bring one of them around a D3V by applying single-qubit gates. &lt;/p>; &lt;p>; While the rules of bishops in chess dictate that the plaquette violations can never meet, the dislocation in the checkerboard lattice allows them to break this rule, meet its partner and annihilate with it. The plaquette violations have now disappeared! But bring the non-Abelian anyons back in contact with one another, and the anyons suddenly morph into the missing plaquette violations. As weird as this behavior seems, it is a manifestation of exactly the fusion rules that we expect these entities to obey. This establishes confidence that the D3Vs are, indeed, non-Abelian anyons. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/s982/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;826&quot; data-original-width=&quot;982&quot; height=&quot;538&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/w640-h538/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Demonstration of anyonic fusion rules (starting with &lt;strong>;panel I&lt;/strong>;, in the &lt;strong>;lower left&lt;/strong>;). We form and separate two D3Vs (yellow triangles), then form two adjacent plaquette violations (red squares) and pass one between the D3Vs. The D3Vs deformation of the “chessboard” changes the bishop rules of the plaquette violations. While they used to lie on adjacent squares, they are now able to move along the same diagonals and collide (as shown by the red lines). When they do collide, they annihilate one another. The D3Vs are brought back together and surprisingly morph into the missing adjacent red plaquette violations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Observation of non-Abelian exchange statistics&lt;/h2>; &lt;p>; After establishing the fusion rules, we want to see the real smoking gun of non-Abelian anyons: non-Abelian exchange statistics. We create two pairs of non-Abelian anyons, then braid them by wrapping one from each pair around each other (shown below). When we fuse the two pairs back together, two pairs of plaquette violations appear. The simple act of braiding the anyons around one another changed the observables of our system. In other words, if you closed your eyes while the non-Abelian anyons were being exchanged, you would still be able to tell that they had been exchanged once you opened your eyes. This is the hallmark of non-Abelian statistics. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/s1648/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;1648&quot; height=&quot;437&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/w640-h437/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Braiding non-Abelian anyons. We make two pairs of D3Vs (&lt;strong>;panel II&lt;/strong>;), then bring one from each pair around each other (&lt;strong>;III-XI&lt;/strong>;). When fusing the two pairs together again in panel XII, two pairs of plaquette violations appear! Braiding the non-Abelian anyons changed the observables of the system from panel I to panel XII; a direct manifestation of non-Abelian exchange statistics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Topological quantum computing&lt;/h2>; &lt;p>; Finally, after establishing their fusion rules and exchange statistics, we demonstrate how we can use these particles in quantum computations. The non-Abelian anyons can be used to encode information, represented by &lt;a href=&quot;https://ai.googleblog.com/2021/08/demonstrating-fundamentals-of-quantum.html&quot;>;logical qubits&lt;/a>;, which should be distinguished from the actual &lt;em>;physical&lt;/em>; qubits used in the experiment. The number of logical qubits encoded in &lt;em>;N&lt;/em>; D3Vs can be shown to be &lt;em>;N&lt;/em>;/2–1, so we use &lt;em>;N&lt;/em>;=8 D3Vs to encode three logical qubits, and perform braiding to entangle them. By studying the resulting state, we find that the braiding has indeed led to the formation of the desired, well-known quantum entangled state called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenberger%E2%80%93Horne%E2%80%93Zeilinger_state&quot;>;Greenberger-Horne-Zeilinger&lt;/a>; (GHZ) state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/s750/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;544&quot; data-original-width=&quot;750&quot; height=&quot;464&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/w640-h464/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using non-Abelian anyons as logical qubits.&lt;strong>; a, &lt;/strong>;We braid the non-Abelian anyons to entangle three qubits encoded in eight D3Vs. &lt;strong>;b, &lt;/strong>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_tomography&quot;>;Quantum state tomography&lt;/a>; allows for reconstructing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Density_matrix&quot;>;density matrix&lt;/a>;, which can be represented in a 3D bar plot and is found to be consistent with the desired highly entangled GHZ-state.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our experiments show the first observation of non-Abelian exchange statistics, and that braiding of the D3Vs can be used to perform quantum computations. With future additions, including error correction during the braiding procedure, this could be a major step towards topological quantum computation, a long-sought method to endow qubits with intrinsic resilience against fluctuations and noise that would otherwise cause errors in computations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Katie McCormick, our Quantum Science Communicator, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4041992163804186827/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html&quot; rel=&quot;alternate&quot; title=&quot;The world&#39;s first braiding of non-Abelian anyons&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s72-c/Non-Abelian%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9139300663122353070&lt;/id>;&lt;published>;2023-06-18T11:00:00.018-07:00&lt;/published>;&lt;updated>;2023-06-19T10:54:26.149-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at CVPR 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shaina Mehta, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s1200/CVPR%20Design-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week marks the beginning of the premier annual &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;Computer Vision and Pattern Recognition&lt;/a>; conference (CVPR 2023), held in-person in Vancouver, BC (with additional virtual content). As a leader in computer vision research and a &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/Sponsors&quot;>;Platinum Sponsor&lt;/a>;, &lt;a href=&quot;https://research.google/&quot;>;Google Research&lt;/a>; will have a strong presence across CVPR 2023 with ~90 papers being presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers&quot;>;main conference&lt;/a>; and active involvement in over 40 conference &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/workshop-list&quot;>;workshops&lt;/a>; and &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/tutorial-list&quot;>;tutorials&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you are attending CVPR this year, please stop by our booth to chat with our researchers who are actively exploring the latest techniques for application to various areas of &lt;a href=&quot;https://research.google/pubs/?area=machine-perception&quot;>;machine perception&lt;/a>;. Our researchers will also be available to talk about and demo several recent efforts, including on-device ML applications with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>;, strategies for differential privacy, neural radiance field technologies and much more. &lt;/p>; &lt;p>; You can also learn more about our research being presented at CVPR 2023 in the list below (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>; Board and organizing committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Senior area chairs include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Area chairs include: &lt;em>;&lt;strong>;Andre Araujo&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Rodrigo Benenson&lt;/strong>;, &lt;strong>;Ayan Chakrabarti&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;, &lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Da-Cheng Jua&lt;/strong>;, &lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Stephen Lombardi&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;Saurabh Singh&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Pratul P. Srinivasan&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;, &lt;strong>;Jasper Uijlings&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Publicity Chair: &lt;strong>;&lt;em>;Boqing Gong&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Demonstration Chair: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Program Advisory Board includes: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Panels&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>;&lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;History and Future of Artificial Intelligence and Computer Vision &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>;&lt;/div>; &lt;p>; &lt;/p>; &lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;Scientific Discovery and the Environment &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;/div>; &lt;/div>; &lt;br />; &lt;h2>;Best Paper Award candidates&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.00277.pdf&quot;>;MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhiqin Chen&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11082.pdf&quot;>;DynIBaR: Neural Dynamic Image-Based Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Qianqian Wang&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Richard Tucker&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.12242.pdf&quot;>;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/a>; &lt;br />; &lt;em>;Nataniel Ruiz*, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Kfir Aberman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03142.pdf&quot;>;On Distillation of Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Chenlin Meng, Robin Rombach,&lt;strong>; Ruiqi Gao&lt;/strong>;,&lt;strong>; Diederik Kingma&lt;/strong>;, Stefano Ermon,&lt;strong>; Jonathan Ho&lt;/strong>;,&lt;strong>; Tim Salimans&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Highlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.11217.pdf&quot;>;Connecting Vision and Language with Video Localized Narratives&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Paul Voigtlaender&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.05496.pdf&quot;>;MaskSketch: Unpaired Structure-Guided Masked Image Generation&lt;/a>; &lt;br />; &lt;em>;Dina Bashkirova*,&lt;strong>; Jose Lezama&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11738.pdf&quot;>;SPARF: Neural Radiance Fields from Sparse and Noisy Poses&lt;/a>; &lt;br />; &lt;em>;Prune Truong*,&lt;strong>; Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Fabian Manhardt&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05199.pdf&quot;>;MAGVIT: Masked Generative Video Transformer&lt;/a>; &lt;br />; &lt;em>;Lijun Yu*,&lt;strong>; Yong Cheng&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alexander Hauptmann&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf&quot;>;Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.02291.pdf&quot;>;I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification&lt;/a>; &lt;br />; &lt;em>;Muhammad Ferjad Naeem, Gul Zain Khan,&lt;strong>; Yongqin Xian&lt;/strong>;, Muhammad Zeshan Afzal, Didier Stricker, Luc Van Gool,&lt;strong>; Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12624.pdf&quot;>;Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization&lt;/a>; &lt;br />; &lt;em>;Zifan Wang*,&lt;strong>; Nan Ding&lt;/strong>;, &lt;strong>;Tomer Levinboim&lt;/strong>;, &lt;strong>;Xi Chen&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/a>; (see &lt;a href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Su Wang&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Ceslee Montgomery&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Shai Noy&lt;/strong>;, &lt;strong>;Stefano Pellegrini&lt;/strong>;, &lt;strong>;Yasumasa Onoe&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;William Cha&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14306.pdf&quot;>;RUST: Latent Neural Scene Representations from Unposed Imagery&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mehdi SM Sajjadi&lt;/strong>;, &lt;strong>;Aravindh Mahendran&lt;/strong>;, &lt;strong>;Thomas Kipf&lt;/strong>;, &lt;strong>;Etienne Pot&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Mario Lučić&lt;/strong>;, &lt;strong>;Klaus Greff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05221.pdf&quot;>;REVEAL: Retrieval-Augmented Visual-Language Pre-training with Multi-Source Multimodal Knowledge Memory&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Ziniu Hu*,&lt;strong>; Ahmet Iscen&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Zirui Wang&lt;/strong>;, Kai-Wei Chang,&lt;strong>; Yizhou Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;David Ross&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.00833.pdf&quot;>;RobustNeRF: Ignoring Distractors with Robust Losses&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Sara Sabour&lt;/strong>;, &lt;strong>;Suhani Vora&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Ivan Krasin&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09682.pdf&quot;>;AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training&lt;/a>; &lt;br />; &lt;em>;Yifan Jiang*, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, Dejia Xu, &lt;strong>;Jonathan T. Barron&lt;/strong>;, Zhangyang Wang, Tianfan Xue*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kania_BlendFields_Few-Shot_Example-Driven_Facial_Modeling_CVPR_2023_paper.pdf&quot;>;BlendFields: Few-Shot Example-Driven Facial Modeling&lt;/a>; &lt;br />; &lt;em>;Kacper Kania, Stephan Garbin, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00583.pdf&quot;>;Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints&lt;/a>; &lt;br />; &lt;em>;Guilherme Potje, Felipe Cadar, &lt;strong>;Andre Araujo&lt;/strong>;, Renato Martins, Erickson Nascimento&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf&quot;>;How Can Objects Help Action Recognition?&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.12652.pdf&quot;>;Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur&lt;/a>; &lt;br />; &lt;em>;Peng Dai, &lt;strong>;Yinda Zhang&lt;/strong>;, Xin Yu, Xiaoyang Lyu, Xiaojuan Qi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14396.pdf&quot;>;IFSeg: Image-Free Semantic Segmentation via Vision-Language Model&lt;/a>; &lt;br />; &lt;em>;Sukmin Yun, Seong Park, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-Aware Saliency Modeling&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Shi Chen*, &lt;strong>;Nachiappan Valliappan&lt;/strong>;, &lt;strong>;Shaolei Shen&lt;/strong>;, &lt;strong>;Xinyu Ye&lt;/strong>;, &lt;strong>;Kai Kohlhoff&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09117.pdf&quot;>;MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis&lt;/a>; &lt;br />; &lt;em>;Tianhong Li*,&lt;strong>; Huiwen Chang&lt;/strong>;, Shlok Kumar Mishra,&lt;strong>; Han Zhang&lt;/strong>;, Dina Katabi,&lt;strong>; Dilip Krishnan&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17603.pdf&quot;>;NeRF-Supervised Deep Stereo&lt;/a>; &lt;br />; &lt;em>;Fabio Tosi, &lt;strong>;Alessio Tonioni&lt;/strong>;, Daniele Gregorio, Matteo Poggi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Suhail_Omnimatte3D_Associating_Objects_and_Their_Effects_in_Unconstrained_Monocular_Video_CVPR_2023_paper.pdf&quot;>;Omnimatte3D: Associating Objects and their Effects in Unconstrained Monocular Video&lt;/a>; &lt;br />; &lt;em>;Mohammed Suhail, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, Leon Sigal, &lt;strong>;Forrester Cole&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15654.pdf&quot;>;OpenScene: 3D Scene Understanding with Open Vocabularies&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Songyou Peng&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;, &lt;strong>;Chiyu Jiang&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Marc Pollefeys&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.08504.pdf&quot;>;PersonNeRF: Personalized Reconstruction from Photo Collections&lt;/a>; &lt;br />; &lt;em>;Chung-Yi Weng,&lt;strong>; Pratul Srinivasan&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Prefix_Conditioning_Unifies_Language_and_Label_Supervision_CVPR_2023_paper.pdf&quot;>;Prefix Conditioning Unifies Language and Label Supervision&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, Kate Saenko, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.pdf&quot;>;Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;AJ Piergiovanni&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01194.pdf&quot;>;Burstormer: Burst Image Restoration and Enhancement Transformer&lt;/a>; &lt;br />; &lt;em>;Akshay Dudhane, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan,&lt;strong>; Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15774.pdf&quot;>;Decentralized Learning with Multi-Headed Distillation&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Andrey Zhmoginov&lt;/strong>;, &lt;strong>;Mark Sandler&lt;/strong>;, &lt;strong>;Nolan Miller&lt;/strong>;, &lt;strong>;Gus Kristiansen&lt;/strong>;, &lt;strong>;Max Vladymyrov&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.02163.pdf&quot;>;GINA-3D: Learning to Generate Implicit Neural Assets in the Wild&lt;/a>; &lt;br />; &lt;em>;Bokui Shen, Xinchen Yan, Charles R. Qi, Mahyar Najibi, Boyang Deng,&lt;strong>; Leonidas Guibas&lt;/strong>;, Yin Zhou, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.11846.pdf&quot;>;Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent with Learned Distance Functions&lt;/a>; &lt;br />; &lt;em>;Yun He, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;, Xiangyang Xue, Yanwei Fu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.11042.pdf&quot;>;Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery from Sparse Image Ensemble&lt;/a>; &lt;br />; &lt;em>;Chun-Han Yao*, Wei-Chih Hung, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.00653.pdf&quot;>;Hyperbolic Contrastive Learning for Visual Representations beyond Objects&lt;/a>; &lt;br />; &lt;em>;Songwei Ge, Shlok Mishra,&lt;strong>; Simon Kornblith&lt;/strong>;, &lt;strong>;Chun-Liang Li, &lt;/strong>;David Jacobs&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09276.pdf&quot;>;Imagic: Text-Based Real Image Editing with Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Bahjat Kawar*,&lt;strong>; Shiran Zada&lt;/strong>;, &lt;strong>;Oran Lang&lt;/strong>;, &lt;strong>;Omer Tov&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Tali Dekel&lt;/strong>;, &lt;strong>;Inbar Mosseri&lt;/strong>;, &lt;strong>;Michal Irani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02743.pdf&quot;>;Incremental 3D Semantic Scene Graph Prediction from RGB Sequences&lt;/a>; &lt;br />; &lt;em>;Shun-Cheng Wu, &lt;strong>;Keisuke Tateno&lt;/strong>;, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.00575.pdf&quot;>;IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction&lt;/a>; &lt;br />; &lt;em>;Dekai Zhu, Guangyao Zhai, Yan Di, &lt;strong>;Fabian Manhardt&lt;/strong>;, Hendrik Berkemeyer, Tuan Tran, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.10844.pdf&quot;>;Learning to Generate Image Embeddings with User-Level Differential Privacy&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zheng Xu, Maxwell Collins, Yuxiao Wang, Liviu Panait, Sewoong Oh, Sean Augenstein, Ting Liu, Florian Schroff, H. Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05866.pdf&quot;>;NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs&lt;/a>; &lt;br />; &lt;em>;Harsh Rangwani, Lavish Bansal, Kartik Sharma,&lt;strong>; Tejan Karmali&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, Venkatesh Babu Radhakrishnan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09794.pdf&quot;>;NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Ron Mokady*, Amir Hertz*, &lt;strong>;Kfir Aberman&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, Daniel Cohen-Or*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14020.pdf&quot;>;SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow&lt;/a>; &lt;br />; &lt;em>;Itai Lang*,&lt;strong>; Dror Aiger&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Shai Avidan&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11674.pdf&quot;>;Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion&lt;/a>; &lt;br />; &lt;em>;Dario Pavllo*,&lt;strong>; David Joseph Tan&lt;/strong>;, &lt;strong>;Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12902.pdf&quot;>;TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Hanzhi Chen, &lt;strong>;Fabian Manhardt&lt;/strong>;, Nassir Navab, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf&quot;>;TryOnDiffusion: A Tale of Two UNets&lt;/a>; &lt;br />; &lt;em>;Luyang Zhu*, &lt;strong>;Dawei Yang&lt;/strong>;, &lt;strong>;Tyler Zhu&lt;/strong>;, &lt;strong>;Fitsum Reda&lt;/strong>;, &lt;strong>;William Chan&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03112.pdf&quot;>;A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning&lt;/a>; &lt;br />; &lt;em>;Aishwarya Kamath*, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;Su Wang&lt;/strong>;, Jing Yu Koh*, &lt;strong>;Alexander Ku&lt;/strong>;, &lt;strong>;Austin Waters&lt;/strong>;, Yinfei Yang*, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Zarana Parekh&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08045.pdf&quot;>;CLIPPO: Image-and-Language Understanding from Pixels Only&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Basil Mustafa&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.04745.pdf&quot;>;Controllable Light Diffusion for Portraits&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;David Futschik&lt;/strong>;, &lt;strong>;Kelvin Ritland&lt;/strong>;, &lt;strong>;James Vecore&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Daniel Sýkora&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Vasconcelos_CUF_Continuous_Upsampling_Filters_CVPR_2023_paper.pdf&quot;>;CUF: Continuous Upsampling Filters&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Cristina Vasconcelos&lt;/strong>;, &lt;strong>;Cengiz Oztireli&lt;/strong>;, &lt;strong>;Mark Matthews&lt;/strong>;, &lt;strong>;Milad Hashemi&lt;/strong>;, &lt;strong>;Kevin Swersky&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01758.pdf&quot;>;Improving Zero-Shot Generalization and Robustness of Multi-modal Models&lt;/a>; &lt;br />; &lt;em>;Yunhao Ge*, &lt;strong>;Jie Ren&lt;/strong>;, &lt;strong>;Andrew Gallagher&lt;/strong>;, &lt;strong>;Yuxiao Wang&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Hartwig Adam&lt;/strong>;, &lt;strong>;Laurent Itti&lt;/strong>;, &lt;strong>;Balaji Lakshminarayanan&lt;/strong>;, &lt;strong>;Jiaping Zhao&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09665.pdf&quot;>;LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding&lt;/a>; &lt;br />; &lt;em>;Gen Li, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, Laura Sevilla-Lara&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03361.pdf&quot;>;Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xiaoshuai Zhang&lt;/strong>;, &lt;strong>;Abhijit Kundu&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Hao Su&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01762.pdf&quot;>;Self-Supervised AutoFlow&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hsin-Ping Huang&lt;/strong>;, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Junhwa Hur&lt;/strong>;, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Kyle Sargent&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf&quot;>;Train-Once-for-All Personalization&lt;/a>; &lt;br />; &lt;em>;Hong-You Chen*,&lt;strong>; Yandong Li&lt;/strong>;, &lt;strong>;Yin Cui&lt;/strong>;, &lt;strong>;Mingda Zhang&lt;/strong>;, Wei-Lun Chao,&lt;strong>; Li Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.14115.pdf&quot;>;Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Antoine Yang*, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Antoine Miech, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, Ivan Laptev, Josef Sivic, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14302.pdf&quot;>;VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Junjie Ke&lt;/strong>;, &lt;strong>;Keren Ye&lt;/strong>;, &lt;strong>;Jiahui Yu&lt;/strong>;, &lt;strong>;Yonghui Wu&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Feng Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11152.pdf&quot;>;You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model&lt;/a>; &lt;br />; &lt;em>;Shengkun Tang, &lt;strong>;Yaqing Wang&lt;/strong>;, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, &lt;strong>;Yi Liang&lt;/strong>;, Dongkuan Xu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.05211.pdf&quot;>;Accidental Light Probes&lt;/a>; &lt;br />; &lt;em>;Hong-Xing Yu, Samir Agarwala, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;, &lt;strong>;Noah Snavely, &lt;/strong>;Jiajun Wu, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.09653.pdf&quot;>;FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning&lt;/a>; &lt;br />; &lt;em>;Yuanhao Xiong, Ruochen Wang, Minhao Cheng,&lt;strong>; Felix Yu&lt;/strong>;, Cho-Jui Hsieh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08013.pdf&quot;>;FlexiViT: One Model for All Patch Sizes&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Lucas Beyer&lt;/strong>;, &lt;strong>;Pavel Izmailov&lt;/strong>;, &lt;strong>;Alexander Kolesnikov&lt;/strong>;, &lt;strong>;Mathilde Caron&lt;/strong>;, &lt;strong>;Simon Kornblith&lt;/strong>;, &lt;strong>;Xiaohua Zhai&lt;/strong>;, &lt;strong>;Matthias Minderer&lt;/strong>;, &lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Ibrahim Alabdulmohsin&lt;/strong>;, &lt;strong>;Filip Pavetic&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03087.pdf&quot;>;Iterative Vision-and-Language Navigation&lt;/a>; &lt;br />; &lt;em>;Jacob Krantz, Shurjo Banerjee, Wang Zhu, Jason Corso,&lt;strong>; Peter Anderson&lt;/strong>;, Stefan Lee, Jesse Thomason&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.08010.pdf&quot;>;MoDi: Unconditional Motion Synthesis from Diverse Data&lt;/a>; &lt;br />; &lt;em>;Sigal Raab, Inbal Leibovitch, Peizhuo Li,&lt;strong>; Kfir Aberman&lt;/strong>;, Olga Sorkine-Hornung, Daniel Cohen-Or&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03369.pdf&quot;>;Multimodal Prompting with Missing Modalities for Visual Recognition&lt;/a>; &lt;br />; &lt;em>;Yi-Lun Lee,&lt;strong>; Yi-Hsuan Tsai&lt;/strong>;, Wei-Chen Chiu,&lt;strong>; Chen-Yu Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf&quot;>;Scene-Aware Egocentric 3D Human Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu,&lt;strong>; Kripasindhu Sarkar&lt;/strong>;, Christian Theobalt&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06247.pdf&quot;>;ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-Based Consistency&lt;/a>; &lt;br />; &lt;em>;Zixuan Huang,&lt;strong>; Varun Jampani&lt;/strong>;, Ngoc Anh Thai,&lt;strong>; Yuanzhen Li&lt;/strong>;, Stefan Stojanov, James M. Rehg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05173.pdf&quot;>;Improving Image Recognition by Retrieving from Web-Scale Image-Text Data&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Ahmet Iscen&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00341.pdf&quot;>;JacobiNeRF: NeRF Shaping with Mutual Information Gradients&lt;/a>; &lt;br />; &lt;em>;Xiaomeng Xu, Yanchao Yang, Kaichun Mo, Boxiao Pan, Li Yi,&lt;strong>; Leonidas Guibas&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01436.pdf&quot;>;Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos&lt;/a>; &lt;br />; &lt;em>;Ziqian Bai*,&lt;strong>; Feitong Tan&lt;/strong>;, &lt;strong>;Zeng Huang&lt;/strong>;, &lt;strong>;Kripasindhu Sarkar&lt;/strong>;, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Di Qiu&lt;/strong>;, &lt;strong>;Abhimitra Meka&lt;/strong>;, &lt;strong>;Ruofei Du&lt;/strong>;, &lt;strong>;Mingsong Dou&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;, Ping Tan,&lt;strong>; Thabo Beeler&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.08556.pdf&quot;>;NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis&lt;/a>; &lt;br />; &lt;em>;Allan Zhou, Mo Jin Kim, Lirui Wang,&lt;strong>; Pete Florence&lt;/strong>;, &lt;strong>;Chelsea Finn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03084.pdf&quot;>;Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*,&lt;strong>; Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13582.pdf&quot;>;SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mikaela Uy&lt;/strong>;, &lt;strong>;Ricardo Martin Brualla&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Ke Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06820.pdf&quot;>;Structured 3D Features for Reconstructing Controllable Avatars&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Enric Corona&lt;/strong>;, &lt;strong>;Mihai Zanfir&lt;/strong>;, &lt;strong>;Thiemo Alldieck&lt;/strong>;, &lt;strong>;Eduard Gabriel Bazavan&lt;/strong>;, &lt;strong>;Andrei Zanfir&lt;/strong>;, &lt;strong>;Cristian Sminchisescu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09119.pdf&quot;>;Token Turing Machines&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael S. Ryoo&lt;/strong>;, &lt;strong>;Keerthana Gopalakrishnan&lt;/strong>;, &lt;strong>;Kumara Kahatapitiya&lt;/strong>;, &lt;strong>;Ted Xiao&lt;/strong>;, &lt;strong>;Kanishka Rao&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Yao Lu&lt;/strong>;, &lt;strong>;Julian Ibarz&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10957.pdf&quot;>;TruFor: Leveraging All-Round Clues for Trustworthy Image Forgery Detection and Localization&lt;/a>; &lt;br />; &lt;em>;Fabrizio Guillaro, Davide Cozzolino,&lt;strong>; Avneesh Sud&lt;/strong>;, &lt;strong>;Nicholas Dufour, &lt;/strong>;Luisa Verdoliva&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.07685.pdf&quot;>;Video Probabilistic Diffusion Models in Projected Latent Space&lt;/a>; &lt;br />; &lt;em>;Sihyun Yu,&lt;strong>; Kihyuk Sohn, &lt;/strong>;Subin Kim, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00990.pdf&quot;>;Visual Prompt Tuning for Generative Transfer Learning&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Luisa Polania&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17811.pdf&quot;>;Zero-Shot Referring Image Segmentation with Global-Local Context Features&lt;/a>; &lt;br />; &lt;em>;Seonghoon Yu,&lt;strong>; Paul Hongsuck Seo&lt;/strong>;, Jeany Son&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.16501.pdf&quot;>;AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.03285.pdf&quot;>;DC2: Dual-Camera Defocus Control by Learning to Refocus&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hadi Alzayer&lt;/strong>;, &lt;strong>;Abdullah Abuolaim&lt;/strong>;, &lt;strong>;Leung Chun Chan&lt;/strong>;, &lt;strong>;Yang Yang&lt;/strong>;, &lt;strong>;Ying Chen Lou&lt;/strong>;, &lt;strong>;Jia-Bin Huang&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_Edges_to_Shapes_to_Concepts_Adversarial_Augmentation_for_Robust_Vision_CVPR_2023_paper.pdf&quot;>;Edges to Shapes to Concepts: Adversarial Augmentation for Robust Vision&lt;/a>; &lt;br />; &lt;em>;Aditay Tripathi*,&lt;strong>; Rishubh Singh&lt;/strong>;, Anirban Chakraborty,&lt;strong>; Pradeep Shenoy&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09898.pdf&quot;>;MetaCLUE: Towards Comprehensive Visual Metaphors Research&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arjun R. Akula&lt;/strong>;, &lt;strong>;Brendan Driscoll&lt;/strong>;, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Zhiwei Jia&lt;/strong>;, &lt;strong>;Suyash Damle&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;William T. Freeman&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13824.pdf&quot;>;Multi-Realism Image Compression with a Conditional Generator&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Eirikur Agustsson&lt;/strong>;, &lt;strong>;David Minnen&lt;/strong>;, &lt;strong>;George Toderici&lt;/strong>;, &lt;strong>;Fabian Mentzer&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.03267.pdf&quot;>;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/a>; &lt;br />; &lt;em>;Congyue Deng, Chiyu Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou,&lt;strong>; Leonidas Guibas&lt;/strong>;, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12053.pdf&quot;>;On Calibrating Semantic Segmentation Models: Analyses and an Algorithm&lt;/a>; &lt;br />; &lt;em>;Dongdong Wang,&lt;strong>; Boqing Gong&lt;/strong>;, Liqiang Wang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13515.pdf&quot;>;Persistent Nature: A Generative Model of Unbounded 3D Worlds&lt;/a>; &lt;br />; &lt;em>;Lucy Chai,&lt;strong>; Richard Tucker&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, Phillip Isola,&lt;strong>; Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13662.pdf&quot;>;Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment&lt;/a>; &lt;br />; &lt;em>;Yiyou Sun*,&lt;strong>; Yaojie Liu&lt;/strong>;, &lt;strong>;Xiaoming Liu&lt;/strong>;, Yixuan Li,&lt;strong>; Wen-Sheng Chu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13277.pdf&quot;>;SINE: Semantic-Driven Image-Based NeRF Editing with Prior-Guided Editing Field&lt;/a>; &lt;br />; &lt;em>;Chong Bao,&lt;strong>; Yinda Zhang&lt;/strong>;, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15533.pdf&quot;>;Sequential Training of GANs Against GAN-Classifiers Reveals Correlated &quot;Knowledge Gaps&quot; Present Among Independently Trained GAN Instances&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arkanath Pathak&lt;/strong>;, &lt;strong>;Nicholas Dufour&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.16991.pdf&quot;>;SparsePose: Sparse-View Camera Pose Regression and Refinement&lt;/a>; &lt;br />; &lt;em>;Samarth Sinha, Jason Zhang,&lt;strong>; Andrea Tagliasacchi&lt;/strong>;, Igor Gilitschenski, David Lindell&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;Teacher-Generated Spatial-Attention Labels Boost Robustness and Accuracy of Contrastive Models&lt;/a>; &lt;br />; &lt;em>;Yushi Yao,&lt;strong>; Chang Ye&lt;/strong>;, &lt;strong>;Gamaleldin F. Elsayed&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://cv4mr.github.io/&quot;>;Computer Vision for Mixed Reality&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ira Kemelmacher-Shlizerman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvpr2023.wad.vision/&quot;>;Workshop on Autonomous Driving (WAD)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://multimodal-content-moderation.github.io/&quot;>;Multimodal Content Moderation (MMCM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Chris Bregler&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mevan Babakar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mcv-workshop.github.io/#updates&quot;>;Medical Computer Vision (MCV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Shekoofeh Azizi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vand-cvpr23/home&quot;>;VAND: Visual Anomaly and Novelty Detection&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Jie Ren&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://struco3d.github.io/cvpr2023/&quot;>;Structural and Compositional Learning on 3D Data&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Fei Xia&lt;/strong>;, &lt;strong>;Amir Hertz&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgvc10&quot;>;Fine-Grained Visual Categorization (FGVC10)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Kimberly Wilber&lt;/strong>;, &lt;strong>;Sara Beery&lt;/strong>;&lt;/em>; &lt;br />; Panelists include: &lt;strong>;&lt;em>;Hartwig Adam&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/xrnerf/&quot;>;XRNeRF: Advances in NeRF for the Metaverse&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnilabel-workshop-cvpr23/overview&quot;>;OmniLabel: Infinite Label Spaces for Semantic Understanding via Natural Language&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Long Zhao&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vittorio Ferrari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://holistic-video-understanding.github.io/workshops/cvpr2023.html&quot;>;Large Scale Holistic Video Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;David Ross&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nice.lgresearch.ai/&quot;>;New Frontiers for Zero-Shot Image Captioning Evaluation (NICE)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ccd2023.github.io/&quot;>;Computational Cameras and Displays (CCD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ulugbek Kamilov&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mauricio Delbracio&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gazeworkshop.github.io/2023/&quot;>;Gaze Estimation and Prediction in the Wild (GAZE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Thabo Beele&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Erroll Wood&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgahi2023/home&quot;>;Face and Gesture Analysis for Health Informatics (FGAHI)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.cv4animals.com/&quot;>;Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;3D Vision and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Pete Florence&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;End-to-End Autonomous Driving: Perception, Prediction, Planning and Simulation (E2EAD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://opendrivelab.com/e2ead/cvpr23&quot;>;End-to-End Autonomous Driving: Emerging Tasks and Challenges&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mula-workshop.github.io/&quot;>;Multi-modal Learning and Applications (MULA)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Aleksander Hołyński&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/sdas2023/&quot;>;Synthetic Data for Autonomous Systems (SDAS)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vdu-cvpr23&quot;>;Vision Datasets Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;José Lezama&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vijay Janapa Reddi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ieeecvf-cvpr2023-precognition/&quot;>;Precognition: Seeing Through the Future&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Utsav Prabhu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvlai.net/ntire/2023/&quot;>;New Trends in Image Restoration and Enhancement (NTIRE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://generative-vision.github.io/workshop-CVPR-23/&quot;>;Generative Models for Computer Vision&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://robustart.github.io/&quot;>;Adversarial Machine Learning on Computer Vision: Art of Robustness&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Xinyun Chen&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Deqing Sun&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/wmf2023/home&quot;>;Media Forensics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Nicholas Carlini&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://taodataset.org/workshop/cvpr23/&quot;>;Tracking and Its Many Guises: Tracking Any Object in Open-World&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Paul Voigtlaender&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://scene-understanding.com/&quot;>;3D Scene Understanding for Vision, Graphics, and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.es.ele.tue.nl/cvpm23/&quot;>;Computer Vision for Physiological Measurement (CVPM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ibug.doc.ic.ac.uk/resources/cvpr-2023-5th-abaw/&quot;>;Affective Behaviour Analysis In-the-Wild&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Stefanos Zafeiriou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2023/home&quot;>;Ethical Considerations in Creative Applications of Computer Vision (EC3V)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Rida Qadri&lt;/strong>;, &lt;strong>;Mohammad Havaei&lt;/strong>;, &lt;strong>;Fernando Diaz&lt;/strong>;, &lt;strong>;Emily Denton&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;Negar Rostamzadeh&lt;/strong>;, &lt;strong>;Pamela Peter-Agbia&lt;/strong>;, &lt;strong>;Eva Kozanecka&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vizwiz.org/workshops/2023-workshop/&quot;>;VizWiz Grand Challenge: Describing Images and Videos Taken by Blind People&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Haoran Qi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ecv23/home&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;blog post&lt;/a>;) &lt;br />; Organizers include: &lt;em>;&lt;strong>;Andrew Howard&lt;/strong>;, &lt;strong>;Chas Leichner&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andrew Howard&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vcdw2023/&quot;>;Visual Copy Detection&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Priya Goyal&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://3dmv2023.github.io/&quot;>;Learning 3D with Multi-View Supervision (3DMV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://image-matching-workshop.github.io/&quot;>;Image Matching: Local Features and Beyond&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Eduard Trulls&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vision4allseason.net/&quot;>;Vision for All Seasons: Adverse Weather and Lightning Conditions (V4AS)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/t4v-cvpr23&quot;>;Transformers for Vision (T4V)&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/academic-cv/&quot;>;Scholars vs Big Models — How Can Academics Adapt?&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Jonathan T. Barron&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://www.scan-net.org/cvpr2023workshop/&quot;>;ScanNet Indoor Scene Understanding Challenge&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Tom Funkhouser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvmi-workshop.github.io/new.html&quot;>;Computer Vision for Microscopy Image Analysis&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Po-Hsuan Cameron Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://embeddedvisionworkshop.wordpress.com/&quot;>;Embedded Vision&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Rahul Sukthankar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sightsound.org/&quot;>;Sight and Sound&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;William Freeman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ai4cc.net/&quot;>;AI for Content Creation&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Tim Salimans&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://computer-vision-in-the-wild.github.io/cvpr-2023/https://computer-vision-in-the-wild.github.io/cvpr-2023/&quot;>;Computer Vision in the Wild&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Xiuye Gu&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vispr-workshop.github.io/&quot;>;Visual Pre-training for Robotics&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnicv2023/home&quot;>;Omnidirectional Computer Vision&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Yi-Hsuan Tsai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://all-things-vits.github.io/atv/&quot;>;All Things ViTs: Understanding and Interpreting Attention in Vision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hila Chefer&lt;/strong>;, &lt;strong>;Sayak Paul&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-tutorial-on-ad/&quot;>;Recent Advances in Anomaly Detection&lt;/a>; &lt;br />; &lt;em>;Guansong Pang, Joey Tianyi Zhou, Radu Tudor Ionescu, Yu Tian,&lt;strong>; Kihyuk Sohn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr-tutorial-2023/home&quot;>;Contactless Healthcare Using Cameras and Wireless Sensors&lt;/a>; &lt;br />; &lt;em>;Wenjin Wang, Xuyu Wang, Jun Luo,&lt;strong>; Daniel McDuff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://osimeoni.github.io/object-localization-for-free/&quot;>;Object Localization for Free: Going Beyond Self-Supervised Learning&lt;/a>; &lt;br />; &lt;em>;Oriane Simeoni,&lt;strong>; &lt;/strong>;Weidi Xie,&lt;strong>; Thomas Kipf&lt;/strong>;, Patrick Pérez&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://prompting-in-vision.github.io/&quot;>;Prompting in Vision&lt;/a>; &lt;br />; &lt;em>;Kaiyang Zhou, Ziwei Liu, Phillip Isola, Hyojin Bahng, Ludwig Schmidt, Sarah Pratt,&lt;strong>; Denny Zhou&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/9139300663122353070/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at CVPR 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s72-c/CVPR%20Design-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2312998356111901143&lt;/id>;&lt;published>;2023-06-15T13:53:00.000-07:00&lt;/published>;&lt;updated>;2023-06-15T13:53:27.418-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Android&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Juhyun Lee and Raman Sarokin, Software Engineers, Core Systems &amp;amp; Experiences&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s700/SpeedHero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The proliferation of large &lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;diffusion models&lt;/a>; for &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;image generation&lt;/a>; has led to a significant increase in model size and inference workloads. On-device ML inference in mobile environments requires meticulous performance optimization and consideration of trade-offs due to resource constraints. Running inference of large diffusion models (LDMs) on-device, driven by the need for cost efficiency and user privacy, presents even greater challenges due to the substantial memory requirements and computational demands of these models. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We address this challenge in our work titled “&lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations&lt;/a>;” (to be presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>; workshop for &lt;a href=&quot;https://sites.google.com/corp/view/ecv23&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>;) focusing on the optimized execution of a foundational LDM model on a mobile GPU. In this blog post, we summarize the core techniques we employed to successfully execute large diffusion models like &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;Stable Diffusion&lt;/a>; at full resolution (512x512 pixels) and 20 iterations on modern smartphones with high-performing inference speed of the original model without distillation of under 12 seconds. As discussed in &lt;a href=&quot;https://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html&quot;>;our previous blog post&lt;/a>;, GPU-accelerated ML inference is often limited by memory performance, and execution of LDMs is no exception.因此，我们优化的中心主题是高效的内存输入/输出 (I/O)，即使这意味着选择内存高效的算法而不是优先考虑算术逻辑单元效率的算法。最终，我们的主要目标是减少机器学习推理的总体延迟。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s512/image4.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A sample output of an LDM on Mobile GPU with the prompt text: “a photo realistic and high resolution image of a cute puppy with surrounding flowers”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Enhanced attention module for memory efficiency&lt;/h2>; &lt;p>; An ML inference engine typically provides a variety of optimized ML operations. Despite this, achieving optimal performance can still be challenging as there is a certain amount of overhead for executing individual neural net operators on a GPU. To mitigate this overhead, ML inference engines incorporate extensive operator fusion rules that consolidate multiple operators into a single operator, thereby reducing the number of iterations across tensor elements while maximizing compute per iteration. For instance, &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; utilizes operator fusion to combine computationally expensive operations, like convolutions, with subsequent activation functions, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;>;rectified linear units&lt;/a>;, into one. &lt;/p>; &lt;p>; A clear opportunity for optimization is the heavily used attention block adopted in the &lt;a href=&quot;https://arxiv.org/pdf/2304.11267.pdf&quot;>;denoiser model&lt;/a>; in the LDM. The attention blocks allow the model to focus on specific parts of the input by assigning higher weights to important regions. There are multiple ways one can optimize the attention modules, and we selectively employ one of the two optimizations explained below depending on which optimization performs better. &lt;/p>; &lt;p>; The first optimization, which we call &lt;em>;partially fused &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;>;softmax&lt;/a>;&lt;/em>;, removes the need for extensive memory writes and reads between the softmax and the matrix multiplication in the attention module. Let the attention block be just a simple matrix multiplication of the form &lt;b>;&lt;em>;Y &lt;/em>;&lt;/b>;= softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) * &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; where &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; are 2D matrices of shape &lt;em>;a&lt;/em>;×&lt;em>;b&lt;/em>; and &lt;em>;b&lt;/em>;×&lt;em>;c&lt;/em>;, respectively (shown below in the top half). &lt;/p>; &lt;p>; For numerical stability, &lt;b>;&lt;em>;T = &lt;/em>;&lt;/b>;softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) is typically calculated in three passes: &lt;/p>; &lt;ol>; &lt;li>;Determine the maximum value in the list, &lt;em>;ie&lt;/em>;.,&lt;em>; &lt;/em>;for each row in matrix &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; &lt;/li>;&lt;li>;Sum up the differences of the exponential of each list item and the maximum value (from pass 1) &lt;/li>;&lt;li>;Divide the exponential of the items minus the maximum value by the sum from pass 2 &lt;/li>; &lt;/ol>; &lt;p>; Carrying out these passes naïvely would result in a huge memory write for the temporary intermediate tensor &lt;strong>;&lt;em>;T&lt;/em>;&lt;/strong>; holding the output of the entire softmax function. We bypass this large memory write if we only store the results of passes 1 and 2, labeled &lt;b>;&lt;em>;m&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;s&lt;/em>;&lt;/b>;, respectively, which are small vectors, with &lt;em>;a&lt;/em>; elements each, compared to &lt;strong>;&lt;em>;T &lt;/em>;&lt;/strong>;which has &lt;em>;a·b &lt;/em>;elements. With this technique, we are able to reduce tens or even hundreds of megabytes of memory consumption by multiple orders of magnitude (shown below in the bottom half). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s568/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;457&quot; data-original-width=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Attention modules. &lt;b>;Top&lt;/b>;: A naïve attention block, composed of a SOFTMAX (with all three passes) and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;MATMUL&lt;/a>;, requires a large memory write for the big intermediate tensor &lt;em>;T&lt;/em>;. &lt;b>;Bottom&lt;/b>;: Our memory-efficient attention block with partially fused softmax in MATMUL only needs to store two small intermediate tensors for &lt;em>;m&lt;/em>; and s.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The other optimization involves employing &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;>;FlashAttention&lt;/a>;, which is an I/O-aware, exact attention algorithm. This algorithm reduces the number of GPU high-bandwidth memory accesses, making it a good fit for our memory bandwidth–limited use case. However, we found this technique to only work for &lt;a href=&quot;https://en.wikipedia.org/wiki/Static_random-access_memory&quot;>;SRAM&lt;/a>; with certain sizes and to require a large number of registers. Therefore, we only leverage this technique for attention matrices with a certain size on a select set of GPUs. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Winograd fast convolution for 3×3 convolution layers&lt;/h2>; &lt;p>; The backbone of common LDMs heavily relies on 3×3 convolution layers (convolutions with filter size 3×3), comprising over 90% of the layers in the decoder. Despite increased memory consumption and numerical errors, we found that &lt;a href=&quot;https://arxiv.org/abs/1509.09308&quot;>;Winograd fast convolution&lt;/a>; to be effective at speeding up the convolutions. Distinct from the &lt;em>;filter size&lt;/em>; 3x3 used in convolutions, &lt;em>;tile size&lt;/em>; refers to the size of a sub region of the input tensor that is processed at a time. Increasing the tile size enhances the efficiency of the convolution in terms of &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_logic_unit&quot;>;arithmetic logic unit&lt;/a>; (ALU) usage. However, this improvement comes at the expense of increased memory consumption. Our tests indicate that a tile size of 4×4 achieves the optimal trade-off between computational efficiency and memory utilization. &lt;/p>; &lt;br>; &lt;table align=&quot;center&quot; style=&quot;text-align: center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;/td>; &lt;td>;&lt;/td>; &lt;td colspan=&quot;2&quot;>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Memory usage&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Tile size&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; savings&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Intermediate tensors&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Weights&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;2×2 &lt;/td>; &lt;td>;2.25× &lt;/td>; &lt;td>;4.00× &lt;/td>; &lt;td>;1.77× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;4×4&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;2.25×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;6×6 &lt;/td>; &lt;td>;5.06× &lt;/td>; &lt;td>;1.80× &lt;/td>; &lt;td>;7.12× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;8×8 &lt;/td>; &lt;td>;5.76× &lt;/td>; &lt;td>;1.56× &lt;/td>; &lt;td>;11.1× &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Impact of Winograd with varying tile sizes for 3×3 convolutions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Specialized operator fusion for memory efficiency&lt;/h2>; &lt;p>; We discovered that performantly inferring LDMs on a mobile GPU requires significantly larger fusion windows for commonly employed layers and units in LDMs than current off-the-shelf on-device GPU-accelerated ML inference engines provide. Consequently, we developed specialized implementations that could execute a larger range of neural operators than typical fusion rules would permit. Specifically, we focused on two specializations: the &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;>;Gaussian Error Linear Unit&lt;/a>; (GELU) and the &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;>;group normalization&lt;/a>; layer. &lt;/p>; &lt;p>; An approximation of GELU with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_functions&quot;>;hyperbolic tangent&lt;/a>; function requires writing to and reading from seven auxiliary intermediate tensors (shown below as light orange rounded rectangles in the figure below), reading from the input tensor &lt;em>;x&lt;/em>; three times, and writing to the output tensor &lt;em>;y&lt;/em>; once across eight GPU programs implementing the labeled operation each (light blue rectangles). A custom GELU implementation that performs the eight operations in a single shader (shown below in the bottom) can bypass all the memory I/O for the intermediate tensors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s958/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;210&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;GELU implementations. &lt;strong>;Top&lt;/strong>;: A naïve implementation with built-in operations would require 8 memory writes and 10 reads. &lt;strong>;Bottom&lt;/strong>;: Our custom GELU only requires 1 memory read (for &lt;em>;x&lt;/em>;) and 1 write (for &lt;em>;y&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; After applying all of these optimizations, we conducted tests of Stable Diffusion 1.5 (image resolution 512x512, 20 iterations) on high-end mobile devices. Running Stable Diffusion with our GPU-accelerated ML inference model uses 2,093MB for the weights and 84MB for the intermediate tensors. With latest high-end smartphones, Stable Diffusion can be run in under 12 seconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s522/image3.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;270&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stable Diffusion runs on modern smartphones in under 12 seconds. Note that running the decoder after each iteration for displaying the intermediate output in this animated GIF results in a ~2× slowdown.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Performing on-device ML inference of large models has proven to be a substantial challenge, encompassing limitations in model file size, extensive runtime memory requirements, and protracted inference latency. By recognizing memory bandwidth usage as the primary bottleneck, we directed our efforts towards optimizing memory bandwidth utilization and striking a delicate balance between ALU efficiency and memory efficiency. As a result, we achieved state-of-the-art inference latency for large diffusion models. You can learn more about this work in &lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;the paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank Yu-Hui Chen, Jiuqiang Tang, Frank Barchard, Yang Zhao, Joe Zou, Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, Lu Wang, and Matthias Grundmann.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2312998356111901143/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot; rel=&quot;alternate&quot; title=&quot;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s72-c/SpeedHero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3072875289355194363&lt;/id>;&lt;published>;2023-06-14T09:00:00.001-07:00&lt;/published>;&lt;updated>;2023-06-22T14:55:31.687-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computational Photography&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Reconstructing indoor spaces with NeRF&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Marcos Seefelder, Software Engineer, and Daniel Duckworth, Research Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s320/Immersive%20View%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; When choosing a venue, we often find ourselves with questions like the following: Does this restaurant have the right vibe for a date? Is there good outdoor seating? Are there enough screens to watch the game? While photos and videos may partially answer questions like these, they are no substitute for feeling like you&#39;re there, even when visiting in person isn&#39;t an option. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Immersive experiences that are interactive, photorealistic, and multi-dimensional stand to bridge this gap and recreate the feel and vibe of a space, empowering users to naturally and intuitively find the information they need. To help with this, Google Maps launched &lt;a href=&quot;https://blog.google/products/maps/google-maps-updates-immersive-view-trip-planning&quot;>;Immersive View&lt;/a>;, which uses advances in machine learning (ML) and computer vision to fuse billions of &lt;a href=&quot;https://www.google.com/streetview/&quot;>;Street View&lt;/a>; and aerial images to create a rich, digital model of the world. Beyond that, it layers helpful information on top, like the weather, traffic, and how busy a place is. Immersive View provides indoor views of restaurants, cafes, and other venues to give users a virtual up-close look that can help them confidently decide where to go. &lt;/p>; &lt;p>; Today we describe the work put into delivering these indoor views in Immersive View. We build on &lt;a href=&quot;https://arxiv.org/abs/2003.08934&quot;>;neural radiance fields&lt;/a>; (NeRF), a state-of-the-art approach for fusing photos to produce a realistic, multi-dimensional reconstruction within a neural network. We describe our pipeline for creation of NeRFs, which includes custom photo capture of the space using DSLR cameras, image processing and scene reproduction. We take advantage of Alphabet&#39;s &lt;a href=&quot;https://arxiv.org/abs/2008.02268&quot;>;recent&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2111.12077&quot;>;advances&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2202.05263&quot;>;in the field&lt;/a>; to design a method matching or outperforming the prior state-of-the-art in visual fidelity. These models are then embedded as interactive 360° videos following curated flight paths, enabling them to be available on smartphones. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/introduction_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The reconstruction of The Seafood Bar in Amsterdam in Immersive View.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;From photos to NeRFs&lt;/h2>; &lt;p>; At the core of our work is &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>;NeRF&lt;/a>;, a recently-developed method for 3D reconstruction and novel view synthesis. Given a collection of photos describing a scene, NeRF distills these photos into a &lt;a href=&quot;https://arxiv.org/abs/2111.11426&quot;>;neural field&lt;/a>;, which can then be used to render photos from viewpoints not present in the original collection. &lt;/p>; &lt;p>; While NeRF largely solves the challenge of reconstruction, a user-facing product based on real-world data brings a wide variety of challenges to the table. For example, reconstruction quality and user experience should remain consistent across venues, from dimly-lit bars to sidewalk cafes to hotel restaurants. At the same time, privacy should be respected and any potentially personally identifiable information should be removed. Importantly, scenes should be captured consistently and efficiently, reliably resulting in high-quality reconstructions while minimizing the effort needed to capture the necessary photographs. Finally, the same natural experience should be available to all mobile users, regardless of the device on hand. &lt;/p>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/overview_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Immersive View indoor reconstruction pipeline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Capture &amp;amp; preprocessing&lt;/h2>; &lt;p>; The first step to producing a high-quality NeRF is the careful capture of a scene: a dense collection of photos from which 3D geometry and color can be derived. To obtain the best possible reconstruction quality, every surface should be observed from multiple different directions. The more information a model has about an object&#39;s surface, the better it will be in discovering the object&#39;s shape and the way it interacts with lights. &lt;/p>; &lt;p>; In addition, NeRF models place further assumptions on the camera and the scene itself. For example, most of the camera&#39;s properties, such as white balance and aperture, are assumed to be fixed throughout the capture. Likewise, the scene itself is assumed to be frozen in time: lighting changes and movement should be avoided. This must be balanced with practical concerns, including the time needed for the capture, available lighting, equipment weight, and privacy. In partnership with professional photographers, we developed a strategy for quickly and reliably capturing venue photos using DSLR cameras within only an hour timeframe. This approach has been used for all of our NeRF reconstructions to date. &lt;/p>; &lt;p>; Once the capture is uploaded to our system, processing begins. As photos may inadvertently contain sensitive information, we automatically scan and blur personally identifiable content. We then apply a &lt;a href=&quot;https://en.wikipedia.org/wiki/Structure_from_motion&quot;>;structure-from-motion&lt;/a>; pipeline to solve for each photo&#39;s &lt;a href=&quot;https://www.mathworks.com/help/vision/ug/camera-calibration.html&quot;>;camera parameters&lt;/a>;: its position and orientation relative to other photos, along with lens properties like &lt;a href=&quot;https://en.wikipedia.org/wiki/Focal_length&quot;>;focal length&lt;/a>;. These parameters associate each pixel with a point and a direction in 3D space and constitute a key signal in the NeRF reconstruction process. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NeRF reconstruction&lt;/h2>; &lt;p>; Unlike many ML models, a new NeRF model is trained from scratch on each captured location. To obtain the best possible reconstruction quality within a target compute budget, we incorporate features from a variety of published works on NeRF developed at Alphabet. Some of these include: &lt;/p>; &lt;ul>; &lt;li>;We build on &lt;a href=&quot;https://jonbarron.info/mipnerf360/&quot;>;mip-NeRF 360&lt;/a>;, one of the best-performing NeRF models to date. While more computationally intensive than Nvidia&#39;s widely-used &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;Instant NGP&lt;/a>;, we find the mip-NeRF 360 consistently produces fewer artifacts and higher reconstruction quality. &lt;/li>;&lt;li>;We incorporate the low-dimensional generative latent optimization (GLO) vectors introduced in &lt;a href=&quot;https://nerf-w.github.io/&quot;>;NeRF in the Wild&lt;/a>; as an auxiliary input to the model&#39;s radiance network. These are learned real-valued latent vectors that embed appearance information for each image. By assigning each image in its own latent vector, the model can capture phenomena such as lighting changes without resorting to cloudy geometry, a common artifact in casual NeRF captures. &lt;/li>;&lt;li>;We also incorporate exposure conditioning as introduced in &lt;a href=&quot;https://waymo.com/research/block-nerf/&quot;>;Block-NeRF&lt;/a>;. Unlike GLO vectors, which are uninterpretable model parameters, exposure is directly derived from a photo&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Exif&quot;>;metadata&lt;/a>; and fed as an additional input to the model&#39;s radiance network. This offers two major benefits: it opens up the possibility of varying &lt;a href=&quot;https://en.wikipedia.org/wiki/Film_speed#Digital_camera_ISO_speed_and_exposure_index&quot;>;ISO&lt;/a>; and provides a method for controlling an image&#39;s brightness at inference time. We find both properties invaluable for capturing and reconstructing dimly-lit venues. &lt;/li>; &lt;/ul>; &lt;p>; We train each NeRF model on TPU or GPU accelerators, which provide different trade-off points. As with all Google products, we continue to search for new ways to improve, from reducing compute requirements to improving reconstruction quality. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/side_by_side_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A side-by-side comparison of our method and a mip-NeRF 360 baseline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A scalable user experience&lt;/h2>; &lt;p>; Once a NeRF is trained, we have the ability to produce new photos of a scene from any viewpoint and camera lens we choose. Our goal is to deliver a meaningful and helpful user experience: not only the reconstructions themselves, but guided, interactive tours that give users the freedom to naturally explore spaces from the comfort of their smartphones. &lt;/p>; &lt;p>; To this end, we designed a controllable 360° video player that emulates flying through an indoor space along a predefined path, allowing the user to freely look around and travel forward or backwards. As the first Google product exploring this new technology, 360° videos were chosen as the format to deliver the generated content for a few reasons. &lt;/p>; &lt;p>; On the technical side, &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;real-time inference&lt;/a>; and &lt;a href=&quot;https://phog.github.io/snerg/&quot;>;baked representations&lt;/a>; are still resource intensive on a per-client basis (either on device or cloud computed), and relying on them would limit the number of users able to access this experience. By using videos, we are able to scale the storage and delivery of videos to all users by taking advantage of the same video management and serving infrastructure used by YouTube. On the operations side, videos give us clearer editorial control over the exploration experience and are easier to inspect for quality in large volumes. &lt;/p>; &lt;p>; While we had considered capturing the space with a 360° camera directly, using a NeRF to reconstruct and render the space has several advantages. A virtual camera can fly anywhere in space, including over obstacles and through windows, and can use any desired camera lens. The camera path can also be edited post-hoc for smoothness and speed, unlike a live recording. A NeRF capture also does not require the use of specialized camera hardware. &lt;/p>; &lt;p>; Our 360° videos are rendered by &lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_casting&quot;>;ray casting&lt;/a>; through each pixel of a virtual, spherical camera and compositing the visible elements of the scene. Each video follows a smooth path defined by a sequence of keyframe photos taken by the photographer during capture. The position of the camera for each picture is computed during structure-from-motion, and the sequence of pictures is smoothly interpolated into a flight path. &lt;/p>; &lt;p>; To keep speed consistent across different venues, we calibrate the distances for each by capturing pairs of images, each of which is 3 meters apart. By knowing measurements in the space, we scale the generated model, and render all videos at a natural velocity. &lt;/p>; &lt;p>; The final experience is surfaced to the user within &lt;a href=&quot;https://blog.google/products/maps/three-maps-updates-io-2022/&quot;>;Immersive View&lt;/a>;: the user can seamlessly fly into restaurants and other indoor venues and discover the space by flying through the photorealistic 360° videos. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Open research questions&lt;/h2>; &lt;p>; We believe that this feature is the first step of many in a journey towards universally accessible, AI-powered, immersive experiences. From a NeRF research perspective, more questions remain open. Some of these include: &lt;/p>; &lt;ol>; &lt;li>;Enhancing reconstructions with scene segmentation, adding semantic information to the scenes that could make scenes, for example, searchable and easier to navigate. &lt;/li>;&lt;li>;Adapting NeRF to outdoor photo collections, in addition to indoor. In doing so, we&#39;d unlock similar experiences to every corner of the world and change how users could experience the outdoor world. &lt;/li>;&lt;li>;Enabling real-time, interactive 3D exploration through neural-rendering on-device. &lt;/li>; &lt;/ol>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/outdoors_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Reconstruction of an outdoor scene with a NeRF model trained on Street View panoramas.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; As we continue to grow, we look forward to engaging with and contributing to the community to build the next generation of immersive experiences. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Contributors to the project include Jon Barron, Julius Beres, Daniel Duckworth, Roman Dudko, Magdalena Filak, Mike Harm, Peter Hedman, Claudio Martella, Ben Mildenhall, Cardin Moffett, Etienne Pot, Konstantinos Rematas, Yves Sallat, Marcos Seefelder, Lilyana Sirakovat, Sven Tresp and Peter Zhizhin.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Also, we&#39;d like to extend our thanks to Luke Barrington, Daniel Filip, Tom Funkhouser, Charles Goran, Pramod Gupta, Santi López, Mario Lučić, Isalo Montacute and Dan Thomasset for valuable feedback and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3072875289355194363/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot; rel=&quot;alternate&quot; title=&quot;Reconstructing indoor spaces with NeRF&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s72-c/Immersive%20View%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7719238929287079732&lt;/id>;&lt;published>;2023-06-13T10:18:00.004-07:00&lt;/published>;&lt;updated>;2023-06-13T13:54:56.343-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling delightful user experiences via predictive models of human attention&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Junfeng He, Senior Research Scientist, and Kai Kohlhoff, Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4ogEMk7vZJDropM08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s1400/HumanAttention.png&quot; style=&quot;display: none;&quot; />; &lt;p>; People have the remarkable ability to take in a tremendous amount of information (estimated to be ~10&lt;sup>;10&lt;/sup>; bits/s entering the retina) and selectively attend to a few task-relevant and interesting regions for further processing (eg, memory, comprehension, action). Modeling human attention (the result of which is often called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Saliency_map&quot;>;saliency&lt;/a>; model) has therefore been of interest across the fields of neuroscience, psychology, &lt;a href=&quot;https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction&quot;>;human-computer interaction&lt;/a>; (HCI) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_vision&quot;>;computer vision&lt;/a>;. The ability to predict which regions are likely to attract attention has numerous important applications in areas like graphics, photography, image compression and processing, and the measurement of visual quality. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We&#39;ve &lt;a href=&quot;https://ai.googleblog.com/2021/05/accelerating-eye-movement-research-for.html&quot;>;previously discussed&lt;/a>; the possibility of accelerating eye movement research using machine learning and smartphone-based gaze estimation, which earlier required specialized hardware costing up to $30,000 per unit. Related research includes “&lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/look-to-speak/&quot;>;Look to Speak&lt;/a>;”, which helps users with accessibility needs (eg, people with &lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;ALS&lt;/a>;) to communicate with their eyes, and the recently published “&lt;a href=&quot;https://ai.googleblog.com/2023/04/differentially-private-heatmaps.html&quot;>;Differentially private heatmaps&lt;/a>;” technique to compute heatmaps, like those for attention, while protecting users&#39; privacy. &lt;/p>; &lt;p>; In this blog, we present two papers (one from &lt;a href=&quot;https://cvpr2022.thecvf.com/&quot;>;CVPR 2022&lt;/a>;, and one just accepted to &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;) that highlight our recent research in the area of human attention modeling: “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf&quot;>;Deep Saliency Prior for Reducing Visual Distraction&lt;/a>;” and “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;”, together with recent research on saliency driven progressive loading for image compression (&lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html&quot;>;2&lt;/a>;). We showcase how predictive models of human attention can enable delightful user experiences such as image editing to minimize visual clutter, distraction or artifacts, image compression for faster loading of webpages or apps, and guiding ML models towards more intuitive human-like interpretation and model performance. We focus on image editing and image compression, and discuss recent advances in modeling in the context of these applications. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Attention-guided image editing&lt;/h2>; &lt;p>; Human attention models usually take an image as input (eg, a natural image or a screenshot of a webpage), and &lt;a href=&quot;https://dl.acm.org/doi/10.1109/TPAMI.2019.2935715&quot;>;predict a heatmap as output&lt;/a>;. The predicted heatmap on the image is &lt;a href=&quot;https://www.computer.org/csdl/journal/tp/2019/03/08315047/17D45Vw15wU&quot;>;evaluated against ground-truth attention data&lt;/a>;, which are typically collected by an eye tracker or &lt;a href=&quot;https://bubbleview.namwkim.org/&quot;>;approximated via mouse hovering/clicking&lt;/a>;. Previous models leveraged handcrafted features for visual clues, like color/brightness contrast, edges, and shape, while more recent approaches automatically learn discriminative features based on deep neural networks, from &lt;a href=&quot;https://arxiv.org/abs/1805.01047&quot;>;convolutional&lt;/a>; and &lt;a href=&quot;https://arxiv.org/pdf/1610.01708.pdf&quot;>;recurrent neural networks&lt;/a>; to more recent &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0925231222004714&quot;>;vision transformer networks&lt;/a>;. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf&quot;>;Deep Saliency Prior for Reducing Visual Distraction&lt;/a>;” (more information on this &lt;a href=&quot;https://deep-saliency-prior.github.io/&quot;>;project site&lt;/a>;), we leverage deep saliency models for dramatic yet visually realistic edits, which can significantly change an observer&#39;s attention to different image regions. For example, removing distracting objects in the background can reduce clutter in photos, leading to increased user satisfaction. Similarly, in video conferencing, reducing clutter in the background may increase focus on the main speaker (&lt;a href=&quot;https://deep-saliency-prior.github.io/supplementary/index.html&quot;>;example demo here&lt;/a>;). &lt;/p>; &lt;p>; To explore what types of editing effects can be achieved and how these affect viewers&#39; attention, we developed an optimization framework for guiding visual attention in images using a differentiable, predictive saliency model. Our method employs a state-of-the-art deep saliency model. Given an input image and a binary mask representing the distractor regions, pixels within the mask will be edited under the guidance of the predictive saliency model such that the saliency within the masked region is reduced. To make sure the edited image is natural and realistic, we carefully choose four image editing operators: two standard image editing operations, namely recolorization and image warping (shift); and two learned operators (we do not define the editing operation explicitly), namely a multi-layer convolution filter, and a generative model (&lt;a href=&quot;https://arxiv.org/abs/1912.04958&quot;>;GAN&lt;/a>;). &lt;/p>; &lt;p>; With those operators, our framework can produce a variety of powerful effects, with examples in the figure below, including recoloring, inpainting, camouflage, object editing or insertion, and facial attribute editing. Importantly, all these effects are driven solely by the single, pre-trained saliency model, without any additional supervision or training. Note that our goal is not to compete with dedicated methods for producing each effect, but rather to demonstrate how multiple editing operations can be guided by the knowledge embedded within deep saliency models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlABhC7P2SufIfcliXyTopTkNGeNAEmMbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlABhC7P2SufIfcliXyTopTkNGeNAEmMbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of reducing visual distractions, guided by the saliency model with several operators. The distractor region is marked on top of the saliency map (red border) in each example.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Enriching experiences with user-aware saliency modeling&lt;/h2>; &lt;p>; Prior research assumes a single saliency model for the whole population. However, human attention varies between individuals — while the detection of salient clues is fairly consistent, their order, interpretation, and gaze distributions can differ substantially. This offers opportunities to create personalized user experiences for individuals or groups. In “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;”, we introduce a user-aware saliency model, the first that can predict attention for one user, a group of users, and the general population, with a single model. &lt;/p>; &lt;p>; As shown in the figure below, core to the model is the combination of each participant&#39;s visual preferences with a per-user attention map and adaptive user masks. This requires per-user attention annotations to be available in the training data, eg, the &lt;a href=&quot;https://www.nature.com/articles/s41467-020-18360-5&quot;>;OSIE mobile gaze dataset for natural images; FiWI&lt;/a>; and &lt;a href=&quot;http://vision.cs.stonybrook.edu/~soura/websaliency.html&quot;>;WebSaliency&lt;/a>; datasets for web pages. Instead of predicting a single saliency map representing attention of all users, this model predicts per-user attention maps to encode individuals&#39; attention patterns. Further, the model adopts a user mask (a binary vector with the size equal to the number of participants) to indicate the presence of participants in the current sample, which makes it possible to select a group of participants and combine their preferences into a single heatmap. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3hCKho1q3HQKEiYgnwnKQtVnCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s1218/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;724&quot; data-original-width=&quot;1218&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3hCKho1q3HQKEiYgnwnKQtVnCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the user aware saliency model framework. The example image is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>; image set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; During inference, the user mask allows making predictions for any combination of participants. In the following figure, the first two rows are attention predictions for two different groups of participants (with three people in each group) on an image. A &lt;a href=&quot;https://arxiv.org/pdf/1805.01047.pdf&quot;>;conventional attention prediction model&lt;/a>; will predict identical attention heatmaps. Our model can distinguish the two groups (eg, the second group pays less attention to the face and more attention to the food than the first). Similarly, the last two rows are predictions on a webpage for two distinctive participants, with our model showing different preferences (eg, the second participant pays more attention to the left region than the first). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY855PtAIfs1vYISgJb5nSx4A0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s961/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;533&quot; data-original-width=&quot;961&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY855PtAIfs1vYISgJb5nSx4A0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Predicted attention vs. ground truth (GT). EML-Net: predictions from a state-of-the-art model, which will have the same predictions for the two participants/groups. Ours: predictions from our proposed user aware saliency model, which can predict the unique preference of each participant/group correctly. The first image is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>; image set, and the second is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/webpage_saliency.html&quot;>;FiWI&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Progressive image decoding centered on salient features&lt;/h2>; &lt;p>; Besides image editing, human attention models can also improve users&#39; browsing experience. One of the most frustrating and annoying user experiences while browsing is waiting for web pages with images to load, especially in conditions with low network connectivity. One way to improve the user experience in such cases is with progressive decoding of images, which decodes and displays increasingly higher-resolution image sections as data are downloaded, until the full-resolution image is ready. Progressive decoding usually proceeds in a sequential order (eg, left to right, top to bottom). With a predictive attention model (&lt;a href=&quot;https://github.com/google/attention-center&quot;>;1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;2&lt;/a>;), we can instead decode images based on saliency, making it possible to send the data necessary to display details of the most salient regions first. For example, in a portrait, bytes for the face can be prioritized over those for the out-of-focus background. Consequently, users perceive better image quality earlier and experience significantly reduced wait times. More details can be found in our open source blog posts (&lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;post 1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html&quot;>;post 2&lt;/a>;). Thus, predictive attention models can help with image compression and faster loading of web pages with images, improve rendering for large images and streaming/VR applications. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We&#39;ve shown how predictive models of human attention can enable delightful user experiences via applications such as image editing that can reduce clutter, distractions or artifacts in images or photos for users, and progressive image decoding that can greatly reduce the perceived waiting time for users while images are fully rendered. Our user-aware saliency model can further personalize the above applications for individual users or groups, enabling richer and more unique experiences. &lt;/p>; &lt;p>; Another interesting direction for predictive attention models is whether they can help improve robustness of computer vision models in tasks such as object classification or detection. For example, in “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;Teacher-generated spatial-attention labels boost robustness and accuracy of contrastive models&lt;/a>;”, we show that a predictive human attention model can guide &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;>;contrastive learning&lt;/a>; models to achieve better representation and improve the accuracy/robustness of classification tasks (on the &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; and &lt;a href=&quot;https://paperswithcode.com/dataset/imagenet-c&quot;>;ImageNet-C&lt;/a>; datasets). Further research in this direction could enable applications such as using radiologist&#39;s attention on medical images to improve health screening or diagnosis, or using human attention in complex driving scenarios to guide autonomous driving systems. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work involved collaborative efforts from a multidisciplinary team of software engineers, researchers, and cross-functional contributors. We&#39;d like to thank all the co-authors of the papers/research, including Kfir Aberman, Gamaleldin F. Elsayed, Moritz Firsching, Shi Chen, Nachiappan Valliappan, Yushi Yao, Chang Ye, Yossi Gandelsman, Inbar Mosseri, David E. Jacobes, Yael Pritch, Shaolei Shen, and Xinyu Ye. We also want to thank team members Oscar Ramirez, Venky Ramachandran and Tim Fujita for their help. Finally, we thank Vidhya Navalpakkam for her technical leadership in initiating and overseeing this body of work.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7719238929287079732/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7719238929287079732&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7719238929287079732&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling delightful user experiences via predictive models of human attention&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4ogEMk7vZJDropM08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s72-c/HumanAttention.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;